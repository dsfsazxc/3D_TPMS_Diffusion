{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b63a336",
   "metadata": {},
   "source": [
    "# Diffusion model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83534315",
   "metadata": {},
   "source": [
    "#### This notebook aims to launch the training of the main diffusion model. It does not train the classifier and regressor that are used to perform *classifier* and *regressor guidance*. The trainings of the three models (diffusion model, regressor and classifier) are independant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e82787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76386936",
   "metadata": {},
   "source": [
    "The environment variable 'TOPODIFF_LOGDIR' defines the directory where the logs and model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOPODIFF_LOGDIR'] = './checkpoints/3d_diff_logdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dee131",
   "metadata": {},
   "source": [
    "The 'TRAIN FLAGS', 'MODEL_FLAGS', 'DIFFUSION_FLAGS' and 'DATA_FLAGS' respectively set the training parameters, the model and diffusion hyperparameters and the directories where the training data are.\n",
    "\n",
    "The default values indicated below correspond to the hyperparameters indicated in the Appendix to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FLAGS = \"--batch_size 4 --save_interval 10000 --use_fp16 True --microbatch 2\"\n",
    "MODEL_FLAGS = \"--image_size 64 --num_channels 64 --num_res_blocks 2 --learn_sigma True --dropout 0.3 --use_checkpoint True\"\n",
    "DIFFUSION_FLAGS = \"--diffusion_steps 1000 --noise_schedule cosine\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d80c5",
   "metadata": {},
   "source": [
    "In order to run the training, make sure you have placed the data folder at the root of this directory.\n",
    "\n",
    "All the images, physical fields, and load arrays must be altogether in the same folder (done by default in the data directory that we provide you with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FLAGS = \"--data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_FLAGS = \"--dims 3 --volume_size 64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ea4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run scripts/image_train.py $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS $DATA_FLAGS $VOLUME_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcdfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOPODIFF_LOGDIR'] = './checkpoints/3d_diff_logdir'\n",
    "\n",
    "# dims 파라미터 없이, UNetModel에서 dims=3으로 하드코딩했으므로\n",
    "TRAIN_FLAGS = \"--batch_size 2 --save_interval 10000 --use_fp16 True\"\n",
    "MODEL_FLAGS = \"--image_size 64 --num_channels 32 --num_res_blocks 2 --learn_sigma True --dropout 0.1 --use_checkpoint True\"\n",
    "DIFFUSION_FLAGS = \"--diffusion_steps 1000 --noise_schedule cosine\"\n",
    "DATA_FLAGS = \"--data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\"\n",
    "\n",
    "%run scripts/image_train.py $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS $DATA_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ad8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./checkpoints/3d_diff_logdir_new\n",
      "Creating model and diffusion...\n",
      "Model parameters: 58,794,114 total, 58,794,114 trainable\n",
      "Model size: 224.3 MB (fp32)\n",
      "Input shape: [batch_size, 1, 64, 64, 64]\n",
      "Output channels: 2\n",
      "Creating data loader...\n",
      "Testing data loading...\n",
      "Loading 5597 files for training\n",
      "VolumeDataset initialized with 5597 volumes, target resolution: 64\n",
      "Loaded test batch: shape=torch.Size([2, 1, 64, 64, 64]), range=[-1.000, 1.000], mean=-0.417\n",
      "Starting training...\n",
      "----------------------------\n",
      "| grad_norm     | 13.8     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 1        |\n",
      "| mse           | 1        |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2        |\n",
      "| step          | 0        |\n",
      "| vb            | 0.00464  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 13.2     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.945    |\n",
      "| mse           | 0.935    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 22       |\n",
      "| step          | 10       |\n",
      "| vb            | 0.0102   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 13.6     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.801    |\n",
      "| mse           | 0.791    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 42       |\n",
      "| step          | 20       |\n",
      "| vb            | 0.0104   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 12.1     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.673    |\n",
      "| mse           | 0.666    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 62       |\n",
      "| step          | 30       |\n",
      "| vb            | 0.00644  |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 19.03400000000004\n",
      "----------------------------\n",
      "| grad_norm     | 11.1     |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.549    |\n",
      "| mse           | 0.541    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 82       |\n",
      "| step          | 40       |\n",
      "| vb            | 0.00765  |\n",
      "----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/scripts/image_train.py:123\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/scripts/image_train.py:59\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded test batch: shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_batch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange=[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_batch\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_batch\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mTrainLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmicrobatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmicrobatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:160\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    157\u001b[0m ):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Get next batch (simplified - only structure data)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdumpkvs()\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:177\u001b[0m, in \u001b[0;36mTrainLoop.run_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a single training step.\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_backward(batch)\n\u001b[0;32m--> 177\u001b[0m took_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m took_step:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ema()\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/fp16_util.py:185\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer.optimize\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, opt: th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fp16:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_fp16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_normal(opt)\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/fp16_util.py:191\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer._optimize_fp16\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_optimize_fp16\u001b[39m(\u001b[38;5;28mself\u001b[39m, opt: th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m    190\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlogkv_mean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlg_loss_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg_loss_scale)\n\u001b[0;32m--> 191\u001b[0m     \u001b[43mmodel_grads_to_master_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups_and_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     grad_norm, param_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_norms(grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg_loss_scale)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_overflow(grad_norm):\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/fp16_util.py:60\u001b[0m, in \u001b[0;36mmodel_grads_to_master_grads\u001b[0;34m(param_groups_and_shapes, master_params)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mCopy the gradients from the model parameters into the master parameters\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mfrom make_master_params().\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m master_param, (param_group, shape) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     58\u001b[0m     master_params, param_groups_and_shapes\n\u001b[1;32m     59\u001b[0m ):\n\u001b[0;32m---> 60\u001b[0m     master_param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[43m_flatten_dense_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_grad_or_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(shape)\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/_utils.py:510\u001b[0m, in \u001b[0;36m_flatten_dense_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dense_tensors\u001b[39m(tensors):\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Flatten dense tensors into a contiguous 1D buffer. Assume tensors are of\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m    same dense type.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m        A contiguous 1D buffer containing input tensors.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_dense_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOPODIFF_LOGDIR'] = './checkpoints/3d_diff_logdir_new'\n",
    "\n",
    "# 모든 학습 파라미터 포함\n",
    "TRAIN_FLAGS = \"\"\"\n",
    "--batch_size 2\n",
    "--save_interval 1000 \n",
    "--use_fp16 True \n",
    "--lr 5e-5 \n",
    "--weight_decay 0.01\n",
    "--ema_rate 0.9999\n",
    "--log_interval 10\n",
    "--microbatch 1\n",
    "--schedule_sampler uniform\n",
    "--resume_checkpoint \"\"\n",
    "\"\"\"\n",
    "\n",
    "MODEL_FLAGS = \"--image_size 64 --num_channels 64 --num_res_blocks 2 --learn_sigma True --dropout 0.1 --use_checkpoint True\"\n",
    "DIFFUSION_FLAGS = \"--diffusion_steps 1000 --noise_schedule cosine\"\n",
    "DATA_FLAGS = \"--data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\"\n",
    "\n",
    "%run scripts/image_train.py $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS $DATA_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa3e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a22e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./checkpoints/3d_diff_logdir_optimized_128\n",
      "Creating model and diffusion...\n",
      "Model parameters: 58,794,114 total, 58,794,114 trainable\n",
      "Model size: 224.3 MB (fp32)\n",
      "Input shape: [batch_size, 1, 64, 64, 64]\n",
      "Output channels: 2\n",
      "Creating data loader...\n",
      "Testing data loading...\n",
      "Loading 5597 files for training\n",
      "VolumeDataset initialized with 5597 volumes, target resolution: 64\n",
      "Loaded test batch: shape=torch.Size([4, 1, 64, 64, 64]), range=[-1.000, 1.000], mean=-0.464\n",
      "Starting training...\n",
      "----------------------------\n",
      "| grad_norm     | 13.5     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 1.01     |\n",
      "| mse           | 1        |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 4        |\n",
      "| step          | 0        |\n",
      "| vb            | 0.00767  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 12.9     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.882    |\n",
      "| mse           | 0.868    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 44       |\n",
      "| step          | 10       |\n",
      "| vb            | 0.0141   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 11.4     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.664    |\n",
      "| mse           | 0.612    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 84       |\n",
      "| step          | 20       |\n",
      "| vb            | 0.0516   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 8.87     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.401    |\n",
      "| mse           | 0.394    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 124      |\n",
      "| step          | 30       |\n",
      "| vb            | 0.00753  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 6.82     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.244    |\n",
      "| mse           | 0.239    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 164      |\n",
      "| step          | 40       |\n",
      "| vb            | 0.00478  |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 19.04900000000006\n",
      "----------------------------\n",
      "| grad_norm     | 4.66     |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.141    |\n",
      "| mse           | 0.136    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 204      |\n",
      "| step          | 50       |\n",
      "| vb            | 0.00472  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 3.2      |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0838   |\n",
      "| mse           | 0.0821   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 244      |\n",
      "| step          | 60       |\n",
      "| vb            | 0.00171  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 3.28     |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0917   |\n",
      "| mse           | 0.0876   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 284      |\n",
      "| step          | 70       |\n",
      "| vb            | 0.00412  |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 18.077000000000094\n",
      "----------------------------\n",
      "| grad_norm     | 1.89     |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0738   |\n",
      "| mse           | 0.0669   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 324      |\n",
      "| step          | 80       |\n",
      "| vb            | 0.00683  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 1.38     |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.0482   |\n",
      "| mse           | 0.0469   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 364      |\n",
      "| step          | 90       |\n",
      "| vb            | 0.00133  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.791    |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.0359   |\n",
      "| mse           | 0.0351   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 404      |\n",
      "| step          | 100      |\n",
      "| vb            | 0.000872 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 1.13     |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.463    |\n",
      "| mse           | 0.0384   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 444      |\n",
      "| step          | 110      |\n",
      "| vb            | 0.424    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 1.72     |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.049    |\n",
      "| mse           | 0.0459   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 484      |\n",
      "| step          | 120      |\n",
      "| vb            | 0.00309  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.978    |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.0384   |\n",
      "| mse           | 0.0372   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 524      |\n",
      "| step          | 130      |\n",
      "| vb            | 0.00116  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 1.14     |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.0477   |\n",
      "| mse           | 0.0438   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 564      |\n",
      "| step          | 140      |\n",
      "| vb            | 0.00386  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.758    |\n",
      "| lg_loss_scale | 18.1     |\n",
      "| loss          | 0.034    |\n",
      "| mse           | 0.0336   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 604      |\n",
      "| step          | 150      |\n",
      "| vb            | 0.000441 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.686    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0299   |\n",
      "| mse           | 0.0296   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 644      |\n",
      "| step          | 160      |\n",
      "| vb            | 0.000291 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.798    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0283   |\n",
      "| mse           | 0.0279   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 684      |\n",
      "| step          | 170      |\n",
      "| vb            | 0.000395 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.956    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0335   |\n",
      "| mse           | 0.0328   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 724      |\n",
      "| step          | 180      |\n",
      "| vb            | 0.000722 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.697    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.284    |\n",
      "| mse           | 0.0232   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 764      |\n",
      "| step          | 190      |\n",
      "| vb            | 0.261    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.856    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0287   |\n",
      "| mse           | 0.0273   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 804      |\n",
      "| step          | 200      |\n",
      "| vb            | 0.00143  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.664    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0303   |\n",
      "| mse           | 0.0295   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 844      |\n",
      "| step          | 210      |\n",
      "| vb            | 0.000823 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.61     |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0256   |\n",
      "| mse           | 0.0249   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 884      |\n",
      "| step          | 220      |\n",
      "| vb            | 0.000729 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.386    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0201   |\n",
      "| mse           | 0.0198   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 924      |\n",
      "| step          | 230      |\n",
      "| vb            | 0.000254 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.677    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0252   |\n",
      "| mse           | 0.024    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 964      |\n",
      "| step          | 240      |\n",
      "| vb            | 0.00121  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.435    |\n",
      "| lg_loss_scale | 18.2     |\n",
      "| loss          | 0.0179   |\n",
      "| mse           | 0.0177   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1e+03    |\n",
      "| step          | 250      |\n",
      "| vb            | 0.000236 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.274    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0134   |\n",
      "| mse           | 0.0133   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.04e+03 |\n",
      "| step          | 260      |\n",
      "| vb            | 0.000127 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 1.2      |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0891   |\n",
      "| mse           | 0.0501   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.08e+03 |\n",
      "| step          | 270      |\n",
      "| vb            | 0.039    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.716    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0278   |\n",
      "| mse           | 0.0261   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.12e+03 |\n",
      "| step          | 280      |\n",
      "| vb            | 0.00167  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.372    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0206   |\n",
      "| mse           | 0.0196   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.16e+03 |\n",
      "| step          | 290      |\n",
      "| vb            | 0.00102  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.246    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0148   |\n",
      "| mse           | 0.0146   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.2e+03  |\n",
      "| step          | 300      |\n",
      "| vb            | 0.000161 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.856    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0494   |\n",
      "| mse           | 0.0351   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.24e+03 |\n",
      "| step          | 310      |\n",
      "| vb            | 0.0143   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.485    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0241   |\n",
      "| mse           | 0.0234   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.28e+03 |\n",
      "| step          | 320      |\n",
      "| vb            | 0.000722 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.554    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0303   |\n",
      "| mse           | 0.0267   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.32e+03 |\n",
      "| step          | 330      |\n",
      "| vb            | 0.00364  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.375    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0193   |\n",
      "| mse           | 0.0188   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.36e+03 |\n",
      "| step          | 340      |\n",
      "| vb            | 0.000493 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.328    |\n",
      "| lg_loss_scale | 18.3     |\n",
      "| loss          | 0.0169   |\n",
      "| mse           | 0.0165   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.4e+03  |\n",
      "| step          | 350      |\n",
      "| vb            | 0.000381 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.295    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0155   |\n",
      "| mse           | 0.0153   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.44e+03 |\n",
      "| step          | 360      |\n",
      "| vb            | 0.000228 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.178    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.013    |\n",
      "| mse           | 0.0129   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.48e+03 |\n",
      "| step          | 370      |\n",
      "| vb            | 0.000138 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.194    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0139   |\n",
      "| mse           | 0.0138   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.52e+03 |\n",
      "| step          | 380      |\n",
      "| vb            | 0.000189 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.192    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0131   |\n",
      "| mse           | 0.0129   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.56e+03 |\n",
      "| step          | 390      |\n",
      "| vb            | 0.000274 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.197    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0146   |\n",
      "| mse           | 0.0143   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.6e+03  |\n",
      "| step          | 400      |\n",
      "| vb            | 0.000374 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.148    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0114   |\n",
      "| mse           | 0.0113   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.64e+03 |\n",
      "| step          | 410      |\n",
      "| vb            | 0.000127 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.201    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0134   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.68e+03 |\n",
      "| step          | 420      |\n",
      "| vb            | 0.00035  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.292    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0136   |\n",
      "| mse           | 0.0131   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.72e+03 |\n",
      "| step          | 430      |\n",
      "| vb            | 0.000528 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.216    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0146   |\n",
      "| mse           | 0.0139   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.76e+03 |\n",
      "| step          | 440      |\n",
      "| vb            | 0.000712 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.162    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0101   |\n",
      "| mse           | 0.0101   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.8e+03  |\n",
      "| step          | 450      |\n",
      "| vb            | 8.24e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.607    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0572   |\n",
      "| mse           | 0.0238   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.84e+03 |\n",
      "| step          | 460      |\n",
      "| vb            | 0.0334   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.568    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0288   |\n",
      "| mse           | 0.0224   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.88e+03 |\n",
      "| step          | 470      |\n",
      "| vb            | 0.00636  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.207    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.009    |\n",
      "| mse           | 0.00892  |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.92e+03 |\n",
      "| step          | 480      |\n",
      "| vb            | 7.73e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.278    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.018    |\n",
      "| mse           | 0.0173   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 1.96e+03 |\n",
      "| step          | 490      |\n",
      "| vb            | 0.000742 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.297    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0174   |\n",
      "| mse           | 0.0167   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2e+03    |\n",
      "| step          | 500      |\n",
      "| vb            | 0.000722 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.196    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0115   |\n",
      "| mse           | 0.0113   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.04e+03 |\n",
      "| step          | 510      |\n",
      "| vb            | 0.000216 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.221    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0129   |\n",
      "| mse           | 0.0127   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.08e+03 |\n",
      "| step          | 520      |\n",
      "| vb            | 0.000231 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.369    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0259   |\n",
      "| mse           | 0.0231   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.12e+03 |\n",
      "| step          | 530      |\n",
      "| vb            | 0.00284  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.23     |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0135   |\n",
      "| mse           | 0.013    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.16e+03 |\n",
      "| step          | 540      |\n",
      "| vb            | 0.000582 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.28     |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0181   |\n",
      "| mse           | 0.0174   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.2e+03  |\n",
      "| step          | 550      |\n",
      "| vb            | 0.000706 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.18     |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0111   |\n",
      "| mse           | 0.0108   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.24e+03 |\n",
      "| step          | 560      |\n",
      "| vb            | 0.000251 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.188    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.01     |\n",
      "| mse           | 0.00996  |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.28e+03 |\n",
      "| step          | 570      |\n",
      "| vb            | 9.16e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.211    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0136   |\n",
      "| mse           | 0.0129   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.32e+03 |\n",
      "| step          | 580      |\n",
      "| vb            | 0.000723 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.199    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0124   |\n",
      "| mse           | 0.0122   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.36e+03 |\n",
      "| step          | 590      |\n",
      "| vb            | 0.000206 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.184    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.012    |\n",
      "| mse           | 0.0119   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.4e+03  |\n",
      "| step          | 600      |\n",
      "| vb            | 0.000175 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.266    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0204   |\n",
      "| mse           | 0.018    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.44e+03 |\n",
      "| step          | 610      |\n",
      "| vb            | 0.00245  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.163    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.01     |\n",
      "| mse           | 0.00991  |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.48e+03 |\n",
      "| step          | 620      |\n",
      "| vb            | 0.000136 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.474    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0299   |\n",
      "| mse           | 0.0273   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.52e+03 |\n",
      "| step          | 630      |\n",
      "| vb            | 0.00258  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.528    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0265   |\n",
      "| mse           | 0.0209   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.56e+03 |\n",
      "| step          | 640      |\n",
      "| vb            | 0.00566  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.279    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0172   |\n",
      "| mse           | 0.0168   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.6e+03  |\n",
      "| step          | 650      |\n",
      "| vb            | 0.000469 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.261    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0135   |\n",
      "| mse           | 0.0133   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.64e+03 |\n",
      "| step          | 660      |\n",
      "| vb            | 0.000176 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.235    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0171   |\n",
      "| mse           | 0.0166   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.68e+03 |\n",
      "| step          | 670      |\n",
      "| vb            | 0.000541 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.177    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0161   |\n",
      "| mse           | 0.0155   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.72e+03 |\n",
      "| step          | 680      |\n",
      "| vb            | 0.000544 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.226    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0153   |\n",
      "| mse           | 0.0147   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.76e+03 |\n",
      "| step          | 690      |\n",
      "| vb            | 0.000578 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.279    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0182   |\n",
      "| mse           | 0.017    |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.8e+03  |\n",
      "| step          | 700      |\n",
      "| vb            | 0.00114  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.179    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0134   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.84e+03 |\n",
      "| step          | 710      |\n",
      "| vb            | 0.000265 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.188    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0134   |\n",
      "| mse           | 0.0132   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.88e+03 |\n",
      "| step          | 720      |\n",
      "| vb            | 0.000211 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.284    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0217   |\n",
      "| mse           | 0.0197   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.92e+03 |\n",
      "| step          | 730      |\n",
      "| vb            | 0.00199  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.222    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0145   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 2.96e+03 |\n",
      "| step          | 740      |\n",
      "| vb            | 0.000276 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.251    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.016    |\n",
      "| mse           | 0.0153   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3e+03    |\n",
      "| step          | 750      |\n",
      "| vb            | 0.000672 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.179    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0124   |\n",
      "| mse           | 0.0123   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.04e+03 |\n",
      "| step          | 760      |\n",
      "| vb            | 0.000134 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.23     |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0154   |\n",
      "| mse           | 0.0149   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.08e+03 |\n",
      "| step          | 770      |\n",
      "| vb            | 0.000525 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.251    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0203   |\n",
      "| mse           | 0.0192   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.12e+03 |\n",
      "| step          | 780      |\n",
      "| vb            | 0.00108  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.181    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0151   |\n",
      "| mse           | 0.0148   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.16e+03 |\n",
      "| step          | 790      |\n",
      "| vb            | 0.000335 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.22     |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0177   |\n",
      "| mse           | 0.0173   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.2e+03  |\n",
      "| step          | 800      |\n",
      "| vb            | 0.000381 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.172    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.176    |\n",
      "| mse           | 0.0138   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.24e+03 |\n",
      "| step          | 810      |\n",
      "| vb            | 0.162    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.221    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.015    |\n",
      "| mse           | 0.0147   |\n",
      "| param_norm    | 139      |\n",
      "| samples       | 3.28e+03 |\n",
      "| step          | 820      |\n",
      "| vb            | 0.000283 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.389    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0235   |\n",
      "| mse           | 0.0216   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.32e+03 |\n",
      "| step          | 830      |\n",
      "| vb            | 0.00193  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.285    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0146   |\n",
      "| mse           | 0.0144   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.36e+03 |\n",
      "| step          | 840      |\n",
      "| vb            | 0.000165 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.212    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.19     |\n",
      "| mse           | 0.0163   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.4e+03  |\n",
      "| step          | 850      |\n",
      "| vb            | 0.174    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.234    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0185   |\n",
      "| mse           | 0.0175   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.44e+03 |\n",
      "| step          | 860      |\n",
      "| vb            | 0.001    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.377    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0229   |\n",
      "| mse           | 0.0213   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.48e+03 |\n",
      "| step          | 870      |\n",
      "| vb            | 0.00154  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.235    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.014    |\n",
      "| mse           | 0.0138   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.52e+03 |\n",
      "| step          | 880      |\n",
      "| vb            | 0.000222 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.196    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0159   |\n",
      "| mse           | 0.0154   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.56e+03 |\n",
      "| step          | 890      |\n",
      "| vb            | 0.000433 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.349    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.028    |\n",
      "| mse           | 0.025    |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.6e+03  |\n",
      "| step          | 900      |\n",
      "| vb            | 0.00302  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.435    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0247   |\n",
      "| mse           | 0.0229   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.64e+03 |\n",
      "| step          | 910      |\n",
      "| vb            | 0.00177  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.396    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0267   |\n",
      "| mse           | 0.0241   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.68e+03 |\n",
      "| step          | 920      |\n",
      "| vb            | 0.0026   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.308    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0176   |\n",
      "| mse           | 0.0173   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.72e+03 |\n",
      "| step          | 930      |\n",
      "| vb            | 0.000363 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.189    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0167   |\n",
      "| mse           | 0.0163   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.76e+03 |\n",
      "| step          | 940      |\n",
      "| vb            | 0.000356 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.166    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0161   |\n",
      "| mse           | 0.0159   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.8e+03  |\n",
      "| step          | 950      |\n",
      "| vb            | 0.000189 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.178    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0143   |\n",
      "| mse           | 0.014    |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.84e+03 |\n",
      "| step          | 960      |\n",
      "| vb            | 0.000331 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.166    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0184   |\n",
      "| mse           | 0.0179   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.88e+03 |\n",
      "| step          | 970      |\n",
      "| vb            | 0.000478 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.286    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0206   |\n",
      "| mse           | 0.0197   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.92e+03 |\n",
      "| step          | 980      |\n",
      "| vb            | 0.000935 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.196    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0162   |\n",
      "| mse           | 0.0159   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 3.96e+03 |\n",
      "| step          | 990      |\n",
      "| vb            | 0.000354 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.17     |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0147   |\n",
      "| mse           | 0.0144   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4e+03    |\n",
      "| step          | 1e+03    |\n",
      "| vb            | 0.000245 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.367    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0251   |\n",
      "| mse           | 0.0212   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.04e+03 |\n",
      "| step          | 1.01e+03 |\n",
      "| vb            | 0.00392  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.296    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.209    |\n",
      "| mse           | 0.0143   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.08e+03 |\n",
      "| step          | 1.02e+03 |\n",
      "| vb            | 0.195    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.241    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0151   |\n",
      "| mse           | 0.0149   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.12e+03 |\n",
      "| step          | 1.03e+03 |\n",
      "| vb            | 0.000174 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.278    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0165   |\n",
      "| mse           | 0.0159   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.16e+03 |\n",
      "| step          | 1.04e+03 |\n",
      "| vb            | 0.00053  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.215    |\n",
      "| lg_loss_scale | 19       |\n",
      "| loss          | 0.0146   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.2e+03  |\n",
      "| step          | 1.05e+03 |\n",
      "| vb            | 0.00038  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.187    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0134   |\n",
      "| mse           | 0.0131   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.24e+03 |\n",
      "| step          | 1.06e+03 |\n",
      "| vb            | 0.000306 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.161    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0135   |\n",
      "| mse           | 0.0133   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.28e+03 |\n",
      "| step          | 1.07e+03 |\n",
      "| vb            | 0.000198 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.223    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0212   |\n",
      "| mse           | 0.02     |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.32e+03 |\n",
      "| step          | 1.08e+03 |\n",
      "| vb            | 0.00115  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.26     |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0151   |\n",
      "| mse           | 0.0148   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.36e+03 |\n",
      "| step          | 1.09e+03 |\n",
      "| vb            | 0.000256 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.509    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0208   |\n",
      "| mse           | 0.0192   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.4e+03  |\n",
      "| step          | 1.1e+03  |\n",
      "| vb            | 0.0016   |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.373    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0237   |\n",
      "| mse           | 0.0225   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.44e+03 |\n",
      "| step          | 1.11e+03 |\n",
      "| vb            | 0.00121  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.178    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0129   |\n",
      "| mse           | 0.0127   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.48e+03 |\n",
      "| step          | 1.12e+03 |\n",
      "| vb            | 0.000209 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.249    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0184   |\n",
      "| mse           | 0.018    |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.52e+03 |\n",
      "| step          | 1.13e+03 |\n",
      "| vb            | 0.00041  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.23     |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0178   |\n",
      "| mse           | 0.0171   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.56e+03 |\n",
      "| step          | 1.14e+03 |\n",
      "| vb            | 0.000735 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.53     |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0518   |\n",
      "| mse           | 0.025    |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.6e+03  |\n",
      "| step          | 1.15e+03 |\n",
      "| vb            | 0.0268   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.257    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0139   |\n",
      "| mse           | 0.0137   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.64e+03 |\n",
      "| step          | 1.16e+03 |\n",
      "| vb            | 0.000129 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.269    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0168   |\n",
      "| mse           | 0.0164   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.68e+03 |\n",
      "| step          | 1.17e+03 |\n",
      "| vb            | 0.000388 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.438    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0221   |\n",
      "| mse           | 0.0214   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.72e+03 |\n",
      "| step          | 1.18e+03 |\n",
      "| vb            | 0.000776 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.289    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0159   |\n",
      "| mse           | 0.0153   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.76e+03 |\n",
      "| step          | 1.19e+03 |\n",
      "| vb            | 0.000553 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.192    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.013    |\n",
      "| mse           | 0.0129   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.8e+03  |\n",
      "| step          | 1.2e+03  |\n",
      "| vb            | 0.000103 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.36     |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0222   |\n",
      "| mse           | 0.021    |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.84e+03 |\n",
      "| step          | 1.21e+03 |\n",
      "| vb            | 0.00119  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.454    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0236   |\n",
      "| mse           | 0.0223   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.88e+03 |\n",
      "| step          | 1.22e+03 |\n",
      "| vb            | 0.00131  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.216    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0164   |\n",
      "| mse           | 0.0156   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.92e+03 |\n",
      "| step          | 1.23e+03 |\n",
      "| vb            | 0.000754 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.235    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.019    |\n",
      "| mse           | 0.0184   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 4.96e+03 |\n",
      "| step          | 1.24e+03 |\n",
      "| vb            | 0.000609 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.162    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.016    |\n",
      "| mse           | 0.0158   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5e+03    |\n",
      "| step          | 1.25e+03 |\n",
      "| vb            | 0.000233 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.117    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0141   |\n",
      "| mse           | 0.014    |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.04e+03 |\n",
      "| step          | 1.26e+03 |\n",
      "| vb            | 0.000117 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.104    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0138   |\n",
      "| mse           | 0.0136   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.08e+03 |\n",
      "| step          | 1.27e+03 |\n",
      "| vb            | 0.000226 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.102    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.013    |\n",
      "| mse           | 0.0128   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.12e+03 |\n",
      "| step          | 1.28e+03 |\n",
      "| vb            | 0.000167 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.12     |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0145   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.16e+03 |\n",
      "| step          | 1.29e+03 |\n",
      "| vb            | 0.000226 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.115    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0125   |\n",
      "| mse           | 0.0123   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.2e+03  |\n",
      "| step          | 1.3e+03  |\n",
      "| vb            | 0.000174 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.158    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.187    |\n",
      "| mse           | 0.0152   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.24e+03 |\n",
      "| step          | 1.31e+03 |\n",
      "| vb            | 0.172    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.155    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0133   |\n",
      "| mse           | 0.0131   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.28e+03 |\n",
      "| step          | 1.32e+03 |\n",
      "| vb            | 0.000277 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.121    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0128   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.32e+03 |\n",
      "| step          | 1.33e+03 |\n",
      "| vb            | 0.000898 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.153    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0153   |\n",
      "| mse           | 0.0146   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.36e+03 |\n",
      "| step          | 1.34e+03 |\n",
      "| vb            | 0.000654 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.193    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0139   |\n",
      "| mse           | 0.0136   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.4e+03  |\n",
      "| step          | 1.35e+03 |\n",
      "| vb            | 0.000268 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.179    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0152   |\n",
      "| mse           | 0.0146   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.44e+03 |\n",
      "| step          | 1.36e+03 |\n",
      "| vb            | 0.000565 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.21     |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0162   |\n",
      "| mse           | 0.0157   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.48e+03 |\n",
      "| step          | 1.37e+03 |\n",
      "| vb            | 0.000543 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.261    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0187   |\n",
      "| mse           | 0.0182   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.52e+03 |\n",
      "| step          | 1.38e+03 |\n",
      "| vb            | 0.000497 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.25     |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0148   |\n",
      "| mse           | 0.0146   |\n",
      "| param_norm    | 140      |\n",
      "| samples       | 5.56e+03 |\n",
      "| step          | 1.39e+03 |\n",
      "| vb            | 0.000234 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.289    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0202   |\n",
      "| mse           | 0.0194   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.6e+03  |\n",
      "| step          | 1.4e+03  |\n",
      "| vb            | 0.00086  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.209    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0134   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.64e+03 |\n",
      "| step          | 1.41e+03 |\n",
      "| vb            | 0.000274 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.152    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0134   |\n",
      "| mse           | 0.0127   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.68e+03 |\n",
      "| step          | 1.42e+03 |\n",
      "| vb            | 0.000648 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.499    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.022    |\n",
      "| mse           | 0.021    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.72e+03 |\n",
      "| step          | 1.43e+03 |\n",
      "| vb            | 0.00104  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.273    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0151   |\n",
      "| mse           | 0.0149   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.76e+03 |\n",
      "| step          | 1.44e+03 |\n",
      "| vb            | 0.000163 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.391    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0219   |\n",
      "| mse           | 0.0208   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.8e+03  |\n",
      "| step          | 1.45e+03 |\n",
      "| vb            | 0.00117  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.289    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0154   |\n",
      "| mse           | 0.0152   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.84e+03 |\n",
      "| step          | 1.46e+03 |\n",
      "| vb            | 0.000167 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.234    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0152   |\n",
      "| mse           | 0.015    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.88e+03 |\n",
      "| step          | 1.47e+03 |\n",
      "| vb            | 0.000249 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.262    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0197   |\n",
      "| mse           | 0.0181   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.92e+03 |\n",
      "| step          | 1.48e+03 |\n",
      "| vb            | 0.00158  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.194    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0138   |\n",
      "| mse           | 0.0137   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 5.96e+03 |\n",
      "| step          | 1.49e+03 |\n",
      "| vb            | 0.000166 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.186    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0174   |\n",
      "| mse           | 0.0165   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6e+03    |\n",
      "| step          | 1.5e+03  |\n",
      "| vb            | 0.000972 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.166    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0166   |\n",
      "| mse           | 0.016    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.04e+03 |\n",
      "| step          | 1.51e+03 |\n",
      "| vb            | 0.000582 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.186    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.017    |\n",
      "| mse           | 0.0155   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.08e+03 |\n",
      "| step          | 1.52e+03 |\n",
      "| vb            | 0.00142  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.111    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0117   |\n",
      "| mse           | 0.0116   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.12e+03 |\n",
      "| step          | 1.53e+03 |\n",
      "| vb            | 0.000101 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.215    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0148   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.16e+03 |\n",
      "| step          | 1.54e+03 |\n",
      "| vb            | 0.000609 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.139    |\n",
      "| lg_loss_scale | 19.5     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0135   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.2e+03  |\n",
      "| step          | 1.55e+03 |\n",
      "| vb            | 0.000263 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.115    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0122   |\n",
      "| mse           | 0.012    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.24e+03 |\n",
      "| step          | 1.56e+03 |\n",
      "| vb            | 0.000216 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.131    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0118   |\n",
      "| mse           | 0.0117   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.28e+03 |\n",
      "| step          | 1.57e+03 |\n",
      "| vb            | 0.000108 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.142    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0123   |\n",
      "| mse           | 0.0122   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.32e+03 |\n",
      "| step          | 1.58e+03 |\n",
      "| vb            | 0.000119 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.114    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0121   |\n",
      "| mse           | 0.0119   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.36e+03 |\n",
      "| step          | 1.59e+03 |\n",
      "| vb            | 0.000218 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.366    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0231   |\n",
      "| mse           | 0.0183   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.4e+03  |\n",
      "| step          | 1.6e+03  |\n",
      "| vb            | 0.00474  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.285    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0167   |\n",
      "| mse           | 0.0161   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.44e+03 |\n",
      "| step          | 1.61e+03 |\n",
      "| vb            | 0.000589 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.214    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0132   |\n",
      "| mse           | 0.013    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.48e+03 |\n",
      "| step          | 1.62e+03 |\n",
      "| vb            | 0.000182 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.271    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0196   |\n",
      "| mse           | 0.0188   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.52e+03 |\n",
      "| step          | 1.63e+03 |\n",
      "| vb            | 0.000817 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.218    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0144   |\n",
      "| mse           | 0.0141   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.56e+03 |\n",
      "| step          | 1.64e+03 |\n",
      "| vb            | 0.000327 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.115    |\n",
      "| lg_loss_scale | 19.6     |\n",
      "| loss          | 0.0119   |\n",
      "| mse           | 0.0117   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.6e+03  |\n",
      "| step          | 1.65e+03 |\n",
      "| vb            | 0.000139 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.206    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0148   |\n",
      "| mse           | 0.014    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.64e+03 |\n",
      "| step          | 1.66e+03 |\n",
      "| vb            | 0.000832 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.163    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0144   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.68e+03 |\n",
      "| step          | 1.67e+03 |\n",
      "| vb            | 0.000234 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.175    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0142   |\n",
      "| mse           | 0.014    |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.72e+03 |\n",
      "| step          | 1.68e+03 |\n",
      "| vb            | 0.000219 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.22     |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0183   |\n",
      "| mse           | 0.0166   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.76e+03 |\n",
      "| step          | 1.69e+03 |\n",
      "| vb            | 0.00174  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.399    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0206   |\n",
      "| mse           | 0.0194   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.8e+03  |\n",
      "| step          | 1.7e+03  |\n",
      "| vb            | 0.00119  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.446    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0249   |\n",
      "| mse           | 0.0229   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.84e+03 |\n",
      "| step          | 1.71e+03 |\n",
      "| vb            | 0.00207  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.233    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.215    |\n",
      "| mse           | 0.0168   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.88e+03 |\n",
      "| step          | 1.72e+03 |\n",
      "| vb            | 0.198    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.297    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.027    |\n",
      "| mse           | 0.0248   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.92e+03 |\n",
      "| step          | 1.73e+03 |\n",
      "| vb            | 0.00221  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.141    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0154   |\n",
      "| mse           | 0.0151   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 6.96e+03 |\n",
      "| step          | 1.74e+03 |\n",
      "| vb            | 0.000227 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.133    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.014    |\n",
      "| mse           | 0.0138   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7e+03    |\n",
      "| step          | 1.75e+03 |\n",
      "| vb            | 0.000217 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.132    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0139   |\n",
      "| mse           | 0.0135   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.04e+03 |\n",
      "| step          | 1.76e+03 |\n",
      "| vb            | 0.000375 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.154    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0149   |\n",
      "| mse           | 0.0145   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.08e+03 |\n",
      "| step          | 1.77e+03 |\n",
      "| vb            | 0.000353 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.121    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0123   |\n",
      "| mse           | 0.0122   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.12e+03 |\n",
      "| step          | 1.78e+03 |\n",
      "| vb            | 0.000162 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.159    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0164   |\n",
      "| mse           | 0.0158   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.16e+03 |\n",
      "| step          | 1.79e+03 |\n",
      "| vb            | 0.000568 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.124    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0136   |\n",
      "| mse           | 0.0133   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.2e+03  |\n",
      "| step          | 1.8e+03  |\n",
      "| vb            | 0.000285 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.0795   |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0135   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.24e+03 |\n",
      "| step          | 1.81e+03 |\n",
      "| vb            | 0.000227 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.123    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0125   |\n",
      "| mse           | 0.0121   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.28e+03 |\n",
      "| step          | 1.82e+03 |\n",
      "| vb            | 0.000458 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.117    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0153   |\n",
      "| mse           | 0.0148   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.32e+03 |\n",
      "| step          | 1.83e+03 |\n",
      "| vb            | 0.000591 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.222    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0204   |\n",
      "| mse           | 0.0182   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.36e+03 |\n",
      "| step          | 1.84e+03 |\n",
      "| vb            | 0.00225  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.252    |\n",
      "| lg_loss_scale | 19.8     |\n",
      "| loss          | 0.0144   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 141      |\n",
      "| samples       | 7.4e+03  |\n",
      "| step          | 1.85e+03 |\n",
      "| vb            | 0.000181 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.389    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0221   |\n",
      "| mse           | 0.0183   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.44e+03 |\n",
      "| step          | 1.86e+03 |\n",
      "| vb            | 0.00383  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.168    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0109   |\n",
      "| mse           | 0.0107   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.48e+03 |\n",
      "| step          | 1.87e+03 |\n",
      "| vb            | 0.000151 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.232    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0216   |\n",
      "| mse           | 0.0176   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.52e+03 |\n",
      "| step          | 1.88e+03 |\n",
      "| vb            | 0.00402  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.178    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0135   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.56e+03 |\n",
      "| step          | 1.89e+03 |\n",
      "| vb            | 0.000154 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.162    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0121   |\n",
      "| mse           | 0.0119   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.6e+03  |\n",
      "| step          | 1.9e+03  |\n",
      "| vb            | 0.000224 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.13     |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0117   |\n",
      "| mse           | 0.0116   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.64e+03 |\n",
      "| step          | 1.91e+03 |\n",
      "| vb            | 0.000143 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.124    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0118   |\n",
      "| mse           | 0.0117   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.68e+03 |\n",
      "| step          | 1.92e+03 |\n",
      "| vb            | 0.00012  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.102    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0118   |\n",
      "| mse           | 0.0116   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.72e+03 |\n",
      "| step          | 1.93e+03 |\n",
      "| vb            | 0.000135 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.163    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.0144   |\n",
      "| mse           | 0.0136   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.76e+03 |\n",
      "| step          | 1.94e+03 |\n",
      "| vb            | 0.000813 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.195    |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.013    |\n",
      "| mse           | 0.0128   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.8e+03  |\n",
      "| step          | 1.95e+03 |\n",
      "| vb            | 0.000216 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.167    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0137   |\n",
      "| mse           | 0.0133   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.84e+03 |\n",
      "| step          | 1.96e+03 |\n",
      "| vb            | 0.000346 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.207    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0204   |\n",
      "| mse           | 0.0168   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.88e+03 |\n",
      "| step          | 1.97e+03 |\n",
      "| vb            | 0.00364  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.148    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0117   |\n",
      "| mse           | 0.0115   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.92e+03 |\n",
      "| step          | 1.98e+03 |\n",
      "| vb            | 0.000144 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.233    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0188   |\n",
      "| mse           | 0.0166   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 7.96e+03 |\n",
      "| step          | 1.99e+03 |\n",
      "| vb            | 0.00216  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.0956   |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.011    |\n",
      "| mse           | 0.0108   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8e+03    |\n",
      "| step          | 2e+03    |\n",
      "| vb            | 0.000131 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.113    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0113   |\n",
      "| mse           | 0.0111   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.04e+03 |\n",
      "| step          | 2.01e+03 |\n",
      "| vb            | 0.000214 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.107    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0125   |\n",
      "| mse           | 0.0121   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.08e+03 |\n",
      "| step          | 2.02e+03 |\n",
      "| vb            | 0.00034  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.162    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0139   |\n",
      "| mse           | 0.0135   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.12e+03 |\n",
      "| step          | 2.03e+03 |\n",
      "| vb            | 0.000378 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.114    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0103   |\n",
      "| mse           | 0.0102   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.16e+03 |\n",
      "| step          | 2.04e+03 |\n",
      "| vb            | 0.000131 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.136    |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.0118   |\n",
      "| mse           | 0.0117   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.2e+03  |\n",
      "| step          | 2.05e+03 |\n",
      "| vb            | 0.000146 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.11     |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.012    |\n",
      "| mse           | 0.0116   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.24e+03 |\n",
      "| step          | 2.06e+03 |\n",
      "| vb            | 0.000342 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.124    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0119   |\n",
      "| mse           | 0.0117   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.28e+03 |\n",
      "| step          | 2.07e+03 |\n",
      "| vb            | 0.000202 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.167    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0158   |\n",
      "| mse           | 0.0147   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.32e+03 |\n",
      "| step          | 2.08e+03 |\n",
      "| vb            | 0.00111  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.114    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0112   |\n",
      "| mse           | 0.0111   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.36e+03 |\n",
      "| step          | 2.09e+03 |\n",
      "| vb            | 0.000116 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.148    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0141   |\n",
      "| mse           | 0.0135   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.4e+03  |\n",
      "| step          | 2.1e+03  |\n",
      "| vb            | 0.000616 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.143    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0122   |\n",
      "| mse           | 0.012    |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.44e+03 |\n",
      "| step          | 2.11e+03 |\n",
      "| vb            | 0.000186 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.195    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0151   |\n",
      "| mse           | 0.0141   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.48e+03 |\n",
      "| step          | 2.12e+03 |\n",
      "| vb            | 0.00104  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.152    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0107   |\n",
      "| mse           | 0.0105   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.52e+03 |\n",
      "| step          | 2.13e+03 |\n",
      "| vb            | 0.000133 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.139    |\n",
      "| lg_loss_scale | 20.1     |\n",
      "| loss          | 0.0107   |\n",
      "| mse           | 0.0105   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.56e+03 |\n",
      "| step          | 2.14e+03 |\n",
      "| vb            | 0.000221 |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 19.14400000000262\n",
      "----------------------------\n",
      "| grad_norm     | 0.157    |\n",
      "| lg_loss_scale | 19.7     |\n",
      "| loss          | 0.0163   |\n",
      "| mse           | 0.0138   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.6e+03  |\n",
      "| step          | 2.15e+03 |\n",
      "| vb            | 0.00248  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.166    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0116   |\n",
      "| mse           | 0.0113   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.64e+03 |\n",
      "| step          | 2.16e+03 |\n",
      "| vb            | 0.000272 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.117    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0103   |\n",
      "| mse           | 0.0102   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.68e+03 |\n",
      "| step          | 2.17e+03 |\n",
      "| vb            | 0.000105 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.152    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0122   |\n",
      "| mse           | 0.0119   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.72e+03 |\n",
      "| step          | 2.18e+03 |\n",
      "| vb            | 0.000297 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.155    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.00895  |\n",
      "| mse           | 0.00884  |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.76e+03 |\n",
      "| step          | 2.19e+03 |\n",
      "| vb            | 0.000107 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.133    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.134    |\n",
      "| mse           | 0.0103   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.8e+03  |\n",
      "| step          | 2.2e+03  |\n",
      "| vb            | 0.124    |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.131    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0104   |\n",
      "| mse           | 0.0101   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.84e+03 |\n",
      "| step          | 2.21e+03 |\n",
      "| vb            | 0.000368 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.269    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0192   |\n",
      "| mse           | 0.0164   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.88e+03 |\n",
      "| step          | 2.22e+03 |\n",
      "| vb            | 0.00284  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.167    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.00945  |\n",
      "| mse           | 0.00931  |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.92e+03 |\n",
      "| step          | 2.23e+03 |\n",
      "| vb            | 0.000136 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.275    |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.0109   |\n",
      "| mse           | 0.0107   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 8.96e+03 |\n",
      "| step          | 2.24e+03 |\n",
      "| vb            | 0.000166 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.22     |\n",
      "| lg_loss_scale | 19.2     |\n",
      "| loss          | 0.00991  |\n",
      "| mse           | 0.00975  |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 9e+03    |\n",
      "| step          | 2.25e+03 |\n",
      "| vb            | 0.000164 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.141    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0106   |\n",
      "| mse           | 0.0104   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 9.04e+03 |\n",
      "| step          | 2.26e+03 |\n",
      "| vb            | 0.000158 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.223    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0127   |\n",
      "| mse           | 0.0123   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 9.08e+03 |\n",
      "| step          | 2.27e+03 |\n",
      "| vb            | 0.000407 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.156    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0114   |\n",
      "| mse           | 0.0111   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 9.12e+03 |\n",
      "| step          | 2.28e+03 |\n",
      "| vb            | 0.000279 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.411    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0194   |\n",
      "| mse           | 0.0166   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 9.16e+03 |\n",
      "| step          | 2.29e+03 |\n",
      "| vb            | 0.00287  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.349    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0152   |\n",
      "| mse           | 0.0144   |\n",
      "| param_norm    | 142      |\n",
      "| samples       | 9.2e+03  |\n",
      "| step          | 2.3e+03  |\n",
      "| vb            | 0.00076  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.324    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0159   |\n",
      "| mse           | 0.0154   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.24e+03 |\n",
      "| step          | 2.31e+03 |\n",
      "| vb            | 0.000471 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.213    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0112   |\n",
      "| mse           | 0.0111   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.28e+03 |\n",
      "| step          | 2.32e+03 |\n",
      "| vb            | 0.000123 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.251    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.013    |\n",
      "| mse           | 0.0121   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.32e+03 |\n",
      "| step          | 2.33e+03 |\n",
      "| vb            | 0.000906 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.161    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0103   |\n",
      "| mse           | 0.00997  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.36e+03 |\n",
      "| step          | 2.34e+03 |\n",
      "| vb            | 0.000355 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.116    |\n",
      "| lg_loss_scale | 19.3     |\n",
      "| loss          | 0.0105   |\n",
      "| mse           | 0.0103   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.4e+03  |\n",
      "| step          | 2.35e+03 |\n",
      "| vb            | 0.000193 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.181    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0122   |\n",
      "| mse           | 0.0111   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.44e+03 |\n",
      "| step          | 2.36e+03 |\n",
      "| vb            | 0.00106  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.146    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.00879  |\n",
      "| mse           | 0.00871  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.48e+03 |\n",
      "| step          | 2.37e+03 |\n",
      "| vb            | 8.03e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.158    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.00948  |\n",
      "| mse           | 0.00941  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.52e+03 |\n",
      "| step          | 2.38e+03 |\n",
      "| vb            | 7.62e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.196    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0119   |\n",
      "| mse           | 0.0117   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.56e+03 |\n",
      "| step          | 2.39e+03 |\n",
      "| vb            | 0.000252 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.174    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0113   |\n",
      "| mse           | 0.0109   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.6e+03  |\n",
      "| step          | 2.4e+03  |\n",
      "| vb            | 0.000474 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.301    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0187   |\n",
      "| mse           | 0.0163   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.64e+03 |\n",
      "| step          | 2.41e+03 |\n",
      "| vb            | 0.0024   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.485    |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.0433   |\n",
      "| mse           | 0.0173   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.68e+03 |\n",
      "| step          | 2.42e+03 |\n",
      "| vb            | 0.026    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.38     |\n",
      "| lg_loss_scale | 19.4     |\n",
      "| loss          | 0.144    |\n",
      "| mse           | 0.0146   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.72e+03 |\n",
      "| step          | 2.43e+03 |\n",
      "| vb            | 0.129    |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 18.434000000002975\n",
      "----------------------------\n",
      "| grad_norm     | 0.253    |\n",
      "| lg_loss_scale | 19.1     |\n",
      "| loss          | 0.0416   |\n",
      "| mse           | 0.0253   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.76e+03 |\n",
      "| step          | 2.44e+03 |\n",
      "| vb            | 0.0163   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.219    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | 0.0128   |\n",
      "| mse           | 0.0126   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.8e+03  |\n",
      "| step          | 2.45e+03 |\n",
      "| vb            | 0.000177 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.147    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.00994  |\n",
      "| mse           | 0.00984  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.84e+03 |\n",
      "| step          | 2.46e+03 |\n",
      "| vb            | 0.000101 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.36     |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0195   |\n",
      "| mse           | 0.0136   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.88e+03 |\n",
      "| step          | 2.47e+03 |\n",
      "| vb            | 0.0059   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.331    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0126   |\n",
      "| mse           | 0.0121   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.92e+03 |\n",
      "| step          | 2.48e+03 |\n",
      "| vb            | 0.000483 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.287    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0125   |\n",
      "| mse           | 0.0122   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 9.96e+03 |\n",
      "| step          | 2.49e+03 |\n",
      "| vb            | 0.000301 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.183    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.00915  |\n",
      "| mse           | 0.00901  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1e+04    |\n",
      "| step          | 2.5e+03  |\n",
      "| vb            | 0.000139 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.387    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.129    |\n",
      "| mse           | 0.02     |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1e+04    |\n",
      "| step          | 2.51e+03 |\n",
      "| vb            | 0.109    |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.262    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0131   |\n",
      "| mse           | 0.0129   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.01e+04 |\n",
      "| step          | 2.52e+03 |\n",
      "| vb            | 0.000198 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.188    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0107   |\n",
      "| mse           | 0.0105   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.01e+04 |\n",
      "| step          | 2.53e+03 |\n",
      "| vb            | 0.000191 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.147    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0105   |\n",
      "| mse           | 0.0101   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.02e+04 |\n",
      "| step          | 2.54e+03 |\n",
      "| vb            | 0.000445 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.428    |\n",
      "| lg_loss_scale | 18.5     |\n",
      "| loss          | 0.0224   |\n",
      "| mse           | 0.0196   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.02e+04 |\n",
      "| step          | 2.55e+03 |\n",
      "| vb            | 0.00277  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.289    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0123   |\n",
      "| mse           | 0.0118   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.02e+04 |\n",
      "| step          | 2.56e+03 |\n",
      "| vb            | 0.000508 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.372    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0226   |\n",
      "| mse           | 0.0169   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.03e+04 |\n",
      "| step          | 2.57e+03 |\n",
      "| vb            | 0.00564  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.259    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0107   |\n",
      "| mse           | 0.0104   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.03e+04 |\n",
      "| step          | 2.58e+03 |\n",
      "| vb            | 0.000243 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.245    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0108   |\n",
      "| mse           | 0.0104   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.04e+04 |\n",
      "| step          | 2.59e+03 |\n",
      "| vb            | 0.000416 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.259    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.00995  |\n",
      "| mse           | 0.00975  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.04e+04 |\n",
      "| step          | 2.6e+03  |\n",
      "| vb            | 0.000195 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.281    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0153   |\n",
      "| mse           | 0.0143   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.04e+04 |\n",
      "| step          | 2.61e+03 |\n",
      "| vb            | 0.00102  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.293    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.00877  |\n",
      "| mse           | 0.00869  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.05e+04 |\n",
      "| step          | 2.62e+03 |\n",
      "| vb            | 8.65e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.224    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.00987  |\n",
      "| mse           | 0.00964  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.05e+04 |\n",
      "| step          | 2.63e+03 |\n",
      "| vb            | 0.000232 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.156    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.0109   |\n",
      "| mse           | 0.0101   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.06e+04 |\n",
      "| step          | 2.64e+03 |\n",
      "| vb            | 0.000782 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.114    |\n",
      "| lg_loss_scale | 18.6     |\n",
      "| loss          | 0.00897  |\n",
      "| mse           | 0.00881  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.06e+04 |\n",
      "| step          | 2.65e+03 |\n",
      "| vb            | 0.000157 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.114    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0078   |\n",
      "| mse           | 0.0077   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.06e+04 |\n",
      "| step          | 2.66e+03 |\n",
      "| vb            | 0.000102 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.15     |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0109   |\n",
      "| mse           | 0.0102   |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.07e+04 |\n",
      "| step          | 2.67e+03 |\n",
      "| vb            | 0.000707 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.121    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.00847  |\n",
      "| mse           | 0.00813  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.07e+04 |\n",
      "| step          | 2.68e+03 |\n",
      "| vb            | 0.000337 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.0937   |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.00735  |\n",
      "| mse           | 0.00728  |\n",
      "| param_norm    | 143      |\n",
      "| samples       | 1.08e+04 |\n",
      "| step          | 2.69e+03 |\n",
      "| vb            | 6.77e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.121    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0108   |\n",
      "| mse           | 0.00983  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.08e+04 |\n",
      "| step          | 2.7e+03  |\n",
      "| vb            | 0.000976 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.0956   |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.00749  |\n",
      "| mse           | 0.0074   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.08e+04 |\n",
      "| step          | 2.71e+03 |\n",
      "| vb            | 8.94e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.266    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.0455   |\n",
      "| mse           | 0.0172   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.09e+04 |\n",
      "| step          | 2.72e+03 |\n",
      "| vb            | 0.0283   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.14     |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.00889  |\n",
      "| mse           | 0.00876  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.09e+04 |\n",
      "| step          | 2.73e+03 |\n",
      "| vb            | 0.000136 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.171    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.00806  |\n",
      "| mse           | 0.0079   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.1e+04  |\n",
      "| step          | 2.74e+03 |\n",
      "| vb            | 0.00016  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.123    |\n",
      "| lg_loss_scale | 18.7     |\n",
      "| loss          | 0.00712  |\n",
      "| mse           | 0.00706  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.1e+04  |\n",
      "| step          | 2.75e+03 |\n",
      "| vb            | 5.84e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.137    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0072   |\n",
      "| mse           | 0.00713  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.1e+04  |\n",
      "| step          | 2.76e+03 |\n",
      "| vb            | 7.75e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.144    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.00817  |\n",
      "| mse           | 0.00803  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.11e+04 |\n",
      "| step          | 2.77e+03 |\n",
      "| vb            | 0.000141 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.128    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.00743  |\n",
      "| mse           | 0.00733  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.11e+04 |\n",
      "| step          | 2.78e+03 |\n",
      "| vb            | 9.73e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.127    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.00774  |\n",
      "| mse           | 0.00762  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.12e+04 |\n",
      "| step          | 2.79e+03 |\n",
      "| vb            | 0.000125 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.225    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.012    |\n",
      "| mse           | 0.0112   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.12e+04 |\n",
      "| step          | 2.8e+03  |\n",
      "| vb            | 0.000755 |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.255    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.00662  |\n",
      "| mse           | 0.00652  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.12e+04 |\n",
      "| step          | 2.81e+03 |\n",
      "| vb            | 9.78e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.21     |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.00811  |\n",
      "| mse           | 0.008    |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.13e+04 |\n",
      "| step          | 2.82e+03 |\n",
      "| vb            | 0.000111 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.167    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0121   |\n",
      "| mse           | 0.0107   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.13e+04 |\n",
      "| step          | 2.83e+03 |\n",
      "| vb            | 0.00142  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.21     |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.0125   |\n",
      "| mse           | 0.0119   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.14e+04 |\n",
      "| step          | 2.84e+03 |\n",
      "| vb            | 0.000571 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.135    |\n",
      "| lg_loss_scale | 18.8     |\n",
      "| loss          | 0.00738  |\n",
      "| mse           | 0.00731  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.14e+04 |\n",
      "| step          | 2.85e+03 |\n",
      "| vb            | 6.86e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.23     |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0141   |\n",
      "| mse           | 0.0131   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.14e+04 |\n",
      "| step          | 2.86e+03 |\n",
      "| vb            | 0.00103  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.213    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.0115   |\n",
      "| mse           | 0.0109   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.15e+04 |\n",
      "| step          | 2.87e+03 |\n",
      "| vb            | 0.00061  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.213    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.00811  |\n",
      "| mse           | 0.0079   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.15e+04 |\n",
      "| step          | 2.88e+03 |\n",
      "| vb            | 0.000206 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.175    |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.00783  |\n",
      "| mse           | 0.00771  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.16e+04 |\n",
      "| step          | 2.89e+03 |\n",
      "| vb            | 0.000116 |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 17.891000000003533\n",
      "----------------------------\n",
      "| grad_norm     | 0.132    |\n",
      "| lg_loss_scale | 18.4     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.16e+04 |\n",
      "| step          | 2.9e+03  |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "----------------------------\n",
      "| grad_norm     | 0.127    |\n",
      "| lg_loss_scale | 17.9     |\n",
      "| loss          | 0.00653  |\n",
      "| mse           | 0.00641  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.16e+04 |\n",
      "| step          | 2.91e+03 |\n",
      "| vb            | 0.000118 |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 16.910000000003556\n",
      "----------------------------\n",
      "| grad_norm     | 0.163    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.17e+04 |\n",
      "| step          | 2.92e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.092    |\n",
      "| lg_loss_scale | 16.9     |\n",
      "| loss          | 0.0059   |\n",
      "| mse           | 0.00585  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.17e+04 |\n",
      "| step          | 2.93e+03 |\n",
      "| vb            | 4.19e-05 |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.096    |\n",
      "| lg_loss_scale | 16.9     |\n",
      "| loss          | 0.00683  |\n",
      "| mse           | 0.00672  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.18e+04 |\n",
      "| step          | 2.94e+03 |\n",
      "| vb            | 0.00011  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.339    |\n",
      "| lg_loss_scale | 16.9     |\n",
      "| loss          | 0.0153   |\n",
      "| mse           | 0.0142   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.18e+04 |\n",
      "| step          | 2.95e+03 |\n",
      "| vb            | 0.00108  |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 15.949000000003604\n",
      "----------------------------\n",
      "| grad_norm     | 0.278    |\n",
      "| lg_loss_scale | 16.4     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.18e+04 |\n",
      "| step          | 2.96e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 14.958000000003599\n",
      "Found NaN, decreased lg_loss_scale to 13.962000000003597\n",
      "----------------------------\n",
      "| grad_norm     | 0.17     |\n",
      "| lg_loss_scale | 15.5     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.19e+04 |\n",
      "| step          | 2.97e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.138    |\n",
      "| lg_loss_scale | 14       |\n",
      "| loss          | 0.00987  |\n",
      "| mse           | 0.00961  |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.19e+04 |\n",
      "| step          | 2.98e+03 |\n",
      "| vb            | 0.000256 |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 12.97500000000359\n",
      "----------------------------\n",
      "| grad_norm     | 0.141    |\n",
      "| lg_loss_scale | 13.4     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.2e+04  |\n",
      "| step          | 2.99e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 11.989000000003582\n",
      "Found NaN, decreased lg_loss_scale to 10.989000000003582\n",
      "----------------------------\n",
      "| grad_norm     | 0.198    |\n",
      "| lg_loss_scale | 12.9     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.2e+04  |\n",
      "| step          | 3e+03    |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Found NaN, decreased lg_loss_scale to 9.989000000003582\n",
      "Found NaN, decreased lg_loss_scale to 8.990000000003581\n",
      "Found NaN, decreased lg_loss_scale to 7.99300000000358\n",
      "Found NaN, decreased lg_loss_scale to 6.99500000000358\n",
      "----------------------------\n",
      "| grad_norm     | 0.218    |\n",
      "| lg_loss_scale | 9.09     |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.2e+04  |\n",
      "| step          | 3.01e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 5.99500000000358\n",
      "Found NaN, decreased lg_loss_scale to 4.997000000003581\n",
      "Found NaN, decreased lg_loss_scale to 3.9990000000035817\n",
      "Found NaN, decreased lg_loss_scale to 3.0010000000035824\n",
      "----------------------------\n",
      "| grad_norm     | 0.182    |\n",
      "| lg_loss_scale | 5.2      |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.21e+04 |\n",
      "| step          | 3.02e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 2.004000000003582\n",
      "Found NaN, decreased lg_loss_scale to 1.0090000000035815\n",
      "----------------------------\n",
      "| grad_norm     | 0.153    |\n",
      "| lg_loss_scale | 2.4      |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.21e+04 |\n",
      "| step          | 3.03e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 0.009000000003581476\n",
      "Found NaN, decreased lg_loss_scale to -0.9879999999964185\n",
      "Found NaN, decreased lg_loss_scale to -1.9879999999964184\n",
      "Found NaN, decreased lg_loss_scale to -2.9869999999964185\n",
      "----------------------------\n",
      "| grad_norm     | 0.201    |\n",
      "| lg_loss_scale | -0.988   |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.22e+04 |\n",
      "| step          | 3.04e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to -3.980999999996419\n",
      "Found NaN, decreased lg_loss_scale to -4.976999999996419\n",
      "----------------------------\n",
      "| grad_norm     | 0.242    |\n",
      "| lg_loss_scale | -3.48    |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.22e+04 |\n",
      "| step          | 3.05e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to -5.976999999996419\n",
      "Found NaN, decreased lg_loss_scale to -6.972999999996418\n",
      "Found NaN, decreased lg_loss_scale to -7.9719999999964175\n",
      "----------------------------\n",
      "| grad_norm     | 0.211    |\n",
      "| lg_loss_scale | -6.47    |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.22e+04 |\n",
      "| step          | 3.06e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to -8.969999999996418\n",
      "Found NaN, decreased lg_loss_scale to -9.968999999996418\n",
      "----------------------------\n",
      "| grad_norm     | 0.271    |\n",
      "| lg_loss_scale | -9.57    |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.23e+04 |\n",
      "| step          | 3.07e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| grad_norm     | 0.283    |\n",
      "| lg_loss_scale | -9.96    |\n",
      "| loss          | 0.0197   |\n",
      "| mse           | 0.0148   |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.23e+04 |\n",
      "| step          | 3.08e+03 |\n",
      "| vb            | 0.00493  |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to -10.951999999996428\n",
      "Found NaN, decreased lg_loss_scale to -11.951999999996428\n",
      "Found NaN, decreased lg_loss_scale to -12.950999999996428\n",
      "Found NaN, decreased lg_loss_scale to -13.950999999996428\n",
      "----------------------------\n",
      "| grad_norm     | 0.178    |\n",
      "| lg_loss_scale | -12.8    |\n",
      "| loss          | nan      |\n",
      "| mse           | nan      |\n",
      "| param_norm    | 144      |\n",
      "| samples       | 1.24e+04 |\n",
      "| step          | 3.09e+03 |\n",
      "| vb            | nan      |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to -14.941999999996433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOPODIFF_LOGDIR'] = './checkpoints/3d_diff_logdir_optimized_128'\n",
    "\n",
    "# 최적화된 학습 파라미터\n",
    "TRAIN_FLAGS = \"\"\"\n",
    "--batch_size 4\n",
    "--save_interval 100 \n",
    "--use_fp16 True \n",
    "--lr 1e-4 \n",
    "--weight_decay 0.01\n",
    "--ema_rate 0.9999\n",
    "--log_interval 10\n",
    "--microbatch 2\n",
    "--schedule_sampler uniform\n",
    "--resume_checkpoint \"\"\n",
    "--num_workers 4\n",
    "\"\"\"\n",
    "\n",
    "# 3D에 최적화된 모델 파라미터\n",
    "MODEL_FLAGS = \"\"\"\n",
    "--image_size 64 \n",
    "--num_channels 64\n",
    "--num_res_blocks 2 \n",
    "--learn_sigma True \n",
    "--dropout 0.1 \n",
    "--use_checkpoint True\n",
    "--attention_resolutions 16,8\n",
    "--channel_mult 1,2,3,4\n",
    "--use_scale_shift_norm True\n",
    "--resblock_updown False\n",
    "--dims 3\n",
    "--in_channels 1\n",
    "\"\"\"\n",
    "\n",
    "# Diffusion 파라미터\n",
    "DIFFUSION_FLAGS = \"\"\"\n",
    "--diffusion_steps 1000 \n",
    "--noise_schedule cosine\n",
    "--use_kl False\n",
    "--predict_xstart False\n",
    "--rescale_timesteps False\n",
    "\"\"\"\n",
    "\n",
    "# 데이터 경로\n",
    "DATA_FLAGS = \"--data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\"\n",
    "\n",
    "# 실행\n",
    "%run scripts/image_train.py $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS $DATA_FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23040d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./checkpoints/3d_diff_logdir8\n",
      "Creating model and diffusion...\n",
      "Model parameters: 49,420,546\n",
      "Input shape: [4, 3, 64, 64, 64]\n",
      "Conditioning: VF range=(0.0, 1.0), YM range=(0.0, 1600.0)\n",
      "Creating data loader...\n",
      "Starting training...\n",
      "TRAIN: 5823 files\n",
      "VF range: (0.0, 1.0), YM range: (0.0, 1600.0)\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.242    |\n",
      "| cond_ym_mean  | 0.0798   |\n",
      "| grad_norm     | 19.1     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 1.15     |\n",
      "| loss_q0       | 1.02     |\n",
      "| loss_q3       | 1.29     |\n",
      "| mse           | 1        |\n",
      "| mse_q0        | 0.999    |\n",
      "| mse_q3        | 1        |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 4        |\n",
      "| step          | 0        |\n",
      "| vb            | 0.153    |\n",
      "| vb_q0         | 0.0198   |\n",
      "| vb_q3         | 0.286    |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.304    |\n",
      "| cond_ym_mean  | 0.122    |\n",
      "| grad_norm     | 25.5     |\n",
      "| lg_loss_scale | 20       |\n",
      "| loss          | 0.876    |\n",
      "| loss_q0       | 0.866    |\n",
      "| loss_q1       | 0.88     |\n",
      "| loss_q2       | 0.864    |\n",
      "| loss_q3       | 0.897    |\n",
      "| mse           | 0.868    |\n",
      "| mse_q0        | 0.856    |\n",
      "| mse_q1        | 0.875    |\n",
      "| mse_q2        | 0.86     |\n",
      "| mse_q3        | 0.88     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 44       |\n",
      "| step          | 10       |\n",
      "| vb            | 0.008    |\n",
      "| vb_q0         | 0.0104   |\n",
      "| vb_q1         | 0.00442  |\n",
      "| vb_q2         | 0.00456  |\n",
      "| vb_q3         | 0.017    |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 19.019000000000023\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.289    |\n",
      "| cond_ym_mean  | 0.105    |\n",
      "| grad_norm     | 20.4     |\n",
      "| lg_loss_scale | 19.9     |\n",
      "| loss          | 0.636    |\n",
      "| loss_q0       | 0.66     |\n",
      "| loss_q1       | 0.618    |\n",
      "| loss_q2       | 0.619    |\n",
      "| loss_q3       | 0.656    |\n",
      "| mse           | 0.627    |\n",
      "| mse_q0        | 0.652    |\n",
      "| mse_q1        | 0.615    |\n",
      "| mse_q2        | 0.616    |\n",
      "| mse_q3        | 0.622    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 84       |\n",
      "| step          | 20       |\n",
      "| vb            | 0.00841  |\n",
      "| vb_q0         | 0.00826  |\n",
      "| vb_q1         | 0.00305  |\n",
      "| vb_q2         | 0.00303  |\n",
      "| vb_q3         | 0.0334   |\n",
      "----------------------------\n",
      "Found NaN, decreased lg_loss_scale to 18.028000000000034\n",
      "Found NaN, decreased lg_loss_scale to 17.028000000000034\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.338    |\n",
      "| cond_ym_mean  | 0.142    |\n",
      "| grad_norm     | 14.7     |\n",
      "| lg_loss_scale | 18.9     |\n",
      "| loss          | 0.483    |\n",
      "| loss_q0       | 0.521    |\n",
      "| loss_q1       | 0.46     |\n",
      "| loss_q2       | 0.455    |\n",
      "| loss_q3       | 0.486    |\n",
      "| mse           | 0.474    |\n",
      "| mse_q0        | 0.512    |\n",
      "| mse_q1        | 0.458    |\n",
      "| mse_q2        | 0.453    |\n",
      "| mse_q3        | 0.467    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 124      |\n",
      "| step          | 30       |\n",
      "| vb            | 0.00922  |\n",
      "| vb_q0         | 0.0089   |\n",
      "| vb_q1         | 0.00221  |\n",
      "| vb_q2         | 0.00226  |\n",
      "| vb_q3         | 0.0186   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.286    |\n",
      "| cond_ym_mean  | 0.1      |\n",
      "| grad_norm     | 11.1     |\n",
      "| lg_loss_scale | 17       |\n",
      "| loss          | 0.391    |\n",
      "| loss_q0       | 0.45     |\n",
      "| loss_q1       | 0.368    |\n",
      "| loss_q2       | 0.377    |\n",
      "| loss_q3       | 0.363    |\n",
      "| mse           | 0.386    |\n",
      "| mse_q0        | 0.44     |\n",
      "| mse_q1        | 0.366    |\n",
      "| mse_q2        | 0.375    |\n",
      "| mse_q3        | 0.358    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 164      |\n",
      "| step          | 40       |\n",
      "| vb            | 0.00508  |\n",
      "| vb_q0         | 0.0105   |\n",
      "| vb_q1         | 0.00179  |\n",
      "| vb_q2         | 0.00188  |\n",
      "| vb_q3         | 0.00528  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.291    |\n",
      "| cond_ym_mean  | 0.117    |\n",
      "| grad_norm     | 7.52     |\n",
      "| lg_loss_scale | 17       |\n",
      "| loss          | 0.314    |\n",
      "| loss_q0       | 0.342    |\n",
      "| loss_q1       | 0.296    |\n",
      "| loss_q2       | 0.301    |\n",
      "| loss_q3       | 0.294    |\n",
      "| mse           | 0.31     |\n",
      "| mse_q0        | 0.338    |\n",
      "| mse_q1        | 0.295    |\n",
      "| mse_q2        | 0.299    |\n",
      "| mse_q3        | 0.285    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 204      |\n",
      "| step          | 50       |\n",
      "| vb            | 0.00409  |\n",
      "| vb_q0         | 0.00457  |\n",
      "| vb_q1         | 0.00149  |\n",
      "| vb_q2         | 0.00142  |\n",
      "| vb_q3         | 0.00879  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.286    |\n",
      "| cond_ym_mean  | 0.106    |\n",
      "| grad_norm     | 7.22     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.287    |\n",
      "| loss_q0       | 0.371    |\n",
      "| loss_q1       | 0.26     |\n",
      "| loss_q2       | 0.26     |\n",
      "| loss_q3       | 0.268    |\n",
      "| mse           | 0.281    |\n",
      "| mse_q0        | 0.356    |\n",
      "| mse_q1        | 0.259    |\n",
      "| mse_q2        | 0.259    |\n",
      "| mse_q3        | 0.259    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 244      |\n",
      "| step          | 60       |\n",
      "| vb            | 0.00597  |\n",
      "| vb_q0         | 0.0156   |\n",
      "| vb_q1         | 0.00131  |\n",
      "| vb_q2         | 0.00136  |\n",
      "| vb_q3         | 0.00845  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.291    |\n",
      "| cond_ym_mean  | 0.108    |\n",
      "| grad_norm     | 5.59     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.268    |\n",
      "| loss_q0       | 0.332    |\n",
      "| loss_q1       | 0.242    |\n",
      "| loss_q2       | 0.235    |\n",
      "| loss_q3       | 0.235    |\n",
      "| mse           | 0.259    |\n",
      "| mse_q0        | 0.308    |\n",
      "| mse_q1        | 0.241    |\n",
      "| mse_q2        | 0.234    |\n",
      "| mse_q3        | 0.234    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 284      |\n",
      "| step          | 70       |\n",
      "| vb            | 0.00859  |\n",
      "| vb_q0         | 0.0237   |\n",
      "| vb_q1         | 0.00116  |\n",
      "| vb_q2         | 0.00114  |\n",
      "| vb_q3         | 0.00197  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.299    |\n",
      "| cond_ym_mean  | 0.117    |\n",
      "| grad_norm     | 3.64     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.235    |\n",
      "| loss_q0       | 0.257    |\n",
      "| loss_q1       | 0.224    |\n",
      "| loss_q2       | 0.222    |\n",
      "| loss_q3       | 0.225    |\n",
      "| mse           | 0.232    |\n",
      "| mse_q0        | 0.254    |\n",
      "| mse_q1        | 0.223    |\n",
      "| mse_q2        | 0.221    |\n",
      "| mse_q3        | 0.22     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 324      |\n",
      "| step          | 80       |\n",
      "| vb            | 0.00268  |\n",
      "| vb_q0         | 0.00349  |\n",
      "| vb_q1         | 0.00112  |\n",
      "| vb_q2         | 0.00111  |\n",
      "| vb_q3         | 0.00508  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.28     |\n",
      "| cond_ym_mean  | 0.0972   |\n",
      "| grad_norm     | 2.9      |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.221    |\n",
      "| loss_q0       | 0.248    |\n",
      "| loss_q1       | 0.213    |\n",
      "| loss_q2       | 0.213    |\n",
      "| loss_q3       | 0.215    |\n",
      "| mse           | 0.217    |\n",
      "| mse_q0        | 0.243    |\n",
      "| mse_q1        | 0.212    |\n",
      "| mse_q2        | 0.212    |\n",
      "| mse_q3        | 0.209    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 364      |\n",
      "| step          | 90       |\n",
      "| vb            | 0.00332  |\n",
      "| vb_q0         | 0.00486  |\n",
      "| vb_q1         | 0.00107  |\n",
      "| vb_q2         | 0.00107  |\n",
      "| vb_q3         | 0.00651  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.285    |\n",
      "| cond_ym_mean  | 0.111    |\n",
      "| grad_norm     | 2.2      |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.213    |\n",
      "| loss_q0       | 0.229    |\n",
      "| loss_q1       | 0.205    |\n",
      "| loss_q2       | 0.203    |\n",
      "| loss_q3       | 0.22     |\n",
      "| mse           | 0.206    |\n",
      "| mse_q0        | 0.226    |\n",
      "| mse_q1        | 0.204    |\n",
      "| mse_q2        | 0.202    |\n",
      "| mse_q3        | 0.202    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 404      |\n",
      "| step          | 100      |\n",
      "| vb            | 0.00668  |\n",
      "| vb_q0         | 0.00283  |\n",
      "| vb_q1         | 0.00105  |\n",
      "| vb_q2         | 0.00105  |\n",
      "| vb_q3         | 0.0175   |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.269    |\n",
      "| cond_ym_mean  | 0.0979   |\n",
      "| grad_norm     | 3.22     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.237    |\n",
      "| loss_q0       | 0.307    |\n",
      "| loss_q1       | 0.199    |\n",
      "| loss_q2       | 0.196    |\n",
      "| loss_q3       | 0.207    |\n",
      "| mse           | 0.222    |\n",
      "| mse_q0        | 0.271    |\n",
      "| mse_q1        | 0.198    |\n",
      "| mse_q2        | 0.195    |\n",
      "| mse_q3        | 0.196    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 444      |\n",
      "| step          | 110      |\n",
      "| vb            | 0.0149   |\n",
      "| vb_q0         | 0.0361   |\n",
      "| vb_q1         | 0.00106  |\n",
      "| vb_q2         | 0.000943 |\n",
      "| vb_q3         | 0.0105   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.287    |\n",
      "| cond_ym_mean  | 0.107    |\n",
      "| grad_norm     | 4.19     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.234    |\n",
      "| loss_q0       | 0.367    |\n",
      "| loss_q1       | 0.198    |\n",
      "| loss_q2       | 0.194    |\n",
      "| loss_q3       | 0.196    |\n",
      "| mse           | 0.228    |\n",
      "| mse_q0        | 0.342    |\n",
      "| mse_q1        | 0.197    |\n",
      "| mse_q2        | 0.193    |\n",
      "| mse_q3        | 0.193    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 484      |\n",
      "| step          | 120      |\n",
      "| vb            | 0.00679  |\n",
      "| vb_q0         | 0.025    |\n",
      "| vb_q1         | 0.000989 |\n",
      "| vb_q2         | 0.000989 |\n",
      "| vb_q3         | 0.00323  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.265    |\n",
      "| cond_ym_mean  | 0.0916   |\n",
      "| grad_norm     | 2.03     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.204    |\n",
      "| loss_q0       | 0.223    |\n",
      "| loss_q1       | 0.197    |\n",
      "| loss_q2       | 0.191    |\n",
      "| loss_q3       | 0.2      |\n",
      "| mse           | 0.201    |\n",
      "| mse_q0        | 0.22     |\n",
      "| mse_q1        | 0.196    |\n",
      "| mse_q2        | 0.19     |\n",
      "| mse_q3        | 0.191    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 524      |\n",
      "| step          | 130      |\n",
      "| vb            | 0.00313  |\n",
      "| vb_q0         | 0.00319  |\n",
      "| vb_q1         | 0.00102  |\n",
      "| vb_q2         | 0.000985 |\n",
      "| vb_q3         | 0.00943  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.338    |\n",
      "| cond_ym_mean  | 0.153    |\n",
      "| grad_norm     | 1.49     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.198    |\n",
      "| loss_q0       | 0.234    |\n",
      "| loss_q1       | 0.192    |\n",
      "| loss_q2       | 0.189    |\n",
      "| loss_q3       | 0.19     |\n",
      "| mse           | 0.196    |\n",
      "| mse_q0        | 0.23     |\n",
      "| mse_q1        | 0.191    |\n",
      "| mse_q2        | 0.188    |\n",
      "| mse_q3        | 0.187    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 564      |\n",
      "| step          | 140      |\n",
      "| vb            | 0.00199  |\n",
      "| vb_q0         | 0.00399  |\n",
      "| vb_q1         | 0.000983 |\n",
      "| vb_q2         | 0.000894 |\n",
      "| vb_q3         | 0.0035   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.296    |\n",
      "| cond_ym_mean  | 0.109    |\n",
      "| grad_norm     | 1.74     |\n",
      "| lg_loss_scale | 17.1     |\n",
      "| loss          | 0.206    |\n",
      "| loss_q0       | 0.244    |\n",
      "| loss_q1       | 0.188    |\n",
      "| loss_q2       | 0.186    |\n",
      "| loss_q3       | 0.188    |\n",
      "| mse           | 0.203    |\n",
      "| mse_q0        | 0.236    |\n",
      "| mse_q1        | 0.187    |\n",
      "| mse_q2        | 0.185    |\n",
      "| mse_q3        | 0.185    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 604      |\n",
      "| step          | 150      |\n",
      "| vb            | 0.00339  |\n",
      "| vb_q0         | 0.00772  |\n",
      "| vb_q1         | 0.000948 |\n",
      "| vb_q2         | 0.000945 |\n",
      "| vb_q3         | 0.0029   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.282    |\n",
      "| cond_ym_mean  | 0.108    |\n",
      "| grad_norm     | 1.13     |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.194    |\n",
      "| loss_q0       | 0.216    |\n",
      "| loss_q1       | 0.187    |\n",
      "| loss_q2       | 0.184    |\n",
      "| loss_q3       | 0.191    |\n",
      "| mse           | 0.191    |\n",
      "| mse_q0        | 0.212    |\n",
      "| mse_q1        | 0.186    |\n",
      "| mse_q2        | 0.183    |\n",
      "| mse_q3        | 0.183    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 644      |\n",
      "| step          | 160      |\n",
      "| vb            | 0.0032   |\n",
      "| vb_q0         | 0.00404  |\n",
      "| vb_q1         | 0.000903 |\n",
      "| vb_q2         | 0.000988 |\n",
      "| vb_q3         | 0.0076   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.285    |\n",
      "| cond_ym_mean  | 0.11     |\n",
      "| grad_norm     | 1.13     |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.192    |\n",
      "| loss_q0       | 0.218    |\n",
      "| loss_q1       | 0.186    |\n",
      "| loss_q2       | 0.182    |\n",
      "| loss_q3       | 0.189    |\n",
      "| mse           | 0.189    |\n",
      "| mse_q0        | 0.214    |\n",
      "| mse_q1        | 0.185    |\n",
      "| mse_q2        | 0.181    |\n",
      "| mse_q3        | 0.181    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 684      |\n",
      "| step          | 170      |\n",
      "| vb            | 0.00303  |\n",
      "| vb_q0         | 0.00443  |\n",
      "| vb_q1         | 0.000983 |\n",
      "| vb_q2         | 0.000924 |\n",
      "| vb_q3         | 0.00785  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.341    |\n",
      "| cond_ym_mean  | 0.149    |\n",
      "| grad_norm     | 0.88     |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.186    |\n",
      "| loss_q0       | 0.204    |\n",
      "| loss_q1       | 0.182    |\n",
      "| loss_q2       | 0.181    |\n",
      "| loss_q3       | 0.181    |\n",
      "| mse           | 0.184    |\n",
      "| mse_q0        | 0.201    |\n",
      "| mse_q1        | 0.181    |\n",
      "| mse_q2        | 0.18     |\n",
      "| mse_q3        | 0.179    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 724      |\n",
      "| step          | 180      |\n",
      "| vb            | 0.00194  |\n",
      "| vb_q0         | 0.00323  |\n",
      "| vb_q1         | 0.000902 |\n",
      "| vb_q2         | 0.000923 |\n",
      "| vb_q3         | 0.00259  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.365    |\n",
      "| cond_ym_mean  | 0.172    |\n",
      "| grad_norm     | 1.03     |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.189    |\n",
      "| loss_q0       | 0.215    |\n",
      "| loss_q1       | 0.183    |\n",
      "| loss_q2       | 0.179    |\n",
      "| loss_q3       | 0.18     |\n",
      "| mse           | 0.187    |\n",
      "| mse_q0        | 0.21     |\n",
      "| mse_q1        | 0.182    |\n",
      "| mse_q2        | 0.178    |\n",
      "| mse_q3        | 0.177    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 764      |\n",
      "| step          | 190      |\n",
      "| vb            | 0.00234  |\n",
      "| vb_q0         | 0.00525  |\n",
      "| vb_q1         | 0.000891 |\n",
      "| vb_q2         | 0.000901 |\n",
      "| vb_q3         | 0.0025   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.267    |\n",
      "| cond_ym_mean  | 0.0979   |\n",
      "| grad_norm     | 1.46     |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.194    |\n",
      "| loss_q0       | 0.216    |\n",
      "| loss_q1       | 0.181    |\n",
      "| loss_q2       | 0.179    |\n",
      "| loss_q3       | 0.18     |\n",
      "| mse           | 0.191    |\n",
      "| mse_q0        | 0.211    |\n",
      "| mse_q1        | 0.18     |\n",
      "| mse_q2        | 0.178    |\n",
      "| mse_q3        | 0.176    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 804      |\n",
      "| step          | 200      |\n",
      "| vb            | 0.00321  |\n",
      "| vb_q0         | 0.00477  |\n",
      "| vb_q1         | 0.000948 |\n",
      "| vb_q2         | 0.000965 |\n",
      "| vb_q3         | 0.00395  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.333    |\n",
      "| cond_ym_mean  | 0.141    |\n",
      "| grad_norm     | 0.654    |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.186    |\n",
      "| loss_q0       | 0.192    |\n",
      "| loss_q1       | 0.18     |\n",
      "| loss_q2       | 0.178    |\n",
      "| loss_q3       | 0.189    |\n",
      "| mse           | 0.181    |\n",
      "| mse_q0        | 0.191    |\n",
      "| mse_q1        | 0.179    |\n",
      "| mse_q2        | 0.177    |\n",
      "| mse_q3        | 0.176    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 844      |\n",
      "| step          | 210      |\n",
      "| vb            | 0.00491  |\n",
      "| vb_q0         | 0.00184  |\n",
      "| vb_q1         | 0.000902 |\n",
      "| vb_q2         | 0.000881 |\n",
      "| vb_q3         | 0.0134   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.314    |\n",
      "| cond_ym_mean  | 0.12     |\n",
      "| grad_norm     | 1.17     |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.202    |\n",
      "| loss_q0       | 0.241    |\n",
      "| loss_q1       | 0.178    |\n",
      "| loss_q2       | 0.177    |\n",
      "| loss_q3       | 0.19     |\n",
      "| mse           | 0.193    |\n",
      "| mse_q0        | 0.223    |\n",
      "| mse_q1        | 0.177    |\n",
      "| mse_q2        | 0.177    |\n",
      "| mse_q3        | 0.174    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 884      |\n",
      "| step          | 220      |\n",
      "| vb            | 0.00971  |\n",
      "| vb_q0         | 0.0176   |\n",
      "| vb_q1         | 0.000893 |\n",
      "| vb_q2         | 0.00087  |\n",
      "| vb_q3         | 0.0157   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.323    |\n",
      "| cond_ym_mean  | 0.136    |\n",
      "| grad_norm     | 0.998    |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 3.34     |\n",
      "| loss_q0       | 0.227    |\n",
      "| loss_q1       | 0.18     |\n",
      "| loss_q2       | 0.176    |\n",
      "| loss_q3       | 10.7     |\n",
      "| mse           | 0.186    |\n",
      "| mse_q0        | 0.22     |\n",
      "| mse_q1        | 0.179    |\n",
      "| mse_q2        | 0.175    |\n",
      "| mse_q3        | 0.175    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 924      |\n",
      "| step          | 230      |\n",
      "| vb            | 3.15     |\n",
      "| vb_q0         | 0.00672  |\n",
      "| vb_q1         | 0.00089  |\n",
      "| vb_q2         | 0.00092  |\n",
      "| vb_q3         | 10.5     |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.311    |\n",
      "| cond_ym_mean  | 0.115    |\n",
      "| grad_norm     | 0.785    |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.186    |\n",
      "| loss_q0       | 0.21     |\n",
      "| loss_q1       | 0.18     |\n",
      "| loss_q2       | 0.178    |\n",
      "| loss_q3       | 0.178    |\n",
      "| mse           | 0.184    |\n",
      "| mse_q0        | 0.205    |\n",
      "| mse_q1        | 0.179    |\n",
      "| mse_q2        | 0.177    |\n",
      "| mse_q3        | 0.174    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 964      |\n",
      "| step          | 240      |\n",
      "| vb            | 0.00263  |\n",
      "| vb_q0         | 0.00509  |\n",
      "| vb_q1         | 0.000886 |\n",
      "| vb_q2         | 0.00086  |\n",
      "| vb_q3         | 0.00321  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.29     |\n",
      "| cond_ym_mean  | 0.117    |\n",
      "| grad_norm     | 0.762    |\n",
      "| lg_loss_scale | 17.2     |\n",
      "| loss          | 0.185    |\n",
      "| loss_q0       | 0.209    |\n",
      "| loss_q1       | 0.177    |\n",
      "| loss_q2       | 0.176    |\n",
      "| loss_q3       | 0.178    |\n",
      "| mse           | 0.182    |\n",
      "| mse_q0        | 0.203    |\n",
      "| mse_q1        | 0.176    |\n",
      "| mse_q2        | 0.175    |\n",
      "| mse_q3        | 0.173    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1e+03    |\n",
      "| step          | 250      |\n",
      "| vb            | 0.00343  |\n",
      "| vb_q0         | 0.00582  |\n",
      "| vb_q1         | 0.000838 |\n",
      "| vb_q2         | 0.000954 |\n",
      "| vb_q3         | 0.00458  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.321    |\n",
      "| cond_ym_mean  | 0.125    |\n",
      "| grad_norm     | 0.473    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.19     |\n",
      "| loss_q1       | 0.178    |\n",
      "| loss_q2       | 0.175    |\n",
      "| loss_q3       | 0.176    |\n",
      "| mse           | 0.177    |\n",
      "| mse_q0        | 0.188    |\n",
      "| mse_q1        | 0.177    |\n",
      "| mse_q2        | 0.174    |\n",
      "| mse_q3        | 0.172    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.04e+03 |\n",
      "| step          | 260      |\n",
      "| vb            | 0.002    |\n",
      "| vb_q0         | 0.00239  |\n",
      "| vb_q1         | 0.000922 |\n",
      "| vb_q2         | 0.000858 |\n",
      "| vb_q3         | 0.00371  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.337    |\n",
      "| cond_ym_mean  | 0.144    |\n",
      "| grad_norm     | 0.828    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.189    |\n",
      "| loss_q0       | 0.21     |\n",
      "| loss_q1       | 0.18     |\n",
      "| loss_q2       | 0.174    |\n",
      "| loss_q3       | 0.179    |\n",
      "| mse           | 0.185    |\n",
      "| mse_q0        | 0.205    |\n",
      "| mse_q1        | 0.179    |\n",
      "| mse_q2        | 0.173    |\n",
      "| mse_q3        | 0.173    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.08e+03 |\n",
      "| step          | 270      |\n",
      "| vb            | 0.00383  |\n",
      "| vb_q0         | 0.00523  |\n",
      "| vb_q1         | 0.001    |\n",
      "| vb_q2         | 0.00088  |\n",
      "| vb_q3         | 0.0065   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.27     |\n",
      "| cond_ym_mean  | 0.0922   |\n",
      "| grad_norm     | 0.47     |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.187    |\n",
      "| loss_q1       | 0.177    |\n",
      "| loss_q2       | 0.172    |\n",
      "| loss_q3       | 0.176    |\n",
      "| mse           | 0.177    |\n",
      "| mse_q0        | 0.185    |\n",
      "| mse_q1        | 0.176    |\n",
      "| mse_q2        | 0.171    |\n",
      "| mse_q3        | 0.173    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.12e+03 |\n",
      "| step          | 280      |\n",
      "| vb            | 0.00197  |\n",
      "| vb_q0         | 0.00215  |\n",
      "| vb_q1         | 0.000893 |\n",
      "| vb_q2         | 0.000937 |\n",
      "| vb_q3         | 0.00302  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.33     |\n",
      "| cond_ym_mean  | 0.141    |\n",
      "| grad_norm     | 0.809    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 6.32     |\n",
      "| loss_q0       | 0.223    |\n",
      "| loss_q1       | 0.177    |\n",
      "| loss_q2       | 0.173    |\n",
      "| loss_q3       | 27.4     |\n",
      "| mse           | 0.185    |\n",
      "| mse_q0        | 0.214    |\n",
      "| mse_q1        | 0.176    |\n",
      "| mse_q2        | 0.172    |\n",
      "| mse_q3        | 0.171    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.16e+03 |\n",
      "| step          | 290      |\n",
      "| vb            | 6.14     |\n",
      "| vb_q0         | 0.00831  |\n",
      "| vb_q1         | 0.000906 |\n",
      "| vb_q2         | 0.000889 |\n",
      "| vb_q3         | 27.3     |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.294    |\n",
      "| cond_ym_mean  | 0.116    |\n",
      "| grad_norm     | 0.413    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.19     |\n",
      "| loss_q1       | 0.176    |\n",
      "| loss_q2       | 0.173    |\n",
      "| loss_q3       | 0.174    |\n",
      "| mse           | 0.177    |\n",
      "| mse_q0        | 0.187    |\n",
      "| mse_q1        | 0.175    |\n",
      "| mse_q2        | 0.172    |\n",
      "| mse_q3        | 0.171    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.2e+03  |\n",
      "| step          | 300      |\n",
      "| vb            | 0.00164  |\n",
      "| vb_q0         | 0.00259  |\n",
      "| vb_q1         | 0.000932 |\n",
      "| vb_q2         | 0.000859 |\n",
      "| vb_q3         | 0.00305  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.292    |\n",
      "| cond_ym_mean  | 0.12     |\n",
      "| grad_norm     | 0.411    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.193    |\n",
      "| loss_q1       | 0.174    |\n",
      "| loss_q2       | 0.172    |\n",
      "| loss_q3       | 0.174    |\n",
      "| mse           | 0.177    |\n",
      "| mse_q0        | 0.19     |\n",
      "| mse_q1        | 0.173    |\n",
      "| mse_q2        | 0.171    |\n",
      "| mse_q3        | 0.171    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.24e+03 |\n",
      "| step          | 310      |\n",
      "| vb            | 0.002    |\n",
      "| vb_q0         | 0.00292  |\n",
      "| vb_q1         | 0.000819 |\n",
      "| vb_q2         | 0.000927 |\n",
      "| vb_q3         | 0.00338  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.302    |\n",
      "| cond_ym_mean  | 0.121    |\n",
      "| grad_norm     | 0.333    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.177    |\n",
      "| loss_q0       | 0.197    |\n",
      "| loss_q1       | 0.175    |\n",
      "| loss_q2       | 0.172    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.192    |\n",
      "| mse_q1        | 0.174    |\n",
      "| mse_q2        | 0.171    |\n",
      "| mse_q3        | 0.17     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.28e+03 |\n",
      "| step          | 320      |\n",
      "| vb            | 0.00174  |\n",
      "| vb_q0         | 0.00472  |\n",
      "| vb_q1         | 0.000892 |\n",
      "| vb_q2         | 0.000885 |\n",
      "| vb_q3         | 0.00275  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.351    |\n",
      "| cond_ym_mean  | 0.15     |\n",
      "| grad_norm     | 0.793    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.195    |\n",
      "| loss_q0       | 0.251    |\n",
      "| loss_q1       | 0.175    |\n",
      "| loss_q2       | 0.173    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.185    |\n",
      "| mse_q0        | 0.217    |\n",
      "| mse_q1        | 0.174    |\n",
      "| mse_q2        | 0.172    |\n",
      "| mse_q3        | 0.17     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.32e+03 |\n",
      "| step          | 330      |\n",
      "| vb            | 0.0103   |\n",
      "| vb_q0         | 0.0337   |\n",
      "| vb_q1         | 0.000862 |\n",
      "| vb_q2         | 0.000853 |\n",
      "| vb_q3         | 0.00338  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.318    |\n",
      "| cond_ym_mean  | 0.136    |\n",
      "| grad_norm     | 1.44     |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.186    |\n",
      "| loss_q0       | 0.246    |\n",
      "| loss_q1       | 0.175    |\n",
      "| loss_q2       | 0.172    |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.182    |\n",
      "| mse_q0        | 0.228    |\n",
      "| mse_q1        | 0.174    |\n",
      "| mse_q2        | 0.171    |\n",
      "| mse_q3        | 0.17     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.36e+03 |\n",
      "| step          | 340      |\n",
      "| vb            | 0.00418  |\n",
      "| vb_q0         | 0.0183   |\n",
      "| vb_q1         | 0.000857 |\n",
      "| vb_q2         | 0.00089  |\n",
      "| vb_q3         | 0.00185  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.268    |\n",
      "| cond_ym_mean  | 0.105    |\n",
      "| grad_norm     | 0.822    |\n",
      "| lg_loss_scale | 17.3     |\n",
      "| loss          | 0.181    |\n",
      "| loss_q0       | 0.208    |\n",
      "| loss_q1       | 0.176    |\n",
      "| loss_q2       | 0.173    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.179    |\n",
      "| mse_q0        | 0.205    |\n",
      "| mse_q1        | 0.175    |\n",
      "| mse_q2        | 0.172    |\n",
      "| mse_q3        | 0.17     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.4e+03  |\n",
      "| step          | 350      |\n",
      "| vb            | 0.00205  |\n",
      "| vb_q0         | 0.0037   |\n",
      "| vb_q1         | 0.000912 |\n",
      "| vb_q2         | 0.000813 |\n",
      "| vb_q3         | 0.00259  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.277    |\n",
      "| cond_ym_mean  | 0.108    |\n",
      "| grad_norm     | 0.436    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.175    |\n",
      "| loss_q0       | 0.188    |\n",
      "| loss_q1       | 0.173    |\n",
      "| loss_q2       | 0.171    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.173    |\n",
      "| mse_q0        | 0.185    |\n",
      "| mse_q1        | 0.172    |\n",
      "| mse_q2        | 0.17     |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.44e+03 |\n",
      "| step          | 360      |\n",
      "| vb            | 0.00213  |\n",
      "| vb_q0         | 0.00231  |\n",
      "| vb_q1         | 0.000844 |\n",
      "| vb_q2         | 0.000903 |\n",
      "| vb_q3         | 0.00365  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.332    |\n",
      "| cond_ym_mean  | 0.132    |\n",
      "| grad_norm     | 0.411    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.178    |\n",
      "| loss_q0       | 0.192    |\n",
      "| loss_q1       | 0.175    |\n",
      "| loss_q2       | 0.171    |\n",
      "| loss_q3       | 0.174    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.188    |\n",
      "| mse_q1        | 0.174    |\n",
      "| mse_q2        | 0.171    |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.48e+03 |\n",
      "| step          | 370      |\n",
      "| vb            | 0.00266  |\n",
      "| vb_q0         | 0.00356  |\n",
      "| vb_q1         | 0.000841 |\n",
      "| vb_q2         | 0.000834 |\n",
      "| vb_q3         | 0.00436  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.32     |\n",
      "| cond_ym_mean  | 0.124    |\n",
      "| grad_norm     | 0.348    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.183    |\n",
      "| loss_q1       | 0.173    |\n",
      "| loss_q2       | 0.17     |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.172    |\n",
      "| mse_q0        | 0.181    |\n",
      "| mse_q1        | 0.172    |\n",
      "| mse_q2        | 0.169    |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.52e+03 |\n",
      "| step          | 380      |\n",
      "| vb            | 0.00164  |\n",
      "| vb_q0         | 0.00182  |\n",
      "| vb_q1         | 0.000903 |\n",
      "| vb_q2         | 0.00085  |\n",
      "| vb_q3         | 0.00342  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.32     |\n",
      "| cond_ym_mean  | 0.145    |\n",
      "| grad_norm     | 0.395    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.177    |\n",
      "| loss_q0       | 0.194    |\n",
      "| loss_q1       | 0.173    |\n",
      "| loss_q2       | 0.171    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.19     |\n",
      "| mse_q1        | 0.172    |\n",
      "| mse_q2        | 0.17     |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.56e+03 |\n",
      "| step          | 390      |\n",
      "| vb            | 0.0021   |\n",
      "| vb_q0         | 0.00445  |\n",
      "| vb_q1         | 0.000866 |\n",
      "| vb_q2         | 0.0009   |\n",
      "| vb_q3         | 0.00422  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.279    |\n",
      "| cond_ym_mean  | 0.116    |\n",
      "| grad_norm     | 0.635    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.194    |\n",
      "| loss_q0       | 0.213    |\n",
      "| loss_q1       | 0.172    |\n",
      "| loss_q2       | 0.17     |\n",
      "| loss_q3       | 0.203    |\n",
      "| mse           | 0.181    |\n",
      "| mse_q0        | 0.204    |\n",
      "| mse_q1        | 0.171    |\n",
      "| mse_q2        | 0.169    |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.6e+03  |\n",
      "| step          | 400      |\n",
      "| vb            | 0.0129   |\n",
      "| vb_q0         | 0.00889  |\n",
      "| vb_q1         | 0.00087  |\n",
      "| vb_q2         | 0.000887 |\n",
      "| vb_q3         | 0.0352   |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.301    |\n",
      "| cond_ym_mean  | 0.118    |\n",
      "| grad_norm     | 0.429    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.18     |\n",
      "| loss_q0       | 0.212    |\n",
      "| loss_q1       | 0.174    |\n",
      "| loss_q2       | 0.17     |\n",
      "| loss_q3       | 0.176    |\n",
      "| mse           | 0.176    |\n",
      "| mse_q0        | 0.203    |\n",
      "| mse_q1        | 0.173    |\n",
      "| mse_q2        | 0.169    |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.64e+03 |\n",
      "| step          | 410      |\n",
      "| vb            | 0.00394  |\n",
      "| vb_q0         | 0.00923  |\n",
      "| vb_q1         | 0.000927 |\n",
      "| vb_q2         | 0.000855 |\n",
      "| vb_q3         | 0.00669  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.347    |\n",
      "| cond_ym_mean  | 0.159    |\n",
      "| grad_norm     | 0.925    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.205    |\n",
      "| loss_q0       | 0.268    |\n",
      "| loss_q1       | 0.174    |\n",
      "| loss_q2       | 0.17     |\n",
      "| loss_q3       | 0.204    |\n",
      "| mse           | 0.19     |\n",
      "| mse_q0        | 0.247    |\n",
      "| mse_q1        | 0.173    |\n",
      "| mse_q2        | 0.17     |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.68e+03 |\n",
      "| step          | 420      |\n",
      "| vb            | 0.0155   |\n",
      "| vb_q0         | 0.0209   |\n",
      "| vb_q1         | 0.000816 |\n",
      "| vb_q2         | 0.000821 |\n",
      "| vb_q3         | 0.0358   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.339    |\n",
      "| cond_ym_mean  | 0.141    |\n",
      "| grad_norm     | 0.665    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.185    |\n",
      "| loss_q0       | 0.27     |\n",
      "| loss_q1       | 0.174    |\n",
      "| loss_q2       | 0.172    |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.179    |\n",
      "| mse_q0        | 0.236    |\n",
      "| mse_q1        | 0.174    |\n",
      "| mse_q2        | 0.171    |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.72e+03 |\n",
      "| step          | 430      |\n",
      "| vb            | 0.00603  |\n",
      "| vb_q0         | 0.0348   |\n",
      "| vb_q1         | 0.000861 |\n",
      "| vb_q2         | 0.000851 |\n",
      "| vb_q3         | 0.00372  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.338    |\n",
      "| cond_ym_mean  | 0.148    |\n",
      "| grad_norm     | 0.852    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.184    |\n",
      "| loss_q0       | 0.21     |\n",
      "| loss_q1       | 0.176    |\n",
      "| loss_q2       | 0.171    |\n",
      "| loss_q3       | 0.174    |\n",
      "| mse           | 0.181    |\n",
      "| mse_q0        | 0.204    |\n",
      "| mse_q1        | 0.175    |\n",
      "| mse_q2        | 0.17     |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.76e+03 |\n",
      "| step          | 440      |\n",
      "| vb            | 0.00321  |\n",
      "| vb_q0         | 0.0058   |\n",
      "| vb_q1         | 0.000892 |\n",
      "| vb_q2         | 0.000896 |\n",
      "| vb_q3         | 0.00575  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.328    |\n",
      "| cond_ym_mean  | 0.148    |\n",
      "| grad_norm     | 0.613    |\n",
      "| lg_loss_scale | 17.4     |\n",
      "| loss          | 0.186    |\n",
      "| loss_q0       | 0.202    |\n",
      "| loss_q1       | 0.173    |\n",
      "| loss_q2       | 0.171    |\n",
      "| loss_q3       | 0.197    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.196    |\n",
      "| mse_q1        | 0.172    |\n",
      "| mse_q2        | 0.17     |\n",
      "| mse_q3        | 0.169    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.8e+03  |\n",
      "| step          | 450      |\n",
      "| vb            | 0.0109   |\n",
      "| vb_q0         | 0.00594  |\n",
      "| vb_q1         | 0.000837 |\n",
      "| vb_q2         | 0.000864 |\n",
      "| vb_q3         | 0.0287   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.271    |\n",
      "| cond_ym_mean  | 0.0994   |\n",
      "| grad_norm     | 0.472    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.206    |\n",
      "| loss_q1       | 0.173    |\n",
      "| loss_q2       | 0.169    |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.176    |\n",
      "| mse_q0        | 0.2      |\n",
      "| mse_q1        | 0.172    |\n",
      "| mse_q2        | 0.169    |\n",
      "| mse_q3        | 0.167    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.84e+03 |\n",
      "| step          | 460      |\n",
      "| vb            | 0.0032   |\n",
      "| vb_q0         | 0.00638  |\n",
      "| vb_q1         | 0.000902 |\n",
      "| vb_q2         | 0.000843 |\n",
      "| vb_q3         | 0.00484  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.329    |\n",
      "| cond_ym_mean  | 0.139    |\n",
      "| grad_norm     | 0.333    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.19     |\n",
      "| loss_q1       | 0.174    |\n",
      "| loss_q2       | 0.171    |\n",
      "| loss_q3       | 0.176    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.187    |\n",
      "| mse_q1        | 0.173    |\n",
      "| mse_q2        | 0.17     |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.88e+03 |\n",
      "| step          | 470      |\n",
      "| vb            | 0.00383  |\n",
      "| vb_q0         | 0.00305  |\n",
      "| vb_q1         | 0.000879 |\n",
      "| vb_q2         | 0.000815 |\n",
      "| vb_q3         | 0.00815  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.306    |\n",
      "| cond_ym_mean  | 0.127    |\n",
      "| grad_norm     | 0.372    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.238    |\n",
      "| loss_q1       | 0.173    |\n",
      "| loss_q2       | 0.169    |\n",
      "| loss_q3       | 0.171    |\n",
      "| mse           | 0.176    |\n",
      "| mse_q0        | 0.222    |\n",
      "| mse_q1        | 0.172    |\n",
      "| mse_q2        | 0.169    |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.92e+03 |\n",
      "| step          | 480      |\n",
      "| vb            | 0.0036   |\n",
      "| vb_q0         | 0.0162   |\n",
      "| vb_q1         | 0.000857 |\n",
      "| vb_q2         | 0.000869 |\n",
      "| vb_q3         | 0.0032   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.304    |\n",
      "| cond_ym_mean  | 0.108    |\n",
      "| grad_norm     | 0.231    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.188    |\n",
      "| loss_q1       | 0.172    |\n",
      "| loss_q2       | 0.169    |\n",
      "| loss_q3       | 0.171    |\n",
      "| mse           | 0.172    |\n",
      "| mse_q0        | 0.184    |\n",
      "| mse_q1        | 0.171    |\n",
      "| mse_q2        | 0.168    |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 1.96e+03 |\n",
      "| step          | 490      |\n",
      "| vb            | 0.00245  |\n",
      "| vb_q0         | 0.00423  |\n",
      "| vb_q1         | 0.000874 |\n",
      "| vb_q2         | 0.00087  |\n",
      "| vb_q3         | 0.00301  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.295    |\n",
      "| cond_ym_mean  | 0.107    |\n",
      "| grad_norm     | 0.227    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.173    |\n",
      "| loss_q0       | 0.185    |\n",
      "| loss_q1       | 0.171    |\n",
      "| loss_q2       | 0.169    |\n",
      "| loss_q3       | 0.171    |\n",
      "| mse           | 0.172    |\n",
      "| mse_q0        | 0.182    |\n",
      "| mse_q1        | 0.17     |\n",
      "| mse_q2        | 0.168    |\n",
      "| mse_q3        | 0.168    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2e+03    |\n",
      "| step          | 500      |\n",
      "| vb            | 0.0018   |\n",
      "| vb_q0         | 0.00278  |\n",
      "| vb_q1         | 0.000876 |\n",
      "| vb_q2         | 0.000907 |\n",
      "| vb_q3         | 0.00304  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.333    |\n",
      "| cond_ym_mean  | 0.139    |\n",
      "| grad_norm     | 0.258    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.177    |\n",
      "| loss_q0       | 0.19     |\n",
      "| loss_q1       | 0.171    |\n",
      "| loss_q2       | 0.168    |\n",
      "| loss_q3       | 0.181    |\n",
      "| mse           | 0.172    |\n",
      "| mse_q0        | 0.184    |\n",
      "| mse_q1        | 0.17     |\n",
      "| mse_q2        | 0.168    |\n",
      "| mse_q3        | 0.166    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.04e+03 |\n",
      "| step          | 510      |\n",
      "| vb            | 0.00468  |\n",
      "| vb_q0         | 0.00615  |\n",
      "| vb_q1         | 0.000903 |\n",
      "| vb_q2         | 0.000843 |\n",
      "| vb_q3         | 0.0151   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.353    |\n",
      "| cond_ym_mean  | 0.161    |\n",
      "| grad_norm     | 0.372    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.178    |\n",
      "| loss_q0       | 0.216    |\n",
      "| loss_q1       | 0.17     |\n",
      "| loss_q2       | 0.169    |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.208    |\n",
      "| mse_q1        | 0.17     |\n",
      "| mse_q2        | 0.169    |\n",
      "| mse_q3        | 0.167    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.08e+03 |\n",
      "| step          | 520      |\n",
      "| vb            | 0.00297  |\n",
      "| vb_q0         | 0.00781  |\n",
      "| vb_q1         | 0.000849 |\n",
      "| vb_q2         | 0.000846 |\n",
      "| vb_q3         | 0.00536  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.284    |\n",
      "| cond_ym_mean  | 0.118    |\n",
      "| grad_norm     | 0.477    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.187    |\n",
      "| loss_q0       | 0.219    |\n",
      "| loss_q1       | 0.171    |\n",
      "| loss_q2       | 0.169    |\n",
      "| loss_q3       | 0.17     |\n",
      "| mse           | 0.181    |\n",
      "| mse_q0        | 0.204    |\n",
      "| mse_q1        | 0.17     |\n",
      "| mse_q2        | 0.168    |\n",
      "| mse_q3        | 0.167    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.12e+03 |\n",
      "| step          | 530      |\n",
      "| vb            | 0.00616  |\n",
      "| vb_q0         | 0.015    |\n",
      "| vb_q1         | 0.000841 |\n",
      "| vb_q2         | 0.000882 |\n",
      "| vb_q3         | 0.00243  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.276    |\n",
      "| cond_ym_mean  | 0.0975   |\n",
      "| grad_norm     | 0.281    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.181    |\n",
      "| loss_q1       | 0.17     |\n",
      "| loss_q2       | 0.168    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.171    |\n",
      "| mse_q0        | 0.178    |\n",
      "| mse_q1        | 0.169    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.167    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.16e+03 |\n",
      "| step          | 540      |\n",
      "| vb            | 0.00315  |\n",
      "| vb_q0         | 0.00275  |\n",
      "| vb_q1         | 0.000811 |\n",
      "| vb_q2         | 0.00086  |\n",
      "| vb_q3         | 0.00631  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.265    |\n",
      "| cond_ym_mean  | 0.0878   |\n",
      "| grad_norm     | 0.184    |\n",
      "| lg_loss_scale | 17.5     |\n",
      "| loss          | 0.175    |\n",
      "| loss_q0       | 0.179    |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.168    |\n",
      "| loss_q3       | 0.179    |\n",
      "| mse           | 0.168    |\n",
      "| mse_q0        | 0.178    |\n",
      "| mse_q1        | 0.169    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.166    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.2e+03  |\n",
      "| step          | 550      |\n",
      "| vb            | 0.00634  |\n",
      "| vb_q0         | 0.0016   |\n",
      "| vb_q1         | 0.000835 |\n",
      "| vb_q2         | 0.000895 |\n",
      "| vb_q3         | 0.0135   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.283    |\n",
      "| cond_ym_mean  | 0.101    |\n",
      "| grad_norm     | 0.189    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.182    |\n",
      "| loss_q0       | 0.184    |\n",
      "| loss_q1       | 0.17     |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.208    |\n",
      "| mse           | 0.171    |\n",
      "| mse_q0        | 0.181    |\n",
      "| mse_q1        | 0.169    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.166    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.24e+03 |\n",
      "| step          | 560      |\n",
      "| vb            | 0.0107   |\n",
      "| vb_q0         | 0.00305  |\n",
      "| vb_q1         | 0.000832 |\n",
      "| vb_q2         | 0.000849 |\n",
      "| vb_q3         | 0.0421   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.307    |\n",
      "| cond_ym_mean  | 0.112    |\n",
      "| grad_norm     | 0.22     |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.172    |\n",
      "| loss_q0       | 0.186    |\n",
      "| loss_q1       | 0.17     |\n",
      "| loss_q2       | 0.168    |\n",
      "| loss_q3       | 0.168    |\n",
      "| mse           | 0.17     |\n",
      "| mse_q0        | 0.181    |\n",
      "| mse_q1        | 0.169    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.166    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.28e+03 |\n",
      "| step          | 570      |\n",
      "| vb            | 0.00209  |\n",
      "| vb_q0         | 0.00457  |\n",
      "| vb_q1         | 0.000896 |\n",
      "| vb_q2         | 0.00086  |\n",
      "| vb_q3         | 0.00234  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.335    |\n",
      "| cond_ym_mean  | 0.141    |\n",
      "| grad_norm     | 0.206    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.173    |\n",
      "| loss_q0       | 0.185    |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.168    |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.17     |\n",
      "| mse_q0        | 0.181    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.32e+03 |\n",
      "| step          | 580      |\n",
      "| vb            | 0.00306  |\n",
      "| vb_q0         | 0.00354  |\n",
      "| vb_q1         | 0.000848 |\n",
      "| vb_q2         | 0.000863 |\n",
      "| vb_q3         | 0.00668  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.262    |\n",
      "| cond_ym_mean  | 0.086    |\n",
      "| grad_norm     | 0.176    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.17     |\n",
      "| loss_q0       | 0.182    |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.169    |\n",
      "| mse           | 0.169    |\n",
      "| mse_q0        | 0.178    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.166    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.36e+03 |\n",
      "| step          | 590      |\n",
      "| vb            | 0.00171  |\n",
      "| vb_q0         | 0.00366  |\n",
      "| vb_q1         | 0.000832 |\n",
      "| vb_q2         | 0.000827 |\n",
      "| vb_q3         | 0.00266  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.298    |\n",
      "| cond_ym_mean  | 0.107    |\n",
      "| grad_norm     | 0.227    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.175    |\n",
      "| loss_q0       | 0.198    |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.17     |\n",
      "| mse           | 0.172    |\n",
      "| mse_q0        | 0.191    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.4e+03  |\n",
      "| step          | 600      |\n",
      "| vb            | 0.00311  |\n",
      "| vb_q0         | 0.00712  |\n",
      "| vb_q1         | 0.00082  |\n",
      "| vb_q2         | 0.000792 |\n",
      "| vb_q3         | 0.00473  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.272    |\n",
      "| cond_ym_mean  | 0.0928   |\n",
      "| grad_norm     | 0.243    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.203    |\n",
      "| loss_q1       | 0.168    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.167    |\n",
      "| mse           | 0.172    |\n",
      "| mse_q0        | 0.193    |\n",
      "| mse_q1        | 0.167    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.44e+03 |\n",
      "| step          | 610      |\n",
      "| vb            | 0.00279  |\n",
      "| vb_q0         | 0.0095   |\n",
      "| vb_q1         | 0.000844 |\n",
      "| vb_q2         | 0.000849 |\n",
      "| vb_q3         | 0.0015   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.315    |\n",
      "| cond_ym_mean  | 0.122    |\n",
      "| grad_norm     | 0.281    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.172    |\n",
      "| loss_q0       | 0.187    |\n",
      "| loss_q1       | 0.168    |\n",
      "| loss_q2       | 0.168    |\n",
      "| loss_q3       | 0.167    |\n",
      "| mse           | 0.171    |\n",
      "| mse_q0        | 0.183    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.48e+03 |\n",
      "| step          | 620      |\n",
      "| vb            | 0.00177  |\n",
      "| vb_q0         | 0.00364  |\n",
      "| vb_q1         | 0.000824 |\n",
      "| vb_q2         | 0.000875 |\n",
      "| vb_q3         | 0.00199  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.292    |\n",
      "| cond_ym_mean  | 0.119    |\n",
      "| grad_norm     | 0.455    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.216    |\n",
      "| loss_q0       | 0.36     |\n",
      "| loss_q1       | 0.168    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.17     |\n",
      "| mse           | 0.179    |\n",
      "| mse_q0        | 0.218    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.166    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.52e+03 |\n",
      "| step          | 630      |\n",
      "| vb            | 0.0368   |\n",
      "| vb_q0         | 0.142    |\n",
      "| vb_q1         | 0.000798 |\n",
      "| vb_q2         | 0.000848 |\n",
      "| vb_q3         | 0.00398  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.293    |\n",
      "| cond_ym_mean  | 0.118    |\n",
      "| grad_norm     | 0.568    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.185    |\n",
      "| loss_q0       | 0.215    |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.177    |\n",
      "| mse_q0        | 0.199    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.56e+03 |\n",
      "| step          | 640      |\n",
      "| vb            | 0.00784  |\n",
      "| vb_q0         | 0.0159   |\n",
      "| vb_q1         | 0.000871 |\n",
      "| vb_q2         | 0.000843 |\n",
      "| vb_q3         | 0.00841  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.321    |\n",
      "| cond_ym_mean  | 0.141    |\n",
      "| grad_norm     | 0.391    |\n",
      "| lg_loss_scale | 17.6     |\n",
      "| loss          | 0.173    |\n",
      "| loss_q0       | 0.18     |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.171    |\n",
      "| mse_q0        | 0.178    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.167    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.6e+03  |\n",
      "| step          | 650      |\n",
      "| vb            | 0.00253  |\n",
      "| vb_q0         | 0.00214  |\n",
      "| vb_q1         | 0.000835 |\n",
      "| vb_q2         | 0.000806 |\n",
      "| vb_q3         | 0.00707  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.309    |\n",
      "| cond_ym_mean  | 0.117    |\n",
      "| grad_norm     | 0.469    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.217    |\n",
      "| loss_q0       | 0.361    |\n",
      "| loss_q1       | 0.168    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.171    |\n",
      "| mse           | 0.179    |\n",
      "| mse_q0        | 0.22     |\n",
      "| mse_q1        | 0.167    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.164    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.64e+03 |\n",
      "| step          | 660      |\n",
      "| vb            | 0.0377   |\n",
      "| vb_q0         | 0.14     |\n",
      "| vb_q1         | 0.000837 |\n",
      "| vb_q2         | 0.000904 |\n",
      "| vb_q3         | 0.00618  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.331    |\n",
      "| cond_ym_mean  | 0.152    |\n",
      "| grad_norm     | 0.291    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.185    |\n",
      "| loss_q1       | 0.168    |\n",
      "| loss_q2       | 0.166    |\n",
      "| loss_q3       | 0.172    |\n",
      "| mse           | 0.171    |\n",
      "| mse_q0        | 0.182    |\n",
      "| mse_q1        | 0.167    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.164    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.68e+03 |\n",
      "| step          | 670      |\n",
      "| vb            | 0.00315  |\n",
      "| vb_q0         | 0.00327  |\n",
      "| vb_q1         | 0.000773 |\n",
      "| vb_q2         | 0.000834 |\n",
      "| vb_q3         | 0.00763  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.28     |\n",
      "| cond_ym_mean  | 0.0945   |\n",
      "| grad_norm     | 0.272    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.184    |\n",
      "| loss_q1       | 0.167    |\n",
      "| loss_q2       | 0.167    |\n",
      "| loss_q3       | 0.167    |\n",
      "| mse           | 0.171    |\n",
      "| mse_q0        | 0.181    |\n",
      "| mse_q1        | 0.166    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.165    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.72e+03 |\n",
      "| step          | 680      |\n",
      "| vb            | 0.00206  |\n",
      "| vb_q0         | 0.00361  |\n",
      "| vb_q1         | 0.000823 |\n",
      "| vb_q2         | 0.000825 |\n",
      "| vb_q3         | 0.00168  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.315    |\n",
      "| cond_ym_mean  | 0.127    |\n",
      "| grad_norm     | 0.204    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.172    |\n",
      "| loss_q0       | 0.182    |\n",
      "| loss_q1       | 0.168    |\n",
      "| loss_q2       | 0.166    |\n",
      "| loss_q3       | 0.169    |\n",
      "| mse           | 0.169    |\n",
      "| mse_q0        | 0.178    |\n",
      "| mse_q1        | 0.167    |\n",
      "| mse_q2        | 0.166    |\n",
      "| mse_q3        | 0.164    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.76e+03 |\n",
      "| step          | 690      |\n",
      "| vb            | 0.00274  |\n",
      "| vb_q0         | 0.00374  |\n",
      "| vb_q1         | 0.000919 |\n",
      "| vb_q2         | 0.000842 |\n",
      "| vb_q3         | 0.00445  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.269    |\n",
      "| cond_ym_mean  | 0.0984   |\n",
      "| grad_norm     | 0.316    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.18     |\n",
      "| loss_q0       | 0.222    |\n",
      "| loss_q1       | 0.166    |\n",
      "| loss_q2       | 0.166    |\n",
      "| loss_q3       | 0.166    |\n",
      "| mse           | 0.174    |\n",
      "| mse_q0        | 0.202    |\n",
      "| mse_q1        | 0.166    |\n",
      "| mse_q2        | 0.165    |\n",
      "| mse_q3        | 0.163    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.8e+03  |\n",
      "| step          | 700      |\n",
      "| vb            | 0.00575  |\n",
      "| vb_q0         | 0.0194   |\n",
      "| vb_q1         | 0.000821 |\n",
      "| vb_q2         | 0.000821 |\n",
      "| vb_q3         | 0.00208  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.292    |\n",
      "| cond_ym_mean  | 0.107    |\n",
      "| grad_norm     | 0.323    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.173    |\n",
      "| loss_q0       | 0.191    |\n",
      "| loss_q1       | 0.167    |\n",
      "| loss_q2       | 0.165    |\n",
      "| loss_q3       | 0.168    |\n",
      "| mse           | 0.17     |\n",
      "| mse_q0        | 0.185    |\n",
      "| mse_q1        | 0.166    |\n",
      "| mse_q2        | 0.164    |\n",
      "| mse_q3        | 0.163    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.84e+03 |\n",
      "| step          | 710      |\n",
      "| vb            | 0.00353  |\n",
      "| vb_q0         | 0.00545  |\n",
      "| vb_q1         | 0.000875 |\n",
      "| vb_q2         | 0.000871 |\n",
      "| vb_q3         | 0.00521  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.314    |\n",
      "| cond_ym_mean  | 0.121    |\n",
      "| grad_norm     | 0.399    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.181    |\n",
      "| loss_q0       | 0.198    |\n",
      "| loss_q1       | 0.169    |\n",
      "| loss_q2       | 0.166    |\n",
      "| loss_q3       | 0.176    |\n",
      "| mse           | 0.175    |\n",
      "| mse_q0        | 0.191    |\n",
      "| mse_q1        | 0.168    |\n",
      "| mse_q2        | 0.165    |\n",
      "| mse_q3        | 0.164    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.88e+03 |\n",
      "| step          | 720      |\n",
      "| vb            | 0.00621  |\n",
      "| vb_q0         | 0.00738  |\n",
      "| vb_q1         | 0.000935 |\n",
      "| vb_q2         | 0.000831 |\n",
      "| vb_q3         | 0.0125   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.248    |\n",
      "| cond_ym_mean  | 0.0762   |\n",
      "| grad_norm     | 0.285    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.179    |\n",
      "| loss_q0       | 0.181    |\n",
      "| loss_q1       | 0.167    |\n",
      "| loss_q2       | 0.166    |\n",
      "| loss_q3       | 0.203    |\n",
      "| mse           | 0.168    |\n",
      "| mse_q0        | 0.178    |\n",
      "| mse_q1        | 0.166    |\n",
      "| mse_q2        | 0.165    |\n",
      "| mse_q3        | 0.164    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.92e+03 |\n",
      "| step          | 730      |\n",
      "| vb            | 0.011    |\n",
      "| vb_q0         | 0.00306  |\n",
      "| vb_q1         | 0.000875 |\n",
      "| vb_q2         | 0.000854 |\n",
      "| vb_q3         | 0.0391   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.299    |\n",
      "| cond_ym_mean  | 0.124    |\n",
      "| grad_norm     | 0.363    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 0.186    |\n",
      "| loss_q0       | 0.228    |\n",
      "| loss_q1       | 0.167    |\n",
      "| loss_q2       | 0.165    |\n",
      "| loss_q3       | 0.166    |\n",
      "| mse           | 0.177    |\n",
      "| mse_q0        | 0.202    |\n",
      "| mse_q1        | 0.167    |\n",
      "| mse_q2        | 0.165    |\n",
      "| mse_q3        | 0.164    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 2.96e+03 |\n",
      "| step          | 740      |\n",
      "| vb            | 0.00948  |\n",
      "| vb_q0         | 0.0262   |\n",
      "| vb_q1         | 0.000865 |\n",
      "| vb_q2         | 0.000817 |\n",
      "| vb_q3         | 0.00235  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.328    |\n",
      "| cond_ym_mean  | 0.132    |\n",
      "| grad_norm     | 0.535    |\n",
      "| lg_loss_scale | 17.7     |\n",
      "| loss          | 3.16     |\n",
      "| loss_q0       | 0.253    |\n",
      "| loss_q1       | 0.167    |\n",
      "| loss_q2       | 0.165    |\n",
      "| loss_q3       | 8.64     |\n",
      "| mse           | 0.18     |\n",
      "| mse_q0        | 0.217    |\n",
      "| mse_q1        | 0.166    |\n",
      "| mse_q2        | 0.164    |\n",
      "| mse_q3        | 0.163    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3e+03    |\n",
      "| step          | 750      |\n",
      "| vb            | 2.98     |\n",
      "| vb_q0         | 0.0357   |\n",
      "| vb_q1         | 0.000774 |\n",
      "| vb_q2         | 0.000868 |\n",
      "| vb_q3         | 8.47     |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.324    |\n",
      "| cond_ym_mean  | 0.125    |\n",
      "| grad_norm     | 0.31     |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.172    |\n",
      "| loss_q0       | 0.19     |\n",
      "| loss_q1       | 0.166    |\n",
      "| loss_q2       | 0.164    |\n",
      "| loss_q3       | 0.166    |\n",
      "| mse           | 0.17     |\n",
      "| mse_q0        | 0.185    |\n",
      "| mse_q1        | 0.165    |\n",
      "| mse_q2        | 0.163    |\n",
      "| mse_q3        | 0.162    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.04e+03 |\n",
      "| step          | 760      |\n",
      "| vb            | 0.00237  |\n",
      "| vb_q0         | 0.0049   |\n",
      "| vb_q1         | 0.000802 |\n",
      "| vb_q2         | 0.000779 |\n",
      "| vb_q3         | 0.00379  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.281    |\n",
      "| cond_ym_mean  | 0.103    |\n",
      "| grad_norm     | 0.261    |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.169    |\n",
      "| loss_q0       | 0.19     |\n",
      "| loss_q1       | 0.166    |\n",
      "| loss_q2       | 0.164    |\n",
      "| loss_q3       | 0.165    |\n",
      "| mse           | 0.167    |\n",
      "| mse_q0        | 0.184    |\n",
      "| mse_q1        | 0.165    |\n",
      "| mse_q2        | 0.163    |\n",
      "| mse_q3        | 0.163    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.08e+03 |\n",
      "| step          | 770      |\n",
      "| vb            | 0.00199  |\n",
      "| vb_q0         | 0.00597  |\n",
      "| vb_q1         | 0.000865 |\n",
      "| vb_q2         | 0.000768 |\n",
      "| vb_q3         | 0.00199  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.307    |\n",
      "| cond_ym_mean  | 0.124    |\n",
      "| grad_norm     | 0.283    |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.174    |\n",
      "| loss_q0       | 0.194    |\n",
      "| loss_q1       | 0.165    |\n",
      "| loss_q2       | 0.164    |\n",
      "| loss_q3       | 0.166    |\n",
      "| mse           | 0.17     |\n",
      "| mse_q0        | 0.187    |\n",
      "| mse_q1        | 0.164    |\n",
      "| mse_q2        | 0.163    |\n",
      "| mse_q3        | 0.162    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.12e+03 |\n",
      "| step          | 780      |\n",
      "| vb            | 0.00346  |\n",
      "| vb_q0         | 0.00724  |\n",
      "| vb_q1         | 0.000812 |\n",
      "| vb_q2         | 0.00078  |\n",
      "| vb_q3         | 0.0037   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.305    |\n",
      "| cond_ym_mean  | 0.122    |\n",
      "| grad_norm     | 0.249    |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.172    |\n",
      "| loss_q0       | 0.179    |\n",
      "| loss_q1       | 0.165    |\n",
      "| loss_q2       | 0.164    |\n",
      "| loss_q3       | 0.181    |\n",
      "| mse           | 0.166    |\n",
      "| mse_q0        | 0.176    |\n",
      "| mse_q1        | 0.165    |\n",
      "| mse_q2        | 0.163    |\n",
      "| mse_q3        | 0.162    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.16e+03 |\n",
      "| step          | 790      |\n",
      "| vb            | 0.0055   |\n",
      "| vb_q0         | 0.00334  |\n",
      "| vb_q1         | 0.000796 |\n",
      "| vb_q2         | 0.0008   |\n",
      "| vb_q3         | 0.0192   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.311    |\n",
      "| cond_ym_mean  | 0.128    |\n",
      "| grad_norm     | 0.24     |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.168    |\n",
      "| loss_q0       | 0.18     |\n",
      "| loss_q1       | 0.166    |\n",
      "| loss_q2       | 0.163    |\n",
      "| loss_q3       | 0.163    |\n",
      "| mse           | 0.167    |\n",
      "| mse_q0        | 0.177    |\n",
      "| mse_q1        | 0.165    |\n",
      "| mse_q2        | 0.163    |\n",
      "| mse_q3        | 0.161    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.2e+03  |\n",
      "| step          | 800      |\n",
      "| vb            | 0.00158  |\n",
      "| vb_q0         | 0.00294  |\n",
      "| vb_q1         | 0.000811 |\n",
      "| vb_q2         | 0.000841 |\n",
      "| vb_q3         | 0.00157  |\n",
      "----------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.281    |\n",
      "| cond_ym_mean  | 0.106    |\n",
      "| grad_norm     | 0.27     |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.171    |\n",
      "| loss_q0       | 0.185    |\n",
      "| loss_q1       | 0.165    |\n",
      "| loss_q2       | 0.162    |\n",
      "| loss_q3       | 0.164    |\n",
      "| mse           | 0.168    |\n",
      "| mse_q0        | 0.18     |\n",
      "| mse_q1        | 0.165    |\n",
      "| mse_q2        | 0.161    |\n",
      "| mse_q3        | 0.161    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.24e+03 |\n",
      "| step          | 810      |\n",
      "| vb            | 0.00294  |\n",
      "| vb_q0         | 0.00536  |\n",
      "| vb_q1         | 0.000837 |\n",
      "| vb_q2         | 0.000879 |\n",
      "| vb_q3         | 0.0031   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.299    |\n",
      "| cond_ym_mean  | 0.111    |\n",
      "| grad_norm     | 0.191    |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 3.05     |\n",
      "| loss_q0       | 0.172    |\n",
      "| loss_q1       | 0.164    |\n",
      "| loss_q2       | 0.164    |\n",
      "| loss_q3       | 10.6     |\n",
      "| mse           | 0.165    |\n",
      "| mse_q0        | 0.17     |\n",
      "| mse_q1        | 0.164    |\n",
      "| mse_q2        | 0.163    |\n",
      "| mse_q3        | 0.161    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.28e+03 |\n",
      "| step          | 820      |\n",
      "| vb            | 2.88     |\n",
      "| vb_q0         | 0.00237  |\n",
      "| vb_q1         | 0.000807 |\n",
      "| vb_q2         | 0.000864 |\n",
      "| vb_q3         | 10.5     |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.261    |\n",
      "| cond_ym_mean  | 0.0847   |\n",
      "| grad_norm     | 0.269    |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.172    |\n",
      "| loss_q0       | 0.186    |\n",
      "| loss_q1       | 0.165    |\n",
      "| loss_q2       | 0.163    |\n",
      "| loss_q3       | 0.164    |\n",
      "| mse           | 0.169    |\n",
      "| mse_q0        | 0.181    |\n",
      "| mse_q1        | 0.164    |\n",
      "| mse_q2        | 0.162    |\n",
      "| mse_q3        | 0.16     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.32e+03 |\n",
      "| step          | 830      |\n",
      "| vb            | 0.00301  |\n",
      "| vb_q0         | 0.00493  |\n",
      "| vb_q1         | 0.000809 |\n",
      "| vb_q2         | 0.000818 |\n",
      "| vb_q3         | 0.0037   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.247    |\n",
      "| cond_ym_mean  | 0.0838   |\n",
      "| grad_norm     | 0.187    |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 0.164    |\n",
      "| loss_q0       | 0.169    |\n",
      "| loss_q1       | 0.165    |\n",
      "| loss_q2       | 0.162    |\n",
      "| loss_q3       | 0.163    |\n",
      "| mse           | 0.163    |\n",
      "| mse_q0        | 0.167    |\n",
      "| mse_q1        | 0.164    |\n",
      "| mse_q2        | 0.161    |\n",
      "| mse_q3        | 0.161    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.36e+03 |\n",
      "| step          | 840      |\n",
      "| vb            | 0.00129  |\n",
      "| vb_q0         | 0.00133  |\n",
      "| vb_q1         | 0.000815 |\n",
      "| vb_q2         | 0.000842 |\n",
      "| vb_q3         | 0.00201  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.28     |\n",
      "| cond_ym_mean  | 0.109    |\n",
      "| grad_norm     | 0.23     |\n",
      "| lg_loss_scale | 17.8     |\n",
      "| loss          | 3.05     |\n",
      "| loss_q0       | 0.173    |\n",
      "| loss_q1       | 0.164    |\n",
      "| loss_q2       | 0.162    |\n",
      "| loss_q3       | 13       |\n",
      "| mse           | 0.164    |\n",
      "| mse_q0        | 0.171    |\n",
      "| mse_q1        | 0.163    |\n",
      "| mse_q2        | 0.161    |\n",
      "| mse_q3        | 0.16     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.4e+03  |\n",
      "| step          | 850      |\n",
      "| vb            | 2.88     |\n",
      "| vb_q0         | 0.00269  |\n",
      "| vb_q1         | 0.000824 |\n",
      "| vb_q2         | 0.000802 |\n",
      "| vb_q3         | 12.8     |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.331    |\n",
      "| cond_ym_mean  | 0.148    |\n",
      "| grad_norm     | 0.298    |\n",
      "| lg_loss_scale | 17.9     |\n",
      "| loss          | 0.169    |\n",
      "| loss_q0       | 0.178    |\n",
      "| loss_q1       | 0.164    |\n",
      "| loss_q2       | 0.162    |\n",
      "| loss_q3       | 0.167    |\n",
      "| mse           | 0.166    |\n",
      "| mse_q0        | 0.175    |\n",
      "| mse_q1        | 0.163    |\n",
      "| mse_q2        | 0.161    |\n",
      "| mse_q3        | 0.161    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.44e+03 |\n",
      "| step          | 860      |\n",
      "| vb            | 0.00281  |\n",
      "| vb_q0         | 0.00373  |\n",
      "| vb_q1         | 0.000795 |\n",
      "| vb_q2         | 0.000851 |\n",
      "| vb_q3         | 0.00636  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.289    |\n",
      "| cond_ym_mean  | 0.107    |\n",
      "| grad_norm     | 0.403    |\n",
      "| lg_loss_scale | 17.9     |\n",
      "| loss          | 0.173    |\n",
      "| loss_q0       | 0.201    |\n",
      "| loss_q1       | 0.163    |\n",
      "| loss_q2       | 0.162    |\n",
      "| loss_q3       | 0.163    |\n",
      "| mse           | 0.168    |\n",
      "| mse_q0        | 0.186    |\n",
      "| mse_q1        | 0.163    |\n",
      "| mse_q2        | 0.161    |\n",
      "| mse_q3        | 0.161    |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.48e+03 |\n",
      "| step          | 870      |\n",
      "| vb            | 0.00505  |\n",
      "| vb_q0         | 0.0155   |\n",
      "| vb_q1         | 0.000814 |\n",
      "| vb_q2         | 0.000829 |\n",
      "| vb_q3         | 0.00254  |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.285    |\n",
      "| cond_ym_mean  | 0.0951   |\n",
      "| grad_norm     | 0.215    |\n",
      "| lg_loss_scale | 17.9     |\n",
      "| loss          | 0.164    |\n",
      "| loss_q0       | 0.167    |\n",
      "| loss_q1       | 0.164    |\n",
      "| loss_q2       | 0.162    |\n",
      "| loss_q3       | 0.163    |\n",
      "| mse           | 0.162    |\n",
      "| mse_q0        | 0.166    |\n",
      "| mse_q1        | 0.163    |\n",
      "| mse_q2        | 0.161    |\n",
      "| mse_q3        | 0.16     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.52e+03 |\n",
      "| step          | 880      |\n",
      "| vb            | 0.00156  |\n",
      "| vb_q0         | 0.00143  |\n",
      "| vb_q1         | 0.000811 |\n",
      "| vb_q2         | 0.000816 |\n",
      "| vb_q3         | 0.0027   |\n",
      "----------------------------\n",
      "----------------------------\n",
      "| cond_vf_mean  | 0.321    |\n",
      "| cond_ym_mean  | 0.126    |\n",
      "| grad_norm     | 0.256    |\n",
      "| lg_loss_scale | 17.9     |\n",
      "| loss          | 0.168    |\n",
      "| loss_q0       | 0.174    |\n",
      "| loss_q1       | 0.163    |\n",
      "| loss_q2       | 0.163    |\n",
      "| loss_q3       | 0.173    |\n",
      "| mse           | 0.165    |\n",
      "| mse_q0        | 0.172    |\n",
      "| mse_q1        | 0.163    |\n",
      "| mse_q2        | 0.162    |\n",
      "| mse_q3        | 0.16     |\n",
      "| param_norm    | 132      |\n",
      "| samples       | 3.56e+03 |\n",
      "| step          | 890      |\n",
      "| vb            | 0.00352  |\n",
      "| vb_q0         | 0.00285  |\n",
      "| vb_q1         | 0.000769 |\n",
      "| vb_q2         | 0.000844 |\n",
      "| vb_q3         | 0.0128   |\n",
      "----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/scripts/image_train.py:101\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/scripts/image_train.py:48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data(\n\u001b[1;32m     41\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[1;32m     42\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     43\u001b[0m     image_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mimage_size,\n\u001b[1;32m     44\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mTrainLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmicrobatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmicrobatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:156\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    153\u001b[0m ):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# __getitem__()에서 (volume, cond_dict)을 반환한다고 가정\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     batch, cond_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    158\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdumpkvs()\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:170\u001b[0m, in \u001b[0;36mTrainLoop.run_step\u001b[0;34m(self, batch, cond_dict)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, cond_dict):\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     took_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_trainer\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m took_step:\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:232\u001b[0m, in \u001b[0;36mTrainLoop.forward_backward\u001b[0;34m(self, batch, cond_dict)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# 🔧 로그에 더 자세한 정보 포함\u001b[39;00m\n\u001b[1;32m    223\u001b[0m log_loss_dict(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion,\n\u001b[1;32m    225\u001b[0m     t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     }\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_trainer\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/fp16_util.py:179\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer.backward\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fp16:\n\u001b[1;32m    178\u001b[0m     loss_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg_loss_scale\n\u001b[0;32m--> 179\u001b[0m     \u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_scale\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "# 프로젝트 루트 경로를 정확히 지정\n",
    "sys.path.insert(0, '/home/yeoneung/Euihyun/3D_TPMS_topoDIff')\n",
    "os.environ['TOPODIFF_LOGDIR'] = './checkpoints/3d_diff_logdir8'\n",
    "\n",
    "\n",
    "# 모든 학습 파라미터 포함\n",
    "TRAIN_FLAGS = \"\"\"\n",
    "--batch_size 4\n",
    "--save_interval 100\n",
    "--use_fp16 True \n",
    "--lr 5e-5 \n",
    "--weight_decay 0.01\n",
    "--ema_rate 0.9999\n",
    "--log_interval 10\n",
    "--microbatch 2\n",
    "--schedule_sampler uniform\n",
    "--resume_checkpoint \"\"\n",
    "\"\"\"\n",
    "\n",
    "MODEL_FLAGS = \"--image_size 64 --num_channels 64 --num_res_blocks 2 --learn_sigma True --dropout 0.1 --use_checkpoint True\"\n",
    "DIFFUSION_FLAGS = \"--diffusion_steps 1000 --noise_schedule cosine\"\n",
    "DATA_FLAGS = \"--data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\"\n",
    "\n",
    "%run scripts/image_train.py \\\n",
    "  $MODEL_FLAGS \\\n",
    "  $DIFFUSION_FLAGS \\\n",
    "  $TRAIN_FLAGS \\\n",
    "  --data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e525ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./checkpoints/3d_diff_logdir10\n",
      "Creating model and diffusion...\n",
      "Model parameters: 12,367,298\n",
      "Input shape: [4, 3, 64, 64, 64]\n",
      "Conditioning: VF range=(0.0, 1.0), YM range=(0.0, 1600.0)\n",
      "Creating data loader...\n",
      "Starting training...\n",
      "TRAIN: 5823 files\n",
      "VF range: (0.0, 1.0), YM range: (0.0, 1600.0)\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 20.7     |\n",
      "| loss         | 1        |\n",
      "| loss_q1      | 1        |\n",
      "| loss_q2      | 1.01     |\n",
      "| mse          | 1        |\n",
      "| mse_q1       | 0.996    |\n",
      "| mse_q2       | 1        |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 4        |\n",
      "| step         | 0        |\n",
      "| vb           | 0.00478  |\n",
      "| vb_q1        | 0.00483  |\n",
      "| vb_q2        | 0.00476  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 18.7     |\n",
      "| loss         | 0.964    |\n",
      "| loss_q0      | 0.995    |\n",
      "| loss_q1      | 0.968    |\n",
      "| loss_q2      | 0.934    |\n",
      "| loss_q3      | 0.959    |\n",
      "| mse          | 0.935    |\n",
      "| mse_q0       | 0.947    |\n",
      "| mse_q1       | 0.963    |\n",
      "| mse_q2       | 0.929    |\n",
      "| mse_q3       | 0.908    |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 44       |\n",
      "| step         | 10       |\n",
      "| vb           | 0.0297   |\n",
      "| vb_q0        | 0.0484   |\n",
      "| vb_q1        | 0.00507  |\n",
      "| vb_q2        | 0.0047   |\n",
      "| vb_q3        | 0.0512   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 17.3     |\n",
      "| loss         | 0.823    |\n",
      "| loss_q0      | 0.88     |\n",
      "| loss_q1      | 0.795    |\n",
      "| loss_q2      | 0.803    |\n",
      "| loss_q3      | 0.815    |\n",
      "| mse          | 0.811    |\n",
      "| mse_q0       | 0.853    |\n",
      "| mse_q1       | 0.791    |\n",
      "| mse_q2       | 0.799    |\n",
      "| mse_q3       | 0.8      |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 84       |\n",
      "| step         | 20       |\n",
      "| vb           | 0.0121   |\n",
      "| vb_q0        | 0.027    |\n",
      "| vb_q1        | 0.00401  |\n",
      "| vb_q2        | 0.00396  |\n",
      "| vb_q3        | 0.0144   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 15.5     |\n",
      "| loss         | 0.689    |\n",
      "| loss_q0      | 0.694    |\n",
      "| loss_q1      | 0.692    |\n",
      "| loss_q2      | 0.675    |\n",
      "| loss_q3      | 0.687    |\n",
      "| mse          | 0.683    |\n",
      "| mse_q0       | 0.686    |\n",
      "| mse_q1       | 0.689    |\n",
      "| mse_q2       | 0.672    |\n",
      "| mse_q3       | 0.676    |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 124      |\n",
      "| step         | 30       |\n",
      "| vb           | 0.00627  |\n",
      "| vb_q0        | 0.00817  |\n",
      "| vb_q1        | 0.00344  |\n",
      "| vb_q2        | 0.00327  |\n",
      "| vb_q3        | 0.0109   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 12.8     |\n",
      "| loss         | 0.627    |\n",
      "| loss_q0      | 0.638    |\n",
      "| loss_q1      | 0.584    |\n",
      "| loss_q2      | 0.58     |\n",
      "| loss_q3      | 0.685    |\n",
      "| mse          | 0.585    |\n",
      "| mse_q0       | 0.62     |\n",
      "| mse_q1       | 0.581    |\n",
      "| mse_q2       | 0.577    |\n",
      "| mse_q3       | 0.572    |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 164      |\n",
      "| step         | 40       |\n",
      "| vb           | 0.0419   |\n",
      "| vb_q0        | 0.0184   |\n",
      "| vb_q1        | 0.00284  |\n",
      "| vb_q2        | 0.00285  |\n",
      "| vb_q3        | 0.113    |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 10.5     |\n",
      "| loss         | 0.551    |\n",
      "| loss_q0      | 0.561    |\n",
      "| loss_q1      | 0.489    |\n",
      "| loss_q2      | 0.505    |\n",
      "| loss_q3      | 0.608    |\n",
      "| mse          | 0.511    |\n",
      "| mse_q0       | 0.541    |\n",
      "| mse_q1       | 0.487    |\n",
      "| mse_q2       | 0.502    |\n",
      "| mse_q3       | 0.499    |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 204      |\n",
      "| step         | 50       |\n",
      "| vb           | 0.0401   |\n",
      "| vb_q0        | 0.0197   |\n",
      "| vb_q1        | 0.00251  |\n",
      "| vb_q2        | 0.00255  |\n",
      "| vb_q3        | 0.109    |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 9.21     |\n",
      "| loss         | 0.489    |\n",
      "| loss_q0      | 0.586    |\n",
      "| loss_q1      | 0.436    |\n",
      "| loss_q2      | 0.427    |\n",
      "| loss_q3      | 0.444    |\n",
      "| mse          | 0.465    |\n",
      "| mse_q0       | 0.526    |\n",
      "| mse_q1       | 0.434    |\n",
      "| mse_q2       | 0.425    |\n",
      "| mse_q3       | 0.438    |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 244      |\n",
      "| step         | 60       |\n",
      "| vb           | 0.0239   |\n",
      "| vb_q0        | 0.0601   |\n",
      "| vb_q1        | 0.00212  |\n",
      "| vb_q2        | 0.00215  |\n",
      "| vb_q3        | 0.00625  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 7.41     |\n",
      "| loss         | 0.402    |\n",
      "| loss_q0      | 0.469    |\n",
      "| loss_q1      | 0.384    |\n",
      "| loss_q2      | 0.386    |\n",
      "| loss_q3      | 0.4      |\n",
      "| mse          | 0.394    |\n",
      "| mse_q0       | 0.439    |\n",
      "| mse_q1       | 0.382    |\n",
      "| mse_q2       | 0.384    |\n",
      "| mse_q3       | 0.393    |\n",
      "| param_norm   | 93.1     |\n",
      "| samples      | 284      |\n",
      "| step         | 70       |\n",
      "| vb           | 0.00771  |\n",
      "| vb_q0        | 0.0302   |\n",
      "| vb_q1        | 0.00197  |\n",
      "| vb_q2        | 0.00183  |\n",
      "| vb_q3        | 0.00672  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 6.22     |\n",
      "| loss         | 0.358    |\n",
      "| loss_q0      | 0.382    |\n",
      "| loss_q1      | 0.345    |\n",
      "| loss_q2      | 0.35     |\n",
      "| loss_q3      | 0.356    |\n",
      "| mse          | 0.354    |\n",
      "| mse_q0       | 0.376    |\n",
      "| mse_q1       | 0.343    |\n",
      "| mse_q2       | 0.348    |\n",
      "| mse_q3       | 0.351    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 324      |\n",
      "| step         | 80       |\n",
      "| vb           | 0.00373  |\n",
      "| vb_q0        | 0.00661  |\n",
      "| vb_q1        | 0.00173  |\n",
      "| vb_q2        | 0.00177  |\n",
      "| vb_q3        | 0.00471  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 5.35     |\n",
      "| loss         | 0.35     |\n",
      "| loss_q0      | 0.339    |\n",
      "| loss_q1      | 0.318    |\n",
      "| loss_q2      | 0.32     |\n",
      "| loss_q3      | 0.385    |\n",
      "| mse          | 0.322    |\n",
      "| mse_q0       | 0.335    |\n",
      "| mse_q1       | 0.316    |\n",
      "| mse_q2       | 0.318    |\n",
      "| mse_q3       | 0.323    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 364      |\n",
      "| step         | 90       |\n",
      "| vb           | 0.0279   |\n",
      "| vb_q0        | 0.00426  |\n",
      "| vb_q1        | 0.00161  |\n",
      "| vb_q2        | 0.00164  |\n",
      "| vb_q3        | 0.0623   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 4.7      |\n",
      "| loss         | 0.311    |\n",
      "| loss_q0      | 0.353    |\n",
      "| loss_q1      | 0.292    |\n",
      "| loss_q2      | 0.3      |\n",
      "| loss_q3      | 0.305    |\n",
      "| mse          | 0.306    |\n",
      "| mse_q0       | 0.343    |\n",
      "| mse_q1       | 0.291    |\n",
      "| mse_q2       | 0.298    |\n",
      "| mse_q3       | 0.295    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 404      |\n",
      "| step         | 100      |\n",
      "| vb           | 0.00588  |\n",
      "| vb_q0        | 0.0103   |\n",
      "| vb_q1        | 0.00157  |\n",
      "| vb_q2        | 0.00154  |\n",
      "| vb_q3        | 0.00939  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 4.1      |\n",
      "| loss         | 5.34     |\n",
      "| loss_q0      | 0.303    |\n",
      "| loss_q1      | 0.278    |\n",
      "| loss_q2      | 0.277    |\n",
      "| loss_q3      | 17.1     |\n",
      "| mse          | 0.282    |\n",
      "| mse_q0       | 0.299    |\n",
      "| mse_q1       | 0.277    |\n",
      "| mse_q2       | 0.275    |\n",
      "| mse_q3       | 0.278    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 444      |\n",
      "| step         | 110      |\n",
      "| vb           | 5.06     |\n",
      "| vb_q0        | 0.00396  |\n",
      "| vb_q1        | 0.00144  |\n",
      "| vb_q2        | 0.00139  |\n",
      "| vb_q3        | 16.9     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 3.61     |\n",
      "| loss         | 0.277    |\n",
      "| loss_q0      | 0.308    |\n",
      "| loss_q1      | 0.261    |\n",
      "| loss_q2      | 0.263    |\n",
      "| loss_q3      | 0.284    |\n",
      "| mse          | 0.269    |\n",
      "| mse_q0       | 0.298    |\n",
      "| mse_q1       | 0.26     |\n",
      "| mse_q2       | 0.261    |\n",
      "| mse_q3       | 0.265    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 484      |\n",
      "| step         | 120      |\n",
      "| vb           | 0.00753  |\n",
      "| vb_q0        | 0.0103   |\n",
      "| vb_q1        | 0.00131  |\n",
      "| vb_q2        | 0.00128  |\n",
      "| vb_q3        | 0.0191   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 4.07     |\n",
      "| loss         | 0.269    |\n",
      "| loss_q0      | 0.38     |\n",
      "| loss_q1      | 0.251    |\n",
      "| loss_q2      | 0.248    |\n",
      "| loss_q3      | 0.257    |\n",
      "| mse          | 0.262    |\n",
      "| mse_q0       | 0.35     |\n",
      "| mse_q1       | 0.25     |\n",
      "| mse_q2       | 0.247    |\n",
      "| mse_q3       | 0.251    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 524      |\n",
      "| step         | 130      |\n",
      "| vb           | 0.00652  |\n",
      "| vb_q0        | 0.0304   |\n",
      "| vb_q1        | 0.00132  |\n",
      "| vb_q2        | 0.00124  |\n",
      "| vb_q3        | 0.00584  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 3.63     |\n",
      "| loss         | 0.258    |\n",
      "| loss_q0      | 0.376    |\n",
      "| loss_q1      | 0.24     |\n",
      "| loss_q2      | 0.241    |\n",
      "| loss_q3      | 0.241    |\n",
      "| mse          | 0.251    |\n",
      "| mse_q0       | 0.339    |\n",
      "| mse_q1       | 0.239    |\n",
      "| mse_q2       | 0.239    |\n",
      "| mse_q3       | 0.237    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 564      |\n",
      "| step         | 140      |\n",
      "| vb           | 0.00632  |\n",
      "| vb_q0        | 0.0368   |\n",
      "| vb_q1        | 0.00111  |\n",
      "| vb_q2        | 0.00122  |\n",
      "| vb_q3        | 0.00401  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 3.01     |\n",
      "| loss         | 0.246    |\n",
      "| loss_q0      | 0.323    |\n",
      "| loss_q1      | 0.234    |\n",
      "| loss_q2      | 0.232    |\n",
      "| loss_q3      | 0.238    |\n",
      "| mse          | 0.241    |\n",
      "| mse_q0       | 0.309    |\n",
      "| mse_q1       | 0.233    |\n",
      "| mse_q2       | 0.23     |\n",
      "| mse_q3       | 0.232    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 604      |\n",
      "| step         | 150      |\n",
      "| vb           | 0.0043   |\n",
      "| vb_q0        | 0.0143   |\n",
      "| vb_q1        | 0.0012   |\n",
      "| vb_q2        | 0.00117  |\n",
      "| vb_q3        | 0.00573  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 2.57     |\n",
      "| loss         | 0.244    |\n",
      "| loss_q0      | 0.288    |\n",
      "| loss_q1      | 0.228    |\n",
      "| loss_q2      | 0.225    |\n",
      "| loss_q3      | 0.231    |\n",
      "| mse          | 0.238    |\n",
      "| mse_q0       | 0.275    |\n",
      "| mse_q1       | 0.227    |\n",
      "| mse_q2       | 0.224    |\n",
      "| mse_q3       | 0.224    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 644      |\n",
      "| step         | 160      |\n",
      "| vb           | 0.006    |\n",
      "| vb_q0        | 0.0128   |\n",
      "| vb_q1        | 0.00112  |\n",
      "| vb_q2        | 0.00112  |\n",
      "| vb_q3        | 0.00762  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 2.24     |\n",
      "| loss         | 0.231    |\n",
      "| loss_q0      | 0.256    |\n",
      "| loss_q1      | 0.22     |\n",
      "| loss_q2      | 0.219    |\n",
      "| loss_q3      | 0.229    |\n",
      "| mse          | 0.227    |\n",
      "| mse_q0       | 0.252    |\n",
      "| mse_q1       | 0.219    |\n",
      "| mse_q2       | 0.218    |\n",
      "| mse_q3       | 0.218    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 684      |\n",
      "| step         | 170      |\n",
      "| vb           | 0.00377  |\n",
      "| vb_q0        | 0.00431  |\n",
      "| vb_q1        | 0.00109  |\n",
      "| vb_q2        | 0.00111  |\n",
      "| vb_q3        | 0.01     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.92     |\n",
      "| loss         | 0.225    |\n",
      "| loss_q0      | 0.237    |\n",
      "| loss_q1      | 0.216    |\n",
      "| loss_q2      | 0.213    |\n",
      "| loss_q3      | 0.233    |\n",
      "| mse          | 0.218    |\n",
      "| mse_q0       | 0.234    |\n",
      "| mse_q1       | 0.214    |\n",
      "| mse_q2       | 0.211    |\n",
      "| mse_q3       | 0.213    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 724      |\n",
      "| step         | 180      |\n",
      "| vb           | 0.00723  |\n",
      "| vb_q0        | 0.00337  |\n",
      "| vb_q1        | 0.00111  |\n",
      "| vb_q2        | 0.00105  |\n",
      "| vb_q3        | 0.0198   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.9      |\n",
      "| loss         | 0.225    |\n",
      "| loss_q0      | 0.254    |\n",
      "| loss_q1      | 0.211    |\n",
      "| loss_q2      | 0.209    |\n",
      "| loss_q3      | 0.212    |\n",
      "| mse          | 0.221    |\n",
      "| mse_q0       | 0.247    |\n",
      "| mse_q1       | 0.21     |\n",
      "| mse_q2       | 0.208    |\n",
      "| mse_q3       | 0.209    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 764      |\n",
      "| step         | 190      |\n",
      "| vb           | 0.00318  |\n",
      "| vb_q0        | 0.00618  |\n",
      "| vb_q1        | 0.00107  |\n",
      "| vb_q2        | 0.000999 |\n",
      "| vb_q3        | 0.00313  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.74     |\n",
      "| loss         | 0.219    |\n",
      "| loss_q0      | 0.25     |\n",
      "| loss_q1      | 0.206    |\n",
      "| loss_q2      | 0.206    |\n",
      "| loss_q3      | 0.219    |\n",
      "| mse          | 0.213    |\n",
      "| mse_q0       | 0.242    |\n",
      "| mse_q1       | 0.205    |\n",
      "| mse_q2       | 0.205    |\n",
      "| mse_q3       | 0.205    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 804      |\n",
      "| step         | 200      |\n",
      "| vb           | 0.00607  |\n",
      "| vb_q0        | 0.00797  |\n",
      "| vb_q1        | 0.00103  |\n",
      "| vb_q2        | 0.00112  |\n",
      "| vb_q3        | 0.0136   |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.96     |\n",
      "| loss         | 0.252    |\n",
      "| loss_q0      | 0.479    |\n",
      "| loss_q1      | 0.202    |\n",
      "| loss_q2      | 0.201    |\n",
      "| loss_q3      | 0.206    |\n",
      "| mse          | 0.214    |\n",
      "| mse_q0       | 0.271    |\n",
      "| mse_q1       | 0.201    |\n",
      "| mse_q2       | 0.2      |\n",
      "| mse_q3       | 0.202    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 844      |\n",
      "| step         | 210      |\n",
      "| vb           | 0.0381   |\n",
      "| vb_q0        | 0.208    |\n",
      "| vb_q1        | 0.00098  |\n",
      "| vb_q2        | 0.00105  |\n",
      "| vb_q3        | 0.00381  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.53     |\n",
      "| loss         | 0.207    |\n",
      "| loss_q0      | 0.246    |\n",
      "| loss_q1      | 0.2      |\n",
      "| loss_q2      | 0.198    |\n",
      "| loss_q3      | 0.2      |\n",
      "| mse          | 0.205    |\n",
      "| mse_q0       | 0.239    |\n",
      "| mse_q1       | 0.199    |\n",
      "| mse_q2       | 0.197    |\n",
      "| mse_q3       | 0.198    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 884      |\n",
      "| step         | 220      |\n",
      "| vb           | 0.00222  |\n",
      "| vb_q0        | 0.00745  |\n",
      "| vb_q1        | 0.000963 |\n",
      "| vb_q2        | 0.000986 |\n",
      "| vb_q3        | 0.00205  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.3      |\n",
      "| loss         | 0.202    |\n",
      "| loss_q0      | 0.212    |\n",
      "| loss_q1      | 0.199    |\n",
      "| loss_q2      | 0.195    |\n",
      "| loss_q3      | 0.199    |\n",
      "| mse          | 0.2      |\n",
      "| mse_q0       | 0.21     |\n",
      "| mse_q1       | 0.198    |\n",
      "| mse_q2       | 0.194    |\n",
      "| mse_q3       | 0.196    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 924      |\n",
      "| step         | 230      |\n",
      "| vb           | 0.00173  |\n",
      "| vb_q0        | 0.00208  |\n",
      "| vb_q1        | 0.000992 |\n",
      "| vb_q2        | 0.000994 |\n",
      "| vb_q3        | 0.00291  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.26     |\n",
      "| loss         | 0.202    |\n",
      "| loss_q0      | 0.223    |\n",
      "| loss_q1      | 0.194    |\n",
      "| loss_q2      | 0.193    |\n",
      "| loss_q3      | 0.196    |\n",
      "| mse          | 0.199    |\n",
      "| mse_q0       | 0.218    |\n",
      "| mse_q1       | 0.194    |\n",
      "| mse_q2       | 0.192    |\n",
      "| mse_q3       | 0.193    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 964      |\n",
      "| step         | 240      |\n",
      "| vb           | 0.0028   |\n",
      "| vb_q0        | 0.00446  |\n",
      "| vb_q1        | 0.000902 |\n",
      "| vb_q2        | 0.000941 |\n",
      "| vb_q3        | 0.00376  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.33     |\n",
      "| loss         | 0.209    |\n",
      "| loss_q0      | 0.279    |\n",
      "| loss_q1      | 0.193    |\n",
      "| loss_q2      | 0.191    |\n",
      "| loss_q3      | 0.197    |\n",
      "| mse          | 0.202    |\n",
      "| mse_q0       | 0.256    |\n",
      "| mse_q1       | 0.193    |\n",
      "| mse_q2       | 0.19     |\n",
      "| mse_q3       | 0.19     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1e+03    |\n",
      "| step         | 250      |\n",
      "| vb           | 0.00614  |\n",
      "| vb_q0        | 0.0222   |\n",
      "| vb_q1        | 0.000959 |\n",
      "| vb_q2        | 0.000966 |\n",
      "| vb_q3        | 0.00679  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.15     |\n",
      "| loss         | 0.199    |\n",
      "| loss_q0      | 0.218    |\n",
      "| loss_q1      | 0.192    |\n",
      "| loss_q2      | 0.189    |\n",
      "| loss_q3      | 0.198    |\n",
      "| mse          | 0.196    |\n",
      "| mse_q0       | 0.213    |\n",
      "| mse_q1       | 0.191    |\n",
      "| mse_q2       | 0.188    |\n",
      "| mse_q3       | 0.188    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.04e+03 |\n",
      "| step         | 260      |\n",
      "| vb           | 0.00351  |\n",
      "| vb_q0        | 0.00507  |\n",
      "| vb_q1        | 0.000961 |\n",
      "| vb_q2        | 0.000921 |\n",
      "| vb_q3        | 0.00914  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.12     |\n",
      "| loss         | 0.23     |\n",
      "| loss_q0      | 1        |\n",
      "| loss_q1      | 0.191    |\n",
      "| loss_q2      | 0.188    |\n",
      "| loss_q3      | 0.189    |\n",
      "| mse          | 0.195    |\n",
      "| mse_q0       | 0.336    |\n",
      "| mse_q1       | 0.19     |\n",
      "| mse_q2       | 0.187    |\n",
      "| mse_q3       | 0.187    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.08e+03 |\n",
      "| step         | 270      |\n",
      "| vb           | 0.0346   |\n",
      "| vb_q0        | 0.668    |\n",
      "| vb_q1        | 0.000965 |\n",
      "| vb_q2        | 0.000963 |\n",
      "| vb_q3        | 0.00248  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.907    |\n",
      "| loss         | 0.19     |\n",
      "| loss_q0      | 0.204    |\n",
      "| loss_q1      | 0.189    |\n",
      "| loss_q2      | 0.186    |\n",
      "| loss_q3      | 0.188    |\n",
      "| mse          | 0.188    |\n",
      "| mse_q0       | 0.201    |\n",
      "| mse_q1       | 0.188    |\n",
      "| mse_q2       | 0.186    |\n",
      "| mse_q3       | 0.185    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.12e+03 |\n",
      "| step         | 280      |\n",
      "| vb           | 0.00176  |\n",
      "| vb_q0        | 0.00252  |\n",
      "| vb_q1        | 0.00092  |\n",
      "| vb_q2        | 0.000975 |\n",
      "| vb_q3        | 0.00288  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.1      |\n",
      "| loss         | 0.198    |\n",
      "| loss_q0      | 0.231    |\n",
      "| loss_q1      | 0.188    |\n",
      "| loss_q2      | 0.185    |\n",
      "| loss_q3      | 0.189    |\n",
      "| mse          | 0.194    |\n",
      "| mse_q0       | 0.223    |\n",
      "| mse_q1       | 0.187    |\n",
      "| mse_q2       | 0.184    |\n",
      "| mse_q3       | 0.184    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.16e+03 |\n",
      "| step         | 290      |\n",
      "| vb           | 0.00351  |\n",
      "| vb_q0        | 0.00748  |\n",
      "| vb_q1        | 0.000882 |\n",
      "| vb_q2        | 0.000898 |\n",
      "| vb_q3        | 0.00442  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.856    |\n",
      "| loss         | 0.193    |\n",
      "| loss_q0      | 0.215    |\n",
      "| loss_q1      | 0.186    |\n",
      "| loss_q2      | 0.183    |\n",
      "| loss_q3      | 0.189    |\n",
      "| mse          | 0.189    |\n",
      "| mse_q0       | 0.21     |\n",
      "| mse_q1       | 0.185    |\n",
      "| mse_q2       | 0.182    |\n",
      "| mse_q3       | 0.183    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.2e+03  |\n",
      "| step         | 300      |\n",
      "| vb           | 0.00358  |\n",
      "| vb_q0        | 0.00539  |\n",
      "| vb_q1        | 0.000934 |\n",
      "| vb_q2        | 0.00089  |\n",
      "| vb_q3        | 0.00643  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.852    |\n",
      "| loss         | 0.198    |\n",
      "| loss_q0      | 0.235    |\n",
      "| loss_q1      | 0.186    |\n",
      "| loss_q2      | 0.182    |\n",
      "| loss_q3      | 0.184    |\n",
      "| mse          | 0.192    |\n",
      "| mse_q0       | 0.218    |\n",
      "| mse_q1       | 0.185    |\n",
      "| mse_q2       | 0.181    |\n",
      "| mse_q3       | 0.18     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.24e+03 |\n",
      "| step         | 310      |\n",
      "| vb           | 0.00552  |\n",
      "| vb_q0        | 0.0161   |\n",
      "| vb_q1        | 0.000908 |\n",
      "| vb_q2        | 0.000929 |\n",
      "| vb_q3        | 0.00372  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.716    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.201    |\n",
      "| loss_q1      | 0.184    |\n",
      "| loss_q2      | 0.182    |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.185    |\n",
      "| mse_q0       | 0.199    |\n",
      "| mse_q1       | 0.183    |\n",
      "| mse_q2       | 0.181    |\n",
      "| mse_q3       | 0.18     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.28e+03 |\n",
      "| step         | 320      |\n",
      "| vb           | 0.0014   |\n",
      "| vb_q0        | 0.00222  |\n",
      "| vb_q1        | 0.000922 |\n",
      "| vb_q2        | 0.000993 |\n",
      "| vb_q3        | 0.00223  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.674    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.197    |\n",
      "| loss_q1      | 0.182    |\n",
      "| loss_q2      | 0.181    |\n",
      "| loss_q3      | 0.185    |\n",
      "| mse          | 0.183    |\n",
      "| mse_q0       | 0.195    |\n",
      "| mse_q1       | 0.181    |\n",
      "| mse_q2       | 0.18     |\n",
      "| mse_q3       | 0.179    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.32e+03 |\n",
      "| step         | 330      |\n",
      "| vb           | 0.00255  |\n",
      "| vb_q0        | 0.00254  |\n",
      "| vb_q1        | 0.000909 |\n",
      "| vb_q2        | 0.000914 |\n",
      "| vb_q3        | 0.00514  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.04     |\n",
      "| loss         | 0.199    |\n",
      "| loss_q0      | 0.247    |\n",
      "| loss_q1      | 0.182    |\n",
      "| loss_q2      | 0.18     |\n",
      "| loss_q3      | 0.184    |\n",
      "| mse          | 0.193    |\n",
      "| mse_q0       | 0.232    |\n",
      "| mse_q1       | 0.181    |\n",
      "| mse_q2       | 0.18     |\n",
      "| mse_q3       | 0.179    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.36e+03 |\n",
      "| step         | 340      |\n",
      "| vb           | 0.00575  |\n",
      "| vb_q0        | 0.0152   |\n",
      "| vb_q1        | 0.000924 |\n",
      "| vb_q2        | 0.000902 |\n",
      "| vb_q3        | 0.00516  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.619    |\n",
      "| loss         | 0.182    |\n",
      "| loss_q0      | 0.195    |\n",
      "| loss_q1      | 0.181    |\n",
      "| loss_q2      | 0.179    |\n",
      "| loss_q3      | 0.181    |\n",
      "| mse          | 0.181    |\n",
      "| mse_q0       | 0.193    |\n",
      "| mse_q1       | 0.18     |\n",
      "| mse_q2       | 0.178    |\n",
      "| mse_q3       | 0.178    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.4e+03  |\n",
      "| step         | 350      |\n",
      "| vb           | 0.00144  |\n",
      "| vb_q0        | 0.00216  |\n",
      "| vb_q1        | 0.000871 |\n",
      "| vb_q2        | 0.000911 |\n",
      "| vb_q3        | 0.00219  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.58     |\n",
      "| loss         | 0.184    |\n",
      "| loss_q0      | 0.193    |\n",
      "| loss_q1      | 0.182    |\n",
      "| loss_q2      | 0.18     |\n",
      "| loss_q3      | 0.183    |\n",
      "| mse          | 0.182    |\n",
      "| mse_q0       | 0.191    |\n",
      "| mse_q1       | 0.181    |\n",
      "| mse_q2       | 0.179    |\n",
      "| mse_q3       | 0.177    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.44e+03 |\n",
      "| step         | 360      |\n",
      "| vb           | 0.00248  |\n",
      "| vb_q0        | 0.00243  |\n",
      "| vb_q1        | 0.000963 |\n",
      "| vb_q2        | 0.000889 |\n",
      "| vb_q3        | 0.0052   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.562    |\n",
      "| loss         | 0.194    |\n",
      "| loss_q0      | 0.195    |\n",
      "| loss_q1      | 0.18     |\n",
      "| loss_q2      | 0.178    |\n",
      "| loss_q3      | 0.221    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.18     |\n",
      "| mse_q2       | 0.177    |\n",
      "| mse_q3       | 0.176    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.48e+03 |\n",
      "| step         | 370      |\n",
      "| vb           | 0.0142   |\n",
      "| vb_q0        | 0.00227  |\n",
      "| vb_q1        | 0.000903 |\n",
      "| vb_q2        | 0.000934 |\n",
      "| vb_q3        | 0.0443   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.778    |\n",
      "| loss         | 0.198    |\n",
      "| loss_q0      | 0.24     |\n",
      "| loss_q1      | 0.181    |\n",
      "| loss_q2      | 0.177    |\n",
      "| loss_q3      | 0.192    |\n",
      "| mse          | 0.19     |\n",
      "| mse_q0       | 0.226    |\n",
      "| mse_q1       | 0.18     |\n",
      "| mse_q2       | 0.176    |\n",
      "| mse_q3       | 0.176    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.52e+03 |\n",
      "| step         | 380      |\n",
      "| vb           | 0.00888  |\n",
      "| vb_q0        | 0.0139   |\n",
      "| vb_q1        | 0.000902 |\n",
      "| vb_q2        | 0.000891 |\n",
      "| vb_q3        | 0.0166   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.04     |\n",
      "| loss         | 0.231    |\n",
      "| loss_q0      | 0.354    |\n",
      "| loss_q1      | 0.178    |\n",
      "| loss_q2      | 0.178    |\n",
      "| loss_q3      | 0.178    |\n",
      "| mse          | 0.193    |\n",
      "| mse_q0       | 0.232    |\n",
      "| mse_q1       | 0.178    |\n",
      "| mse_q2       | 0.177    |\n",
      "| mse_q3       | 0.175    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.56e+03 |\n",
      "| step         | 390      |\n",
      "| vb           | 0.0377   |\n",
      "| vb_q0        | 0.122    |\n",
      "| vb_q1        | 0.00086  |\n",
      "| vb_q2        | 0.000836 |\n",
      "| vb_q3        | 0.0024   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 1.94     |\n",
      "| loss         | 0.23     |\n",
      "| loss_q0      | 0.386    |\n",
      "| loss_q1      | 0.18     |\n",
      "| loss_q2      | 0.177    |\n",
      "| loss_q3      | 0.178    |\n",
      "| mse          | 0.193    |\n",
      "| mse_q0       | 0.241    |\n",
      "| mse_q1       | 0.18     |\n",
      "| mse_q2       | 0.176    |\n",
      "| mse_q3       | 0.175    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.6e+03  |\n",
      "| step         | 400      |\n",
      "| vb           | 0.0377   |\n",
      "| vb_q0        | 0.145    |\n",
      "| vb_q1        | 0.000888 |\n",
      "| vb_q2        | 0.000909 |\n",
      "| vb_q3        | 0.0036   |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.547    |\n",
      "| loss         | 0.187    |\n",
      "| loss_q0      | 0.212    |\n",
      "| loss_q1      | 0.181    |\n",
      "| loss_q2      | 0.178    |\n",
      "| loss_q3      | 0.184    |\n",
      "| mse          | 0.184    |\n",
      "| mse_q0       | 0.208    |\n",
      "| mse_q1       | 0.18     |\n",
      "| mse_q2       | 0.177    |\n",
      "| mse_q3       | 0.176    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.64e+03 |\n",
      "| step         | 410      |\n",
      "| vb           | 0.00373  |\n",
      "| vb_q0        | 0.00435  |\n",
      "| vb_q1        | 0.000926 |\n",
      "| vb_q2        | 0.000881 |\n",
      "| vb_q3        | 0.00869  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.534    |\n",
      "| loss         | 0.184    |\n",
      "| loss_q0      | 0.202    |\n",
      "| loss_q1      | 0.18     |\n",
      "| loss_q2      | 0.177    |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.181    |\n",
      "| mse_q0       | 0.198    |\n",
      "| mse_q1       | 0.179    |\n",
      "| mse_q2       | 0.176    |\n",
      "| mse_q3       | 0.175    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.68e+03 |\n",
      "| step         | 420      |\n",
      "| vb           | 0.00357  |\n",
      "| vb_q0        | 0.00386  |\n",
      "| vb_q1        | 0.000957 |\n",
      "| vb_q2        | 0.000909 |\n",
      "| vb_q3        | 0.00725  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.577    |\n",
      "| loss         | 0.191    |\n",
      "| loss_q0      | 0.223    |\n",
      "| loss_q1      | 0.178    |\n",
      "| loss_q2      | 0.176    |\n",
      "| loss_q3      | 0.179    |\n",
      "| mse          | 0.188    |\n",
      "| mse_q0       | 0.215    |\n",
      "| mse_q1       | 0.177    |\n",
      "| mse_q2       | 0.175    |\n",
      "| mse_q3       | 0.174    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.72e+03 |\n",
      "| step         | 430      |\n",
      "| vb           | 0.00375  |\n",
      "| vb_q0        | 0.00744  |\n",
      "| vb_q1        | 0.000884 |\n",
      "| vb_q2        | 0.00084  |\n",
      "| vb_q3        | 0.00492  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.391    |\n",
      "| loss         | 0.183    |\n",
      "| loss_q0      | 0.19     |\n",
      "| loss_q1      | 0.178    |\n",
      "| loss_q2      | 0.175    |\n",
      "| loss_q3      | 0.19     |\n",
      "| mse          | 0.178    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.177    |\n",
      "| mse_q2       | 0.174    |\n",
      "| mse_q3       | 0.173    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.76e+03 |\n",
      "| step         | 440      |\n",
      "| vb           | 0.00589  |\n",
      "| vb_q0        | 0.00242  |\n",
      "| vb_q1        | 0.000908 |\n",
      "| vb_q2        | 0.000861 |\n",
      "| vb_q3        | 0.0164   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.691    |\n",
      "| loss         | 0.204    |\n",
      "| loss_q0      | 0.266    |\n",
      "| loss_q1      | 0.178    |\n",
      "| loss_q2      | 0.175    |\n",
      "| loss_q3      | 0.189    |\n",
      "| mse          | 0.189    |\n",
      "| mse_q0       | 0.229    |\n",
      "| mse_q1       | 0.177    |\n",
      "| mse_q2       | 0.174    |\n",
      "| mse_q3       | 0.173    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.8e+03  |\n",
      "| step         | 450      |\n",
      "| vb           | 0.0151   |\n",
      "| vb_q0        | 0.037    |\n",
      "| vb_q1        | 0.000897 |\n",
      "| vb_q2        | 0.0009   |\n",
      "| vb_q3        | 0.0165   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.458    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.207    |\n",
      "| loss_q1      | 0.178    |\n",
      "| loss_q2      | 0.175    |\n",
      "| loss_q3      | 0.19     |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.201    |\n",
      "| mse_q1       | 0.177    |\n",
      "| mse_q2       | 0.174    |\n",
      "| mse_q3       | 0.172    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.84e+03 |\n",
      "| step         | 460      |\n",
      "| vb           | 0.00623  |\n",
      "| vb_q0        | 0.00617  |\n",
      "| vb_q1        | 0.000904 |\n",
      "| vb_q2        | 0.000853 |\n",
      "| vb_q3        | 0.0181   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.477    |\n",
      "| loss         | 0.183    |\n",
      "| loss_q0      | 0.199    |\n",
      "| loss_q1      | 0.177    |\n",
      "| loss_q2      | 0.174    |\n",
      "| loss_q3      | 0.177    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.196    |\n",
      "| mse_q1       | 0.176    |\n",
      "| mse_q2       | 0.173    |\n",
      "| mse_q3       | 0.172    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.88e+03 |\n",
      "| step         | 470      |\n",
      "| vb           | 0.00291  |\n",
      "| vb_q0        | 0.00389  |\n",
      "| vb_q1        | 0.000861 |\n",
      "| vb_q2        | 0.000892 |\n",
      "| vb_q3        | 0.00465  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.428    |\n",
      "| loss         | 0.183    |\n",
      "| loss_q0      | 0.213    |\n",
      "| loss_q1      | 0.176    |\n",
      "| loss_q2      | 0.174    |\n",
      "| loss_q3      | 0.184    |\n",
      "| mse          | 0.179    |\n",
      "| mse_q0       | 0.207    |\n",
      "| mse_q1       | 0.175    |\n",
      "| mse_q2       | 0.173    |\n",
      "| mse_q3       | 0.172    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.92e+03 |\n",
      "| step         | 480      |\n",
      "| vb           | 0.00484  |\n",
      "| vb_q0        | 0.00621  |\n",
      "| vb_q1        | 0.000854 |\n",
      "| vb_q2        | 0.000836 |\n",
      "| vb_q3        | 0.0115   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.334    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.176    |\n",
      "| loss_q2      | 0.175    |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.176    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.175    |\n",
      "| mse_q2       | 0.174    |\n",
      "| mse_q3       | 0.172    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 1.96e+03 |\n",
      "| step         | 490      |\n",
      "| vb           | 0.00185  |\n",
      "| vb_q0        | 0.00187  |\n",
      "| vb_q1        | 0.000859 |\n",
      "| vb_q2        | 0.0009   |\n",
      "| vb_q3        | 0.00378  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.371    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.189    |\n",
      "| loss_q1      | 0.175    |\n",
      "| loss_q2      | 0.173    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.186    |\n",
      "| mse_q1       | 0.174    |\n",
      "| mse_q2       | 0.172    |\n",
      "| mse_q3       | 0.171    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2e+03    |\n",
      "| step         | 500      |\n",
      "| vb           | 0.00178  |\n",
      "| vb_q0        | 0.00297  |\n",
      "| vb_q1        | 0.000892 |\n",
      "| vb_q2        | 0.000824 |\n",
      "| vb_q3        | 0.00237  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.337    |\n",
      "| loss         | 3.26     |\n",
      "| loss_q0      | 0.199    |\n",
      "| loss_q1      | 0.175    |\n",
      "| loss_q2      | 0.173    |\n",
      "| loss_q3      | 8.38     |\n",
      "| mse          | 0.177    |\n",
      "| mse_q0       | 0.193    |\n",
      "| mse_q1       | 0.174    |\n",
      "| mse_q2       | 0.172    |\n",
      "| mse_q3       | 0.171    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.04e+03 |\n",
      "| step         | 510      |\n",
      "| vb           | 3.08     |\n",
      "| vb_q0        | 0.00569  |\n",
      "| vb_q1        | 0.000893 |\n",
      "| vb_q2        | 0.000891 |\n",
      "| vb_q3        | 8.21     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.327    |\n",
      "| loss         | 0.181    |\n",
      "| loss_q0      | 0.204    |\n",
      "| loss_q1      | 0.175    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.178    |\n",
      "| mse_q0       | 0.199    |\n",
      "| mse_q1       | 0.174    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.171    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.08e+03 |\n",
      "| step         | 520      |\n",
      "| vb           | 0.00233  |\n",
      "| vb_q0        | 0.00547  |\n",
      "| vb_q1        | 0.000892 |\n",
      "| vb_q2        | 0.000886 |\n",
      "| vb_q3        | 0.00294  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.3      |\n",
      "| loss         | 0.181    |\n",
      "| loss_q0      | 0.196    |\n",
      "| loss_q1      | 0.174    |\n",
      "| loss_q2      | 0.173    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.179    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.173    |\n",
      "| mse_q2       | 0.172    |\n",
      "| mse_q3       | 0.172    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.12e+03 |\n",
      "| step         | 530      |\n",
      "| vb           | 0.00213  |\n",
      "| vb_q0        | 0.0041   |\n",
      "| vb_q1        | 0.000882 |\n",
      "| vb_q2        | 0.000841 |\n",
      "| vb_q3        | 0.00257  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.273    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.175    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.174    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.17     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.16e+03 |\n",
      "| step         | 540      |\n",
      "| vb           | 0.00291  |\n",
      "| vb_q0        | 0.0023   |\n",
      "| vb_q1        | 0.000881 |\n",
      "| vb_q2        | 0.000925 |\n",
      "| vb_q3        | 0.0117   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.29     |\n",
      "| loss         | 0.187    |\n",
      "| loss_q0      | 0.19     |\n",
      "| loss_q1      | 0.174    |\n",
      "| loss_q2      | 0.173    |\n",
      "| loss_q3      | 0.2      |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.173    |\n",
      "| mse_q2       | 0.172    |\n",
      "| mse_q3       | 0.17     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.2e+03  |\n",
      "| step         | 550      |\n",
      "| vb           | 0.0121   |\n",
      "| vb_q0        | 0.00335  |\n",
      "| vb_q1        | 0.000864 |\n",
      "| vb_q2        | 0.000872 |\n",
      "| vb_q3        | 0.0293   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.228    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.177    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.17     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.24e+03 |\n",
      "| step         | 560      |\n",
      "| vb           | 0.00158  |\n",
      "| vb_q0        | 0.00171  |\n",
      "| vb_q1        | 0.000857 |\n",
      "| vb_q2        | 0.000838 |\n",
      "| vb_q3        | 0.00256  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.248    |\n",
      "| loss         | 0.18     |\n",
      "| loss_q0      | 0.193    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.176    |\n",
      "| mse_q0       | 0.19     |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.17     |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.28e+03 |\n",
      "| step         | 570      |\n",
      "| vb           | 0.00385  |\n",
      "| vb_q0        | 0.00363  |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.000825 |\n",
      "| vb_q3        | 0.0121   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.211    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.192    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.32e+03 |\n",
      "| step         | 580      |\n",
      "| vb           | 0.00379  |\n",
      "| vb_q0        | 0.00215  |\n",
      "| vb_q1        | 0.000861 |\n",
      "| vb_q2        | 0.00084  |\n",
      "| vb_q3        | 0.0226   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.334    |\n",
      "| loss         | 0.185    |\n",
      "| loss_q0      | 0.222    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.208    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.36e+03 |\n",
      "| step         | 590      |\n",
      "| vb           | 0.00473  |\n",
      "| vb_q0        | 0.0136   |\n",
      "| vb_q1        | 0.000842 |\n",
      "| vb_q2        | 0.000885 |\n",
      "| vb_q3        | 0.00538  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.214    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.188    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.2     |\n",
      "| samples      | 2.4e+03  |\n",
      "| step         | 600      |\n",
      "| vb           | 0.00284  |\n",
      "| vb_q0        | 0.00333  |\n",
      "| vb_q1        | 0.000875 |\n",
      "| vb_q2        | 0.000883 |\n",
      "| vb_q3        | 0.00599  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.345    |\n",
      "| loss         | 0.185    |\n",
      "| loss_q0      | 0.217    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.181    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.207    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.17     |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.44e+03 |\n",
      "| step         | 610      |\n",
      "| vb           | 0.00589  |\n",
      "| vb_q0        | 0.0106   |\n",
      "| vb_q1        | 0.000849 |\n",
      "| vb_q2        | 0.000938 |\n",
      "| vb_q3        | 0.0112   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.256    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.191    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.188    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.48e+03 |\n",
      "| step         | 620      |\n",
      "| vb           | 0.0018   |\n",
      "| vb_q0        | 0.00311  |\n",
      "| vb_q1        | 0.000852 |\n",
      "| vb_q2        | 0.000822 |\n",
      "| vb_q3        | 0.00265  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.231    |\n",
      "| loss         | 0.179    |\n",
      "| loss_q0      | 0.193    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.177    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.189    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.52e+03 |\n",
      "| step         | 630      |\n",
      "| vb           | 0.00376  |\n",
      "| vb_q0        | 0.00373  |\n",
      "| vb_q1        | 0.00084  |\n",
      "| vb_q2        | 0.000839 |\n",
      "| vb_q3        | 0.00817  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.352    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.223    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.178    |\n",
      "| mse_q0       | 0.198    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.56e+03 |\n",
      "| step         | 640      |\n",
      "| vb           | 0.00774  |\n",
      "| vb_q0        | 0.0251   |\n",
      "| vb_q1        | 0.00088  |\n",
      "| vb_q2        | 0.000873 |\n",
      "| vb_q3        | 0.00254  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.293    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.187    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.184    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.6e+03  |\n",
      "| step         | 650      |\n",
      "| vb           | 0.00232  |\n",
      "| vb_q0        | 0.00304  |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.000847 |\n",
      "| vb_q3        | 0.00431  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.205    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.64e+03 |\n",
      "| step         | 660      |\n",
      "| vb           | 0.00181  |\n",
      "| vb_q0        | 0.0028   |\n",
      "| vb_q1        | 0.000846 |\n",
      "| vb_q2        | 0.00089  |\n",
      "| vb_q3        | 0.00242  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.336    |\n",
      "| loss         | 0.184    |\n",
      "| loss_q0      | 0.206    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.179    |\n",
      "| mse_q0       | 0.196    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.68e+03 |\n",
      "| step         | 670      |\n",
      "| vb           | 0.00477  |\n",
      "| vb_q0        | 0.0102   |\n",
      "| vb_q1        | 0.000874 |\n",
      "| vb_q2        | 0.000857 |\n",
      "| vb_q3        | 0.00396  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.285    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.237    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.178    |\n",
      "| mse_q0       | 0.207    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.72e+03 |\n",
      "| step         | 680      |\n",
      "| vb           | 0.00779  |\n",
      "| vb_q0        | 0.0305   |\n",
      "| vb_q1        | 0.000832 |\n",
      "| vb_q2        | 0.000823 |\n",
      "| vb_q3        | 0.00276  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.285    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.194    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.76e+03 |\n",
      "| step         | 690      |\n",
      "| vb           | 0.00327  |\n",
      "| vb_q0        | 0.00624  |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.000857 |\n",
      "| vb_q3        | 0.00429  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.255    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.19     |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.223    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.8e+03  |\n",
      "| step         | 700      |\n",
      "| vb           | 0.0111   |\n",
      "| vb_q0        | 0.00379  |\n",
      "| vb_q1        | 0.000848 |\n",
      "| vb_q2        | 0.000828 |\n",
      "| vb_q3        | 0.0545   |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.497    |\n",
      "| loss         | 0.189    |\n",
      "| loss_q0      | 0.227    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.182    |\n",
      "| mse_q0       | 0.209    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.84e+03 |\n",
      "| step         | 710      |\n",
      "| vb           | 0.00668  |\n",
      "| vb_q0        | 0.0173   |\n",
      "| vb_q1        | 0.000892 |\n",
      "| vb_q2        | 0.000875 |\n",
      "| vb_q3        | 0.00291  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.29     |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.194    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.19     |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.88e+03 |\n",
      "| step         | 720      |\n",
      "| vb           | 0.00185  |\n",
      "| vb_q0        | 0.00468  |\n",
      "| vb_q1        | 0.000877 |\n",
      "| vb_q2        | 0.000905 |\n",
      "| vb_q3        | 0.00156  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.17     |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.92e+03 |\n",
      "| step         | 730      |\n",
      "| vb           | 0.00206  |\n",
      "| vb_q0        | 0.00283  |\n",
      "| vb_q1        | 0.000847 |\n",
      "| vb_q2        | 0.000842 |\n",
      "| vb_q3        | 0.00293  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.221    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.21     |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.176    |\n",
      "| mse_q0       | 0.202    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 2.96e+03 |\n",
      "| step         | 740      |\n",
      "| vb           | 0.00276  |\n",
      "| vb_q0        | 0.00781  |\n",
      "| vb_q1        | 0.000874 |\n",
      "| vb_q2        | 0.000815 |\n",
      "| vb_q3        | 0.00394  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.246    |\n",
      "| loss         | 0.18     |\n",
      "| loss_q0      | 0.199    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.176    |\n",
      "| mse_q0       | 0.193    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3e+03    |\n",
      "| step         | 750      |\n",
      "| vb           | 0.00374  |\n",
      "| vb_q0        | 0.00675  |\n",
      "| vb_q1        | 0.000827 |\n",
      "| vb_q2        | 0.000859 |\n",
      "| vb_q3        | 0.0073   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.149    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.183    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.04e+03 |\n",
      "| step         | 760      |\n",
      "| vb           | 0.00195  |\n",
      "| vb_q0        | 0.00245  |\n",
      "| vb_q1        | 0.000851 |\n",
      "| vb_q2        | 0.000867 |\n",
      "| vb_q3        | 0.00323  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.182    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.2      |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.08e+03 |\n",
      "| step         | 770      |\n",
      "| vb           | 0.00254  |\n",
      "| vb_q0        | 0.0077   |\n",
      "| vb_q1        | 0.000836 |\n",
      "| vb_q2        | 0.000861 |\n",
      "| vb_q3        | 0.00183  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.534    |\n",
      "| loss         | 0.236    |\n",
      "| loss_q0      | 0.36     |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.189    |\n",
      "| mse_q0       | 0.23     |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.12e+03 |\n",
      "| step         | 780      |\n",
      "| vb           | 0.0469   |\n",
      "| vb_q0        | 0.13     |\n",
      "| vb_q1        | 0.000789 |\n",
      "| vb_q2        | 0.000832 |\n",
      "| vb_q3        | 0.0146   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.202    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.18     |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.177    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.16e+03 |\n",
      "| step         | 790      |\n",
      "| vb           | 0.00346  |\n",
      "| vb_q0        | 0.00201  |\n",
      "| vb_q1        | 0.000937 |\n",
      "| vb_q2        | 0.000834 |\n",
      "| vb_q3        | 0.0097   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.394    |\n",
      "| loss         | 0.223    |\n",
      "| loss_q0      | 0.314    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.208    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.206    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.2e+03  |\n",
      "| step         | 800      |\n",
      "| vb           | 0.0428   |\n",
      "| vb_q0        | 0.109    |\n",
      "| vb_q1        | 0.000857 |\n",
      "| vb_q2        | 0.000841 |\n",
      "| vb_q3        | 0.0395   |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.182    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.181    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.24e+03 |\n",
      "| step         | 810      |\n",
      "| vb           | 0.00432  |\n",
      "| vb_q0        | 0.0032   |\n",
      "| vb_q1        | 0.000848 |\n",
      "| vb_q2        | 0.000872 |\n",
      "| vb_q3        | 0.0136   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.156    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.28e+03 |\n",
      "| step         | 820      |\n",
      "| vb           | 0.00184  |\n",
      "| vb_q0        | 0.0033   |\n",
      "| vb_q1        | 0.000912 |\n",
      "| vb_q2        | 0.000848 |\n",
      "| vb_q3        | 0.00254  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.15     |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.19     |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.176    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.186    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.32e+03 |\n",
      "| step         | 830      |\n",
      "| vb           | 0.00436  |\n",
      "| vb_q0        | 0.00361  |\n",
      "| vb_q1        | 0.000875 |\n",
      "| vb_q2        | 0.00084  |\n",
      "| vb_q3        | 0.00972  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.176    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.192    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.36e+03 |\n",
      "| step         | 840      |\n",
      "| vb           | 0.00232  |\n",
      "| vb_q0        | 0.00504  |\n",
      "| vb_q1        | 0.000854 |\n",
      "| vb_q2        | 0.000854 |\n",
      "| vb_q3        | 0.0024   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.211    |\n",
      "| loss         | 0.179    |\n",
      "| loss_q0      | 0.207    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.4e+03  |\n",
      "| step         | 850      |\n",
      "| vb           | 0.0052   |\n",
      "| vb_q0        | 0.0155   |\n",
      "| vb_q1        | 0.000849 |\n",
      "| vb_q2        | 0.000863 |\n",
      "| vb_q3        | 0.00407  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.143    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.182    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.44e+03 |\n",
      "| step         | 860      |\n",
      "| vb           | 0.00203  |\n",
      "| vb_q0        | 0.00258  |\n",
      "| vb_q1        | 0.000817 |\n",
      "| vb_q2        | 0.000837 |\n",
      "| vb_q3        | 0.00464  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.317    |\n",
      "| loss         | 0.19     |\n",
      "| loss_q0      | 0.234    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.176    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.206    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.48e+03 |\n",
      "| step         | 870      |\n",
      "| vb           | 0.0102   |\n",
      "| vb_q0        | 0.0284   |\n",
      "| vb_q1        | 0.000818 |\n",
      "| vb_q2        | 0.000914 |\n",
      "| vb_q3        | 0.00829  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.156    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.182    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.3     |\n",
      "| samples      | 3.52e+03 |\n",
      "| step         | 880      |\n",
      "| vb           | 0.00204  |\n",
      "| vb_q0        | 0.00241  |\n",
      "| vb_q1        | 0.000853 |\n",
      "| vb_q2        | 0.000813 |\n",
      "| vb_q3        | 0.00393  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.257    |\n",
      "| loss         | 0.182    |\n",
      "| loss_q0      | 0.237    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.177    |\n",
      "| mse_q0       | 0.219    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.56e+03 |\n",
      "| step         | 890      |\n",
      "| vb           | 0.00451  |\n",
      "| vb_q0        | 0.0186   |\n",
      "| vb_q1        | 0.000858 |\n",
      "| vb_q2        | 0.000841 |\n",
      "| vb_q3        | 0.00328  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.164    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.6e+03  |\n",
      "| step         | 900      |\n",
      "| vb           | 0.00189  |\n",
      "| vb_q0        | 0.00322  |\n",
      "| vb_q1        | 0.00084  |\n",
      "| vb_q2        | 0.000861 |\n",
      "| vb_q3        | 0.00369  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.104    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.18     |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.177    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.177    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.64e+03 |\n",
      "| step         | 910      |\n",
      "| vb           | 0.00398  |\n",
      "| vb_q0        | 0.00231  |\n",
      "| vb_q1        | 0.000858 |\n",
      "| vb_q2        | 0.000872 |\n",
      "| vb_q3        | 0.0107   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.093    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.175    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.173    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.68e+03 |\n",
      "| step         | 920      |\n",
      "| vb           | 0.00211  |\n",
      "| vb_q0        | 0.00154  |\n",
      "| vb_q1        | 0.000873 |\n",
      "| vb_q2        | 0.000852 |\n",
      "| vb_q3        | 0.0044   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.105    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.176    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.72e+03 |\n",
      "| step         | 930      |\n",
      "| vb           | 0.00155  |\n",
      "| vb_q0        | 0.00209  |\n",
      "| vb_q1        | 0.000838 |\n",
      "| vb_q2        | 0.0008   |\n",
      "| vb_q3        | 0.00276  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.109    |\n",
      "| loss         | 0.17     |\n",
      "| loss_q0      | 0.177    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.76e+03 |\n",
      "| step         | 940      |\n",
      "| vb           | 0.00137  |\n",
      "| vb_q0        | 0.00202  |\n",
      "| vb_q1        | 0.000815 |\n",
      "| vb_q2        | 0.000843 |\n",
      "| vb_q3        | 0.00216  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.138    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.192    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.188    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.8e+03  |\n",
      "| step         | 950      |\n",
      "| vb           | 0.00253  |\n",
      "| vb_q0        | 0.00477  |\n",
      "| vb_q1        | 0.00086  |\n",
      "| vb_q2        | 0.00088  |\n",
      "| vb_q3        | 0.00476  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.0975   |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.181    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.84e+03 |\n",
      "| step         | 960      |\n",
      "| vb           | 0.00212  |\n",
      "| vb_q0        | 0.00239  |\n",
      "| vb_q1        | 0.000803 |\n",
      "| vb_q2        | 0.000854 |\n",
      "| vb_q3        | 0.00503  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.157    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.194    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.88e+03 |\n",
      "| step         | 970      |\n",
      "| vb           | 0.00358  |\n",
      "| vb_q0        | 0.00983  |\n",
      "| vb_q1        | 0.000837 |\n",
      "| vb_q2        | 0.000861 |\n",
      "| vb_q3        | 0.00214  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.238    |\n",
      "| loss         | 0.183    |\n",
      "| loss_q0      | 0.227    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.178    |\n",
      "| mse_q0       | 0.209    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.92e+03 |\n",
      "| step         | 980      |\n",
      "| vb           | 0.00544  |\n",
      "| vb_q0        | 0.0183   |\n",
      "| vb_q1        | 0.00083  |\n",
      "| vb_q2        | 0.000849 |\n",
      "| vb_q3        | 0.00198  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.121    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.182    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 3.96e+03 |\n",
      "| step         | 990      |\n",
      "| vb           | 0.00173  |\n",
      "| vb_q0        | 0.00335  |\n",
      "| vb_q1        | 0.000833 |\n",
      "| vb_q2        | 0.000857 |\n",
      "| vb_q3        | 0.0024   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.136    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.176    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4e+03    |\n",
      "| step         | 1e+03    |\n",
      "| vb           | 0.0021   |\n",
      "| vb_q0        | 0.0022   |\n",
      "| vb_q1        | 0.000798 |\n",
      "| vb_q2        | 0.000845 |\n",
      "| vb_q3        | 0.00569  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.103    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.179    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.176    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.04e+03 |\n",
      "| step         | 1.01e+03 |\n",
      "| vb           | 0.0018   |\n",
      "| vb_q0        | 0.00269  |\n",
      "| vb_q1        | 0.000827 |\n",
      "| vb_q2        | 0.000832 |\n",
      "| vb_q3        | 0.00242  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.103    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.18     |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.08e+03 |\n",
      "| step         | 1.02e+03 |\n",
      "| vb           | 0.00225  |\n",
      "| vb_q0        | 0.00231  |\n",
      "| vb_q1        | 0.000841 |\n",
      "| vb_q2        | 0.000807 |\n",
      "| vb_q3        | 0.0052   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.159    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.193    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.12e+03 |\n",
      "| step         | 1.03e+03 |\n",
      "| vb           | 0.00283  |\n",
      "| vb_q0        | 0.0066   |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.000816 |\n",
      "| vb_q3        | 0.00552  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.125    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.16e+03 |\n",
      "| step         | 1.04e+03 |\n",
      "| vb           | 0.00161  |\n",
      "| vb_q0        | 0.00344  |\n",
      "| vb_q1        | 0.00084  |\n",
      "| vb_q2        | 0.000893 |\n",
      "| vb_q3        | 0.00158  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.115    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.2e+03  |\n",
      "| step         | 1.05e+03 |\n",
      "| vb           | 0.0021   |\n",
      "| vb_q0        | 0.00376  |\n",
      "| vb_q1        | 0.000839 |\n",
      "| vb_q2        | 0.000868 |\n",
      "| vb_q3        | 0.00313  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.188    |\n",
      "| loss         | 3.16     |\n",
      "| loss_q0      | 0.21     |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 13.4     |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.195    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.165    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.24e+03 |\n",
      "| step         | 1.06e+03 |\n",
      "| vb           | 2.98     |\n",
      "| vb_q0        | 0.0147   |\n",
      "| vb_q1        | 0.00083  |\n",
      "| vb_q2        | 0.00082  |\n",
      "| vb_q3        | 13.2     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.174    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.246    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.202    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.28e+03 |\n",
      "| step         | 1.07e+03 |\n",
      "| vb           | 0.00666  |\n",
      "| vb_q0        | 0.0435   |\n",
      "| vb_q1        | 0.000827 |\n",
      "| vb_q2        | 0.000828 |\n",
      "| vb_q3        | 0.00282  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.154    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.198    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.32e+03 |\n",
      "| step         | 1.08e+03 |\n",
      "| vb           | 0.00229  |\n",
      "| vb_q0        | 0.00611  |\n",
      "| vb_q1        | 0.000887 |\n",
      "| vb_q2        | 0.000841 |\n",
      "| vb_q3        | 0.00213  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.27     |\n",
      "| loss         | 0.181    |\n",
      "| loss_q0      | 0.229    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.205    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.36e+03 |\n",
      "| step         | 1.09e+03 |\n",
      "| vb           | 0.00592  |\n",
      "| vb_q0        | 0.024    |\n",
      "| vb_q1        | 0.000889 |\n",
      "| vb_q2        | 0.000857 |\n",
      "| vb_q3        | 0.00331  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.137    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.165    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.4e+03  |\n",
      "| step         | 1.1e+03  |\n",
      "| vb           | 0.00204  |\n",
      "| vb_q0        | 0.00393  |\n",
      "| vb_q1        | 0.000847 |\n",
      "| vb_q2        | 0.000825 |\n",
      "| vb_q3        | 0.00463  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.411    |\n",
      "| loss         | 0.213    |\n",
      "| loss_q0      | 0.344    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.179    |\n",
      "| mse_q0       | 0.214    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.4     |\n",
      "| samples      | 4.44e+03 |\n",
      "| step         | 1.11e+03 |\n",
      "| vb           | 0.0335   |\n",
      "| vb_q0        | 0.13     |\n",
      "| vb_q1        | 0.000843 |\n",
      "| vb_q2        | 0.000853 |\n",
      "| vb_q3        | 0.0023   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.202    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.48e+03 |\n",
      "| step         | 1.12e+03 |\n",
      "| vb           | 0.00214  |\n",
      "| vb_q0        | 0.00332  |\n",
      "| vb_q1        | 0.000883 |\n",
      "| vb_q2        | 0.000886 |\n",
      "| vb_q3        | 0.00349  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.153    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.186    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.167    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.166    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.52e+03 |\n",
      "| step         | 1.13e+03 |\n",
      "| vb           | 0.00373  |\n",
      "| vb_q0        | 0.00364  |\n",
      "| vb_q1        | 0.000843 |\n",
      "| vb_q2        | 0.000873 |\n",
      "| vb_q3        | 0.00669  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.174    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.202    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.56e+03 |\n",
      "| step         | 1.14e+03 |\n",
      "| vb           | 0.00346  |\n",
      "| vb_q0        | 0.00933  |\n",
      "| vb_q1        | 0.000825 |\n",
      "| vb_q2        | 0.00084  |\n",
      "| vb_q3        | 0.00652  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.156    |\n",
      "| loss         | 6.12     |\n",
      "| loss_q0      | 0.181    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 47.8     |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.165    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.6e+03  |\n",
      "| step         | 1.15e+03 |\n",
      "| vb           | 5.95     |\n",
      "| vb_q0        | 0.00301  |\n",
      "| vb_q1        | 0.000854 |\n",
      "| vb_q2        | 0.000924 |\n",
      "| vb_q3        | 47.6     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.152    |\n",
      "| loss         | 0.183    |\n",
      "| loss_q0      | 0.183    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.218    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.64e+03 |\n",
      "| step         | 1.16e+03 |\n",
      "| vb           | 0.012    |\n",
      "| vb_q0        | 0.00331  |\n",
      "| vb_q1        | 0.00085  |\n",
      "| vb_q2        | 0.000815 |\n",
      "| vb_q3        | 0.0527   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.175    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.189    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.167    |\n",
      "| loss_q3      | 0.179    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.166    |\n",
      "| mse_q3       | 0.165    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.68e+03 |\n",
      "| step         | 1.17e+03 |\n",
      "| vb           | 0.0045   |\n",
      "| vb_q0        | 0.00547  |\n",
      "| vb_q1        | 0.000831 |\n",
      "| vb_q2        | 0.000849 |\n",
      "| vb_q3        | 0.0136   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.118    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.187    |\n",
      "| loss_q1      | 0.168    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.177    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.167    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.72e+03 |\n",
      "| step         | 1.18e+03 |\n",
      "| vb           | 0.00469  |\n",
      "| vb_q0        | 0.00514  |\n",
      "| vb_q1        | 0.000814 |\n",
      "| vb_q2        | 0.000837 |\n",
      "| vb_q3        | 0.0107   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.68     |\n",
      "| loss         | 0.26     |\n",
      "| loss_q0      | 0.533    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.191    |\n",
      "| mse_q0       | 0.263    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.76e+03 |\n",
      "| step         | 1.19e+03 |\n",
      "| vb           | 0.0692   |\n",
      "| vb_q0        | 0.271    |\n",
      "| vb_q1        | 0.000838 |\n",
      "| vb_q2        | 0.000813 |\n",
      "| vb_q3        | 0.00639  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.245    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.204    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.198    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.8e+03  |\n",
      "| step         | 1.2e+03  |\n",
      "| vb           | 0.00298  |\n",
      "| vb_q0        | 0.00564  |\n",
      "| vb_q1        | 0.000815 |\n",
      "| vb_q2        | 0.000855 |\n",
      "| vb_q3        | 0.00602  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.14     |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.176    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.177    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.84e+03 |\n",
      "| step         | 1.21e+03 |\n",
      "| vb           | 0.00331  |\n",
      "| vb_q0        | 0.00177  |\n",
      "| vb_q1        | 0.000881 |\n",
      "| vb_q2        | 0.000859 |\n",
      "| vb_q3        | 0.00962  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.466    |\n",
      "| loss         | 0.194    |\n",
      "| loss_q0      | 0.253    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.183    |\n",
      "| mse_q0       | 0.219    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.88e+03 |\n",
      "| step         | 1.22e+03 |\n",
      "| vb           | 0.0111   |\n",
      "| vb_q0        | 0.0342   |\n",
      "| vb_q1        | 0.000869 |\n",
      "| vb_q2        | 0.000855 |\n",
      "| vb_q3        | 0.00192  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.211    |\n",
      "| loss         | 0.181    |\n",
      "| loss_q0      | 0.201    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.193    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.92e+03 |\n",
      "| step         | 1.23e+03 |\n",
      "| vb           | 0.00576  |\n",
      "| vb_q0        | 0.00727  |\n",
      "| vb_q1        | 0.000901 |\n",
      "| vb_q2        | 0.000818 |\n",
      "| vb_q3        | 0.0148   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.128    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.165    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 4.96e+03 |\n",
      "| step         | 1.24e+03 |\n",
      "| vb           | 0.0019   |\n",
      "| vb_q0        | 0.00271  |\n",
      "| vb_q1        | 0.000819 |\n",
      "| vb_q2        | 0.00086  |\n",
      "| vb_q3        | 0.00305  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.182    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.186    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.186    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 5e+03    |\n",
      "| step         | 1.25e+03 |\n",
      "| vb           | 0.00564  |\n",
      "| vb_q0        | 0.00369  |\n",
      "| vb_q1        | 0.000868 |\n",
      "| vb_q2        | 0.000882 |\n",
      "| vb_q3        | 0.0205   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.179    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.21     |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.199    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.5     |\n",
      "| samples      | 5.04e+03 |\n",
      "| step         | 1.26e+03 |\n",
      "| vb           | 0.00316  |\n",
      "| vb_q0        | 0.0109   |\n",
      "| vb_q1        | 0.000833 |\n",
      "| vb_q2        | 0.000821 |\n",
      "| vb_q3        | 0.00382  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.179    |\n",
      "| loss         | 0.187    |\n",
      "| loss_q0      | 0.193    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.219    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.188    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.165    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.08e+03 |\n",
      "| step         | 1.27e+03 |\n",
      "| vb           | 0.0118   |\n",
      "| vb_q0        | 0.00554  |\n",
      "| vb_q1        | 0.000809 |\n",
      "| vb_q2        | 0.000824 |\n",
      "| vb_q3        | 0.0534   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.218    |\n",
      "| loss         | 0.179    |\n",
      "| loss_q0      | 0.204    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.12e+03 |\n",
      "| step         | 1.28e+03 |\n",
      "| vb           | 0.00469  |\n",
      "| vb_q0        | 0.0115   |\n",
      "| vb_q1        | 0.000839 |\n",
      "| vb_q2        | 0.000795 |\n",
      "| vb_q3        | 0.00502  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.152    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.198    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.167    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.191    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.166    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.16e+03 |\n",
      "| step         | 1.29e+03 |\n",
      "| vb           | 0.00312  |\n",
      "| vb_q0        | 0.00742  |\n",
      "| vb_q1        | 0.000838 |\n",
      "| vb_q2        | 0.000814 |\n",
      "| vb_q3        | 0.00472  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.188    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.206    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.198    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.2e+03  |\n",
      "| step         | 1.3e+03  |\n",
      "| vb           | 0.00278  |\n",
      "| vb_q0        | 0.00858  |\n",
      "| vb_q1        | 0.000877 |\n",
      "| vb_q2        | 0.000819 |\n",
      "| vb_q3        | 0.00235  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.383    |\n",
      "| loss         | 0.207    |\n",
      "| loss_q0      | 0.385    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.211    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.24e+03 |\n",
      "| step         | 1.31e+03 |\n",
      "| vb           | 0.0316   |\n",
      "| vb_q0        | 0.174    |\n",
      "| vb_q1        | 0.000839 |\n",
      "| vb_q2        | 0.000878 |\n",
      "| vb_q3        | 0.00293  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.269    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.28e+03 |\n",
      "| step         | 1.32e+03 |\n",
      "| vb           | 0.00184  |\n",
      "| vb_q0        | 0.00312  |\n",
      "| vb_q1        | 0.00085  |\n",
      "| vb_q2        | 0.000844 |\n",
      "| vb_q3        | 0.00244  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.217    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.19     |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.32e+03 |\n",
      "| step         | 1.33e+03 |\n",
      "| vb           | 0.00354  |\n",
      "| vb_q0        | 0.00464  |\n",
      "| vb_q1        | 0.000857 |\n",
      "| vb_q2        | 0.000865 |\n",
      "| vb_q3        | 0.00614  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.173    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.193    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.188    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.36e+03 |\n",
      "| step         | 1.34e+03 |\n",
      "| vb           | 0.00299  |\n",
      "| vb_q0        | 0.00509  |\n",
      "| vb_q1        | 0.000891 |\n",
      "| vb_q2        | 0.000863 |\n",
      "| vb_q3        | 0.00381  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.121    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.18     |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.177    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.4e+03  |\n",
      "| step         | 1.35e+03 |\n",
      "| vb           | 0.0015   |\n",
      "| vb_q0        | 0.00285  |\n",
      "| vb_q1        | 0.00084  |\n",
      "| vb_q2        | 0.00084  |\n",
      "| vb_q3        | 0.00226  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.259    |\n",
      "| loss         | 0.184    |\n",
      "| loss_q0      | 0.201    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.197    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.189    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.44e+03 |\n",
      "| step         | 1.36e+03 |\n",
      "| vb           | 0.00953  |\n",
      "| vb_q0        | 0.0122   |\n",
      "| vb_q1        | 0.000875 |\n",
      "| vb_q2        | 0.000803 |\n",
      "| vb_q3        | 0.031    |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.219    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.202    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.192    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.48e+03 |\n",
      "| step         | 1.37e+03 |\n",
      "| vb           | 0.00395  |\n",
      "| vb_q0        | 0.0101   |\n",
      "| vb_q1        | 0.000827 |\n",
      "| vb_q2        | 0.000842 |\n",
      "| vb_q3        | 0.00646  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.224    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.205    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.168    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.197    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.6     |\n",
      "| samples      | 5.52e+03 |\n",
      "| step         | 1.38e+03 |\n",
      "| vb           | 0.00284  |\n",
      "| vb_q0        | 0.00776  |\n",
      "| vb_q1        | 0.000816 |\n",
      "| vb_q2        | 0.000804 |\n",
      "| vb_q3        | 0.00197  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.123    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.56e+03 |\n",
      "| step         | 1.39e+03 |\n",
      "| vb           | 0.00188  |\n",
      "| vb_q0        | 0.00231  |\n",
      "| vb_q1        | 0.000831 |\n",
      "| vb_q2        | 0.000829 |\n",
      "| vb_q3        | 0.00263  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.191    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.195    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.6e+03  |\n",
      "| step         | 1.4e+03  |\n",
      "| vb           | 0.00489  |\n",
      "| vb_q0        | 0.0101   |\n",
      "| vb_q1        | 0.000811 |\n",
      "| vb_q2        | 0.000911 |\n",
      "| vb_q3        | 0.00739  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.114    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.177    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.168    |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.64e+03 |\n",
      "| step         | 1.41e+03 |\n",
      "| vb           | 0.00358  |\n",
      "| vb_q0        | 0.00185  |\n",
      "| vb_q1        | 0.000851 |\n",
      "| vb_q2        | 0.000847 |\n",
      "| vb_q3        | 0.00888  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.163    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.19     |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.68e+03 |\n",
      "| step         | 1.42e+03 |\n",
      "| vb           | 0.00294  |\n",
      "| vb_q0        | 0.00633  |\n",
      "| vb_q1        | 0.000883 |\n",
      "| vb_q2        | 0.000817 |\n",
      "| vb_q3        | 0.00285  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.175    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.195    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.189    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.72e+03 |\n",
      "| step         | 1.43e+03 |\n",
      "| vb           | 0.00309  |\n",
      "| vb_q0        | 0.00529  |\n",
      "| vb_q1        | 0.000868 |\n",
      "| vb_q2        | 0.00085  |\n",
      "| vb_q3        | 0.00372  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.129    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.181    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.76e+03 |\n",
      "| step         | 1.44e+03 |\n",
      "| vb           | 0.00213  |\n",
      "| vb_q0        | 0.00277  |\n",
      "| vb_q1        | 0.000862 |\n",
      "| vb_q2        | 0.000843 |\n",
      "| vb_q3        | 0.00548  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.417    |\n",
      "| loss         | 0.247    |\n",
      "| loss_q0      | 0.406    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.183    |\n",
      "| mse_q0       | 0.214    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.8e+03  |\n",
      "| step         | 1.45e+03 |\n",
      "| vb           | 0.0634   |\n",
      "| vb_q0        | 0.192    |\n",
      "| vb_q1        | 0.000907 |\n",
      "| vb_q2        | 0.000831 |\n",
      "| vb_q3        | 0.00271  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.492    |\n",
      "| loss         | 0.185    |\n",
      "| loss_q0      | 0.211    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.202    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.84e+03 |\n",
      "| step         | 1.46e+03 |\n",
      "| vb           | 0.0045   |\n",
      "| vb_q0        | 0.00932  |\n",
      "| vb_q1        | 0.000867 |\n",
      "| vb_q2        | 0.000845 |\n",
      "| vb_q3        | 0.00442  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.226    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.181    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.88e+03 |\n",
      "| step         | 1.47e+03 |\n",
      "| vb           | 0.00262  |\n",
      "| vb_q0        | 0.00207  |\n",
      "| vb_q1        | 0.000851 |\n",
      "| vb_q2        | 0.000833 |\n",
      "| vb_q3        | 0.0064   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.189    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.188    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.184    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.92e+03 |\n",
      "| step         | 1.48e+03 |\n",
      "| vb           | 0.00264  |\n",
      "| vb_q0        | 0.00421  |\n",
      "| vb_q1        | 0.000778 |\n",
      "| vb_q2        | 0.000843 |\n",
      "| vb_q3        | 0.00437  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.371    |\n",
      "| loss         | 0.211    |\n",
      "| loss_q0      | 0.279    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.18     |\n",
      "| mse_q0       | 0.198    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.7     |\n",
      "| samples      | 5.96e+03 |\n",
      "| step         | 1.49e+03 |\n",
      "| vb           | 0.0312   |\n",
      "| vb_q0        | 0.0813   |\n",
      "| vb_q1        | 0.000867 |\n",
      "| vb_q2        | 0.000863 |\n",
      "| vb_q3        | 0.00232  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.186    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.2      |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.19     |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6e+03    |\n",
      "| step         | 1.5e+03  |\n",
      "| vb           | 0.00361  |\n",
      "| vb_q0        | 0.00952  |\n",
      "| vb_q1        | 0.000854 |\n",
      "| vb_q2        | 0.000887 |\n",
      "| vb_q3        | 0.00419  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.124    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.04e+03 |\n",
      "| step         | 1.51e+03 |\n",
      "| vb           | 0.00214  |\n",
      "| vb_q0        | 0.00308  |\n",
      "| vb_q1        | 0.000858 |\n",
      "| vb_q2        | 0.000801 |\n",
      "| vb_q3        | 0.0039   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.154    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.182    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.08e+03 |\n",
      "| step         | 1.52e+03 |\n",
      "| vb           | 0.00232  |\n",
      "| vb_q0        | 0.0032   |\n",
      "| vb_q1        | 0.00089  |\n",
      "| vb_q2        | 0.000835 |\n",
      "| vb_q3        | 0.00406  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.144    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.12e+03 |\n",
      "| step         | 1.53e+03 |\n",
      "| vb           | 0.00195  |\n",
      "| vb_q0        | 0.00317  |\n",
      "| vb_q1        | 0.000838 |\n",
      "| vb_q2        | 0.000872 |\n",
      "| vb_q3        | 0.00218  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.167    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.186    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.16e+03 |\n",
      "| step         | 1.54e+03 |\n",
      "| vb           | 0.00185  |\n",
      "| vb_q0        | 0.00322  |\n",
      "| vb_q1        | 0.000899 |\n",
      "| vb_q2        | 0.000864 |\n",
      "| vb_q3        | 0.00232  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.168    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.18     |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.2e+03  |\n",
      "| step         | 1.55e+03 |\n",
      "| vb           | 0.00167  |\n",
      "| vb_q0        | 0.00234  |\n",
      "| vb_q1        | 0.000917 |\n",
      "| vb_q2        | 0.000891 |\n",
      "| vb_q3        | 0.00202  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.554    |\n",
      "| loss         | 0.225    |\n",
      "| loss_q0      | 0.369    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.189    |\n",
      "| mse_q0       | 0.239    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.24e+03 |\n",
      "| step         | 1.56e+03 |\n",
      "| vb           | 0.0368   |\n",
      "| vb_q0        | 0.13     |\n",
      "| vb_q1        | 0.000876 |\n",
      "| vb_q2        | 0.00082  |\n",
      "| vb_q3        | 0.00254  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.345    |\n",
      "| loss         | 0.183    |\n",
      "| loss_q0      | 0.218    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.191    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.28e+03 |\n",
      "| step         | 1.57e+03 |\n",
      "| vb           | 0.00831  |\n",
      "| vb_q0        | 0.0267   |\n",
      "| vb_q1        | 0.00089  |\n",
      "| vb_q2        | 0.000909 |\n",
      "| vb_q3        | 0.00516  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.247    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.197    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.189    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.8     |\n",
      "| samples      | 6.32e+03 |\n",
      "| step         | 1.58e+03 |\n",
      "| vb           | 0.00306  |\n",
      "| vb_q0        | 0.0079   |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.00088  |\n",
      "| vb_q3        | 0.00352  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.12     |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.176    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.187    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.36e+03 |\n",
      "| step         | 1.59e+03 |\n",
      "| vb           | 0.00505  |\n",
      "| vb_q0        | 0.00174  |\n",
      "| vb_q1        | 0.000834 |\n",
      "| vb_q2        | 0.000873 |\n",
      "| vb_q3        | 0.0186   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.163    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.201    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.194    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.4e+03  |\n",
      "| step         | 1.6e+03  |\n",
      "| vb           | 0.00242  |\n",
      "| vb_q0        | 0.00656  |\n",
      "| vb_q1        | 0.000853 |\n",
      "| vb_q2        | 0.000907 |\n",
      "| vb_q3        | 0.00432  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.151    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.188    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.44e+03 |\n",
      "| step         | 1.61e+03 |\n",
      "| vb           | 0.00288  |\n",
      "| vb_q0        | 0.00476  |\n",
      "| vb_q1        | 0.000872 |\n",
      "| vb_q2        | 0.000874 |\n",
      "| vb_q3        | 0.00426  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.129    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.186    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.182    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.48e+03 |\n",
      "| step         | 1.62e+03 |\n",
      "| vb           | 0.00584  |\n",
      "| vb_q0        | 0.00468  |\n",
      "| vb_q1        | 0.000868 |\n",
      "| vb_q2        | 0.000873 |\n",
      "| vb_q3        | 0.0138   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.172    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.192    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.186    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.52e+03 |\n",
      "| step         | 1.63e+03 |\n",
      "| vb           | 0.00233  |\n",
      "| vb_q0        | 0.00538  |\n",
      "| vb_q1        | 0.000849 |\n",
      "| vb_q2        | 0.000864 |\n",
      "| vb_q3        | 0.00201  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.121    |\n",
      "| loss         | 0.179    |\n",
      "| loss_q0      | 0.181    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.195    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.56e+03 |\n",
      "| step         | 1.64e+03 |\n",
      "| vb           | 0.0095   |\n",
      "| vb_q0        | 0.00225  |\n",
      "| vb_q1        | 0.000899 |\n",
      "| vb_q2        | 0.000821 |\n",
      "| vb_q3        | 0.027    |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.133    |\n",
      "| loss         | 3.21     |\n",
      "| loss_q0      | 0.206    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 7.76     |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.196    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.6e+03  |\n",
      "| step         | 1.65e+03 |\n",
      "| vb           | 3.04     |\n",
      "| vb_q0        | 0.0105   |\n",
      "| vb_q1        | 0.000895 |\n",
      "| vb_q2        | 0.000882 |\n",
      "| vb_q3        | 7.59     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.125    |\n",
      "| loss         | 3.22     |\n",
      "| loss_q0      | 0.183    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 8.31     |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.64e+03 |\n",
      "| step         | 1.66e+03 |\n",
      "| vb           | 3.05     |\n",
      "| vb_q0        | 0.00259  |\n",
      "| vb_q1        | 0.000814 |\n",
      "| vb_q2        | 0.000866 |\n",
      "| vb_q3        | 8.14     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.151    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.191    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.184    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.68e+03 |\n",
      "| step         | 1.67e+03 |\n",
      "| vb           | 0.00373  |\n",
      "| vb_q0        | 0.00778  |\n",
      "| vb_q1        | 0.000839 |\n",
      "| vb_q2        | 0.000861 |\n",
      "| vb_q3        | 0.00524  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.148    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.72e+03 |\n",
      "| step         | 1.68e+03 |\n",
      "| vb           | 0.00245  |\n",
      "| vb_q0        | 0.00329  |\n",
      "| vb_q1        | 0.000921 |\n",
      "| vb_q2        | 0.000865 |\n",
      "| vb_q3        | 0.0045   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.138    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.183    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 93.9     |\n",
      "| samples      | 6.76e+03 |\n",
      "| step         | 1.69e+03 |\n",
      "| vb           | 0.00275  |\n",
      "| vb_q0        | 0.00268  |\n",
      "| vb_q1        | 0.000868 |\n",
      "| vb_q2        | 0.000821 |\n",
      "| vb_q3        | 0.00534  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.146    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.187    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 6.8e+03  |\n",
      "| step         | 1.7e+03  |\n",
      "| vb           | 0.0023   |\n",
      "| vb_q0        | 0.00405  |\n",
      "| vb_q1        | 0.000842 |\n",
      "| vb_q2        | 0.000844 |\n",
      "| vb_q3        | 0.00506  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.17     |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.187    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 6.84e+03 |\n",
      "| step         | 1.71e+03 |\n",
      "| vb           | 0.00178  |\n",
      "| vb_q0        | 0.00385  |\n",
      "| vb_q1        | 0.000864 |\n",
      "| vb_q2        | 0.000843 |\n",
      "| vb_q3        | 0.00136  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.211    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.188    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 6.88e+03 |\n",
      "| step         | 1.72e+03 |\n",
      "| vb           | 0.00323  |\n",
      "| vb_q0        | 0.00452  |\n",
      "| vb_q1        | 0.000859 |\n",
      "| vb_q2        | 0.000828 |\n",
      "| vb_q3        | 0.0049   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.174    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.182    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.185    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 6.92e+03 |\n",
      "| step         | 1.73e+03 |\n",
      "| vb           | 0.00403  |\n",
      "| vb_q0        | 0.00277  |\n",
      "| vb_q1        | 0.000899 |\n",
      "| vb_q2        | 0.000854 |\n",
      "| vb_q3        | 0.0172   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.209    |\n",
      "| loss         | 0.179    |\n",
      "| loss_q0      | 0.195    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.184    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 6.96e+03 |\n",
      "| step         | 1.74e+03 |\n",
      "| vb           | 0.006    |\n",
      "| vb_q0        | 0.0114   |\n",
      "| vb_q1        | 0.000799 |\n",
      "| vb_q2        | 0.000838 |\n",
      "| vb_q3        | 0.00753  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.156    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.181    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.181    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 7e+03    |\n",
      "| step         | 1.75e+03 |\n",
      "| vb           | 0.00593  |\n",
      "| vb_q0        | 0.00264  |\n",
      "| vb_q1        | 0.00093  |\n",
      "| vb_q2        | 0.000827 |\n",
      "| vb_q3        | 0.0137   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.121    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.174    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.173    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 7.04e+03 |\n",
      "| step         | 1.76e+03 |\n",
      "| vb           | 0.0013   |\n",
      "| vb_q0        | 0.00164  |\n",
      "| vb_q1        | 0.000873 |\n",
      "| vb_q2        | 0.000839 |\n",
      "| vb_q3        | 0.00226  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.138    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.194    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.187    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 7.08e+03 |\n",
      "| step         | 1.77e+03 |\n",
      "| vb           | 0.00267  |\n",
      "| vb_q0        | 0.00659  |\n",
      "| vb_q1        | 0.00084  |\n",
      "| vb_q2        | 0.000831 |\n",
      "| vb_q3        | 0.00306  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.105    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.179    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.176    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94       |\n",
      "| samples      | 7.12e+03 |\n",
      "| step         | 1.78e+03 |\n",
      "| vb           | 0.00187  |\n",
      "| vb_q0        | 0.00264  |\n",
      "| vb_q1        | 0.000864 |\n",
      "| vb_q2        | 0.000895 |\n",
      "| vb_q3        | 0.00372  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.23     |\n",
      "| loss         | 0.188    |\n",
      "| loss_q0      | 0.202    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.215    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.189    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.16e+03 |\n",
      "| step         | 1.79e+03 |\n",
      "| vb           | 0.0137   |\n",
      "| vb_q0        | 0.0131   |\n",
      "| vb_q1        | 0.000866 |\n",
      "| vb_q2        | 0.000806 |\n",
      "| vb_q3        | 0.047    |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.198    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.189    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.2e+03  |\n",
      "| step         | 1.8e+03  |\n",
      "| vb           | 0.00222  |\n",
      "| vb_q0        | 0.0046   |\n",
      "| vb_q1        | 0.000908 |\n",
      "| vb_q2        | 0.000879 |\n",
      "| vb_q3        | 0.00476  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.209    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.24e+03 |\n",
      "| step         | 1.81e+03 |\n",
      "| vb           | 0.00207  |\n",
      "| vb_q0        | 0.00362  |\n",
      "| vb_q1        | 0.000841 |\n",
      "| vb_q2        | 0.00084  |\n",
      "| vb_q3        | 0.00244  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.534    |\n",
      "| loss         | 0.198    |\n",
      "| loss_q0      | 0.283    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.184    |\n",
      "| mse_q0       | 0.231    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.28e+03 |\n",
      "| step         | 1.82e+03 |\n",
      "| vb           | 0.014    |\n",
      "| vb_q0        | 0.0525   |\n",
      "| vb_q1        | 0.00084  |\n",
      "| vb_q2        | 0.000873 |\n",
      "| vb_q3        | 0.00561  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.336    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.186    |\n",
      "| loss_q1      | 0.174    |\n",
      "| loss_q2      | 0.172    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.174    |\n",
      "| mse_q2       | 0.171    |\n",
      "| mse_q3       | 0.17     |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.32e+03 |\n",
      "| step         | 1.83e+03 |\n",
      "| vb           | 0.00166  |\n",
      "| vb_q0        | 0.00306  |\n",
      "| vb_q1        | 0.000882 |\n",
      "| vb_q2        | 0.000868 |\n",
      "| vb_q3        | 0.00248  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.342    |\n",
      "| loss         | 0.186    |\n",
      "| loss_q0      | 0.243    |\n",
      "| loss_q1      | 0.173    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.177    |\n",
      "| mse_q0       | 0.208    |\n",
      "| mse_q1       | 0.172    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.36e+03 |\n",
      "| step         | 1.84e+03 |\n",
      "| vb           | 0.00848  |\n",
      "| vb_q0        | 0.0355   |\n",
      "| vb_q1        | 0.000918 |\n",
      "| vb_q2        | 0.000899 |\n",
      "| vb_q3        | 0.00352  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.188    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.172    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.171    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.4e+03  |\n",
      "| step         | 1.85e+03 |\n",
      "| vb           | 0.00145  |\n",
      "| vb_q0        | 0.00283  |\n",
      "| vb_q1        | 0.000841 |\n",
      "| vb_q2        | 0.000875 |\n",
      "| vb_q3        | 0.00175  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.164    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.191    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.171    |\n",
      "| loss_q3      | 0.178    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.186    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.169    |\n",
      "| param_norm   | 94.1     |\n",
      "| samples      | 7.44e+03 |\n",
      "| step         | 1.86e+03 |\n",
      "| vb           | 0.00434  |\n",
      "| vb_q0        | 0.00463  |\n",
      "| vb_q1        | 0.000857 |\n",
      "| vb_q2        | 0.000866 |\n",
      "| vb_q3        | 0.00923  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.179    |\n",
      "| loss         | 0.178    |\n",
      "| loss_q0      | 0.196    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.175    |\n",
      "| mse_q0       | 0.188    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.17     |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.48e+03 |\n",
      "| step         | 1.87e+03 |\n",
      "| vb           | 0.00301  |\n",
      "| vb_q0        | 0.00781  |\n",
      "| vb_q1        | 0.000842 |\n",
      "| vb_q2        | 0.000836 |\n",
      "| vb_q3        | 0.00161  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.161    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.189    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.52e+03 |\n",
      "| step         | 1.88e+03 |\n",
      "| vb           | 0.00294  |\n",
      "| vb_q0        | 0.00639  |\n",
      "| vb_q1        | 0.000867 |\n",
      "| vb_q2        | 0.000818 |\n",
      "| vb_q3        | 0.00227  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.121    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.18     |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.171    |\n",
      "| mse_q0       | 0.177    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.56e+03 |\n",
      "| step         | 1.89e+03 |\n",
      "| vb           | 0.00185  |\n",
      "| vb_q0        | 0.00287  |\n",
      "| vb_q1        | 0.000834 |\n",
      "| vb_q2        | 0.000846 |\n",
      "| vb_q3        | 0.00262  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.131    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.181    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.6e+03  |\n",
      "| step         | 1.9e+03  |\n",
      "| vb           | 0.00329  |\n",
      "| vb_q0        | 0.00377  |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.000884 |\n",
      "| vb_q3        | 0.00625  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.108    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.177    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.183    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.64e+03 |\n",
      "| step         | 1.91e+03 |\n",
      "| vb           | 0.00585  |\n",
      "| vb_q0        | 0.0022   |\n",
      "| vb_q1        | 0.000813 |\n",
      "| vb_q2        | 0.000946 |\n",
      "| vb_q3        | 0.0152   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.0864   |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.178    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.176    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.68e+03 |\n",
      "| step         | 1.92e+03 |\n",
      "| vb           | 0.00433  |\n",
      "| vb_q0        | 0.00197  |\n",
      "| vb_q1        | 0.000834 |\n",
      "| vb_q2        | 0.000828 |\n",
      "| vb_q3        | 0.0103   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.238    |\n",
      "| loss         | 0.185    |\n",
      "| loss_q0      | 0.222    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.176    |\n",
      "| mse_q0       | 0.199    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.72e+03 |\n",
      "| step         | 1.93e+03 |\n",
      "| vb           | 0.00947  |\n",
      "| vb_q0        | 0.0237   |\n",
      "| vb_q1        | 0.000765 |\n",
      "| vb_q2        | 0.000847 |\n",
      "| vb_q3        | 0.00708  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.136    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.76e+03 |\n",
      "| step         | 1.94e+03 |\n",
      "| vb           | 0.00264  |\n",
      "| vb_q0        | 0.00425  |\n",
      "| vb_q1        | 0.000867 |\n",
      "| vb_q2        | 0.000957 |\n",
      "| vb_q3        | 0.00459  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.137    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.191    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.186    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.8e+03  |\n",
      "| step         | 1.95e+03 |\n",
      "| vb           | 0.00263  |\n",
      "| vb_q0        | 0.00499  |\n",
      "| vb_q1        | 0.000844 |\n",
      "| vb_q2        | 0.000836 |\n",
      "| vb_q3        | 0.00515  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.137    |\n",
      "| loss         | 0.175    |\n",
      "| loss_q0      | 0.184    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.172    |\n",
      "| mse_q0       | 0.18     |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.2     |\n",
      "| samples      | 7.84e+03 |\n",
      "| step         | 1.96e+03 |\n",
      "| vb           | 0.00287  |\n",
      "| vb_q0        | 0.00369  |\n",
      "| vb_q1        | 0.00086  |\n",
      "| vb_q2        | 0.000829 |\n",
      "| vb_q3        | 0.00497  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.12     |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.204    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.195    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 7.88e+03 |\n",
      "| step         | 1.97e+03 |\n",
      "| vb           | 0.00284  |\n",
      "| vb_q0        | 0.00824  |\n",
      "| vb_q1        | 0.000875 |\n",
      "| vb_q2        | 0.000794 |\n",
      "| vb_q3        | 0.00343  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.171    |\n",
      "| loss         | 0.179    |\n",
      "| loss_q0      | 0.195    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.175    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.186    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 7.92e+03 |\n",
      "| step         | 1.98e+03 |\n",
      "| vb           | 0.00555  |\n",
      "| vb_q0        | 0.00984  |\n",
      "| vb_q1        | 0.000905 |\n",
      "| vb_q2        | 0.000865 |\n",
      "| vb_q3        | 0.00807  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.143    |\n",
      "| loss         | 0.177    |\n",
      "| loss_q0      | 0.192    |\n",
      "| loss_q1      | 0.171    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.171    |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 7.96e+03 |\n",
      "| step         | 1.99e+03 |\n",
      "| vb           | 0.00329  |\n",
      "| vb_q0        | 0.00717  |\n",
      "| vb_q1        | 0.000916 |\n",
      "| vb_q2        | 0.00087  |\n",
      "| vb_q3        | 0.00319  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.229    |\n",
      "| loss         | 3.16     |\n",
      "| loss_q0      | 0.198    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 13.4     |\n",
      "| mse          | 0.174    |\n",
      "| mse_q0       | 0.188    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8e+03    |\n",
      "| step         | 2e+03    |\n",
      "| vb           | 2.99     |\n",
      "| vb_q0        | 0.00933  |\n",
      "| vb_q1        | 0.000855 |\n",
      "| vb_q2        | 0.000811 |\n",
      "| vb_q3        | 13.3     |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.164    |\n",
      "| loss         | 0.172    |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.168    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.04e+03 |\n",
      "| step         | 2.01e+03 |\n",
      "| vb           | 0.00202  |\n",
      "| vb_q0        | 0.00241  |\n",
      "| vb_q1        | 0.000849 |\n",
      "| vb_q2        | 0.000891 |\n",
      "| vb_q3        | 0.00381  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.241    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.191    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.17     |\n",
      "| loss_q3      | 0.172    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.185    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.169    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.08e+03 |\n",
      "| step         | 2.02e+03 |\n",
      "| vb           | 0.00277  |\n",
      "| vb_q0        | 0.00533  |\n",
      "| vb_q1        | 0.000869 |\n",
      "| vb_q2        | 0.000858 |\n",
      "| vb_q3        | 0.00473  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.137    |\n",
      "| loss         | 0.174    |\n",
      "| loss_q0      | 0.182    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.183    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.12e+03 |\n",
      "| step         | 2.03e+03 |\n",
      "| vb           | 0.00416  |\n",
      "| vb_q0        | 0.00308  |\n",
      "| vb_q1        | 0.000827 |\n",
      "| vb_q2        | 0.000868 |\n",
      "| vb_q3        | 0.0154   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.0877   |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.17     |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.174    |\n",
      "| mse          | 0.168    |\n",
      "| mse_q0       | 0.169    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.16e+03 |\n",
      "| step         | 2.04e+03 |\n",
      "| vb           | 0.00263  |\n",
      "| vb_q0        | 0.00114  |\n",
      "| vb_q1        | 0.000857 |\n",
      "| vb_q2        | 0.000818 |\n",
      "| vb_q3        | 0.00675  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.133    |\n",
      "| loss         | 0.176    |\n",
      "| loss_q0      | 0.189    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 0.178    |\n",
      "| mse          | 0.173    |\n",
      "| mse_q0       | 0.184    |\n",
      "| mse_q1       | 0.17     |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.2e+03  |\n",
      "| step         | 2.05e+03 |\n",
      "| vb           | 0.00332  |\n",
      "| vb_q0        | 0.00493  |\n",
      "| vb_q1        | 0.000873 |\n",
      "| vb_q2        | 0.000817 |\n",
      "| vb_q3        | 0.0109   |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.101    |\n",
      "| loss         | 3.16     |\n",
      "| loss_q0      | 0.185    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.169    |\n",
      "| loss_q3      | 10.1     |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.179    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.24e+03 |\n",
      "| step         | 2.06e+03 |\n",
      "| vb           | 2.99     |\n",
      "| vb_q0        | 0.00509  |\n",
      "| vb_q1        | 0.000853 |\n",
      "| vb_q2        | 0.000856 |\n",
      "| vb_q3        | 9.95     |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.116    |\n",
      "| loss         | 0.173    |\n",
      "| loss_q0      | 0.191    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.173    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.183    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.3     |\n",
      "| samples      | 8.28e+03 |\n",
      "| step         | 2.07e+03 |\n",
      "| vb           | 0.00345  |\n",
      "| vb_q0        | 0.00868  |\n",
      "| vb_q1        | 0.000833 |\n",
      "| vb_q2        | 0.000838 |\n",
      "| vb_q3        | 0.00563  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.0812   |\n",
      "| loss         | 0.17     |\n",
      "| loss_q0      | 0.178    |\n",
      "| loss_q1      | 0.17     |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.176    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.4     |\n",
      "| samples      | 8.32e+03 |\n",
      "| step         | 2.08e+03 |\n",
      "| vb           | 0.00134  |\n",
      "| vb_q0        | 0.00228  |\n",
      "| vb_q1        | 0.000821 |\n",
      "| vb_q2        | 0.000872 |\n",
      "| vb_q3        | 0.00182  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.114    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.179    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.169    |\n",
      "| mse          | 0.17     |\n",
      "| mse_q0       | 0.175    |\n",
      "| mse_q1       | 0.169    |\n",
      "| mse_q2       | 0.168    |\n",
      "| mse_q3       | 0.167    |\n",
      "| param_norm   | 94.4     |\n",
      "| samples      | 8.36e+03 |\n",
      "| step         | 2.09e+03 |\n",
      "| vb           | 0.00157  |\n",
      "| vb_q0        | 0.00325  |\n",
      "| vb_q1        | 0.000846 |\n",
      "| vb_q2        | 0.000861 |\n",
      "| vb_q3        | 0.00174  |\n",
      "---------------------------\n",
      "---------------------------\n",
      "| cond_vf_mean | 0.5      |\n",
      "| cond_ym_mean | 0.5      |\n",
      "| grad_norm    | 0.091    |\n",
      "| loss         | 0.171    |\n",
      "| loss_q0      | 0.183    |\n",
      "| loss_q1      | 0.169    |\n",
      "| loss_q2      | 0.168    |\n",
      "| loss_q3      | 0.17     |\n",
      "| mse          | 0.169    |\n",
      "| mse_q0       | 0.178    |\n",
      "| mse_q1       | 0.168    |\n",
      "| mse_q2       | 0.167    |\n",
      "| mse_q3       | 0.166    |\n",
      "| param_norm   | 94.4     |\n",
      "| samples      | 8.4e+03  |\n",
      "| step         | 2.1e+03  |\n",
      "| vb           | 0.00216  |\n",
      "| vb_q0        | 0.00514  |\n",
      "| vb_q1        | 0.000821 |\n",
      "| vb_q2        | 0.000886 |\n",
      "| vb_q3        | 0.00379  |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "saving model 0.9999...\n",
      "Skipping meta save (no dataset): 'generator' object has no attribute 'dataset'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/scripts/image_train.py:101\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/scripts/image_train.py:48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data(\n\u001b[1;32m     41\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[1;32m     42\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     43\u001b[0m     image_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mimage_size,\n\u001b[1;32m     44\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mTrainLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmicrobatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmicrobatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:156\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    153\u001b[0m ):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# __getitem__()에서 (volume, cond_dict)을 반환한다고 가정\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     batch, cond_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    158\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdumpkvs()\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:170\u001b[0m, in \u001b[0;36mTrainLoop.run_step\u001b[0;34m(self, batch, cond_dict)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, cond_dict):\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     took_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_trainer\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m took_step:\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/train_util.py:232\u001b[0m, in \u001b[0;36mTrainLoop.forward_backward\u001b[0;34m(self, batch, cond_dict)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# 🔧 로그에 더 자세한 정보 포함\u001b[39;00m\n\u001b[1;32m    223\u001b[0m log_loss_dict(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion,\n\u001b[1;32m    225\u001b[0m     t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     }\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Euihyun/3D_TPMS_topoDIff/topodiff/topodiff/fp16_util.py:181\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer.backward\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    179\u001b[0m     (loss \u001b[38;5;241m*\u001b[39m loss_scale)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "# 프로젝트 루트 경로를 정확히 지정\n",
    "sys.path.insert(0, '/home/yeoneung/Euihyun/3D_TPMS_topoDIff')\n",
    "os.environ['TOPODIFF_LOGDIR'] = './checkpoints/3d_diff_logdir10'\n",
    "\n",
    "\n",
    "# 모든 학습 파라미터 포함\n",
    "TRAIN_FLAGS = \"\"\"\n",
    "--batch_size 4\n",
    "--save_interval 100\n",
    "--use_fp16 False \n",
    "--lr 5e-5 \n",
    "--weight_decay 0.01\n",
    "--ema_rate 0.9999\n",
    "--log_interval 10\n",
    "--microbatch 2\n",
    "--schedule_sampler uniform\n",
    "--resume_checkpoint \"\"\n",
    "\"\"\"\n",
    "\n",
    "MODEL_FLAGS = \"--image_size 64 --num_channels 32 --num_res_blocks 2 --learn_sigma True --dropout 0.1 --use_checkpoint True\"\n",
    "DIFFUSION_FLAGS = \"--diffusion_steps 1000 --noise_schedule cosine\"\n",
    "DATA_FLAGS = \"--data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\"\n",
    "\n",
    "%run scripts/image_train.py \\\n",
    "  $MODEL_FLAGS \\\n",
    "  $DIFFUSION_FLAGS \\\n",
    "  $TRAIN_FLAGS \\\n",
    "  --data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c49efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/topodiff-2025-06-10-16-39-01-702738\n",
      "Creating model and diffusion...\n",
      "Loading model from ./checkpoints/3d_diff_logdir10/ema_0.9999_002100.pt...\n",
      "No meta.pt found, calculating from data...\n",
      "Calculating VF/YM ranges from data...\n",
      "Calculated VF range: (0.07453536987304688, 0.8730812072753906)\n",
      "Calculated YM range: (0.0, 1154.5745849609375)\n",
      "Starting sampling...\n",
      "Target conditions: VF=0.3, YM=50.0\n",
      "Using ranges: VF=(0.07453536987304688, 0.8730812072753906), YM=(0.0, 1154.5745849609375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Unconditional sampling (ignoring VF=0.3, YM=50.0)\n",
      "Sampling shape: (1, 1, 64, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples shape: torch.Size([1, 1, 64, 64, 64])\n",
      "Sample stats: mean=0.000, std=0.720\n",
      "Sample range: [-1.000, 1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating samples: 100%|██████████| 1/1 [00:09<00:00,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving samples...\n",
      "Saving 1 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualizations for 1 samples...\n",
      "Saved visualizations to ./generated/samples/vf0.3_ym50/visualizations\n",
      "Sampling complete! Saved to ./generated/samples/vf0.3_ym50\n"
     ]
    }
   ],
   "source": [
    "# Jupyter에서 실행\n",
    "import sys, os\n",
    "sys.path.insert(0, '/home/yeoneung/Euihyun/3D_TPMS_topoDIff')\n",
    "\n",
    "MODEL_FLAGS = \"\"\"\n",
    "--image_size 64 \n",
    "--num_channels 32\n",
    "--num_res_blocks 2\n",
    "--attention_resolutions 16,8\n",
    "--channel_mult 1,2,2,4\n",
    "--learn_sigma True\n",
    "--use_fp16 False\n",
    "\"\"\"\n",
    "\n",
    "DIFFUSION_FLAGS = \"\"\"\n",
    "--diffusion_steps 1000\n",
    "--timestep_respacing 100\n",
    "--noise_schedule cosine\n",
    "\"\"\"\n",
    "\n",
    "SAMPLE_FLAGS = \"\"\"\n",
    "--num_samples 1\n",
    "--batch_size 4\n",
    "--target_vf 0.3\n",
    "--target_ym 50.0\n",
    "--output_dir ./generated/samples/vf0.3_ym50\n",
    "--visualize True\n",
    "--clip_denoised True\n",
    "\"\"\"\n",
    "\n",
    "PATHS = \"--model_path ./checkpoints/3d_diff_logdir10/ema_0.9999_002100.pt\"\n",
    "\n",
    "%run scripts/topodiff_sample.py $MODEL_FLAGS $DIFFUSION_FLAGS $SAMPLE_FLAGS $PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f6b56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure mean: 0.000\n",
      "Structure std:  0.720\n",
      "Min / Max:      -1.000 / 1.000\n",
      "Actual VF (vol>0): 0.500\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 사용자 환경에 맞게 파일 경로 수정 ===\n",
    "file_path = '/home/yeoneung/Euihyun/3D_TPMS_topoDIff/topodiff/generated/samples/vf0.3_ym50/generated_vf0.300_ym50.0_sample0000.npz'\n",
    "\n",
    "# 1) .npz 파일 로드\n",
    "data = np.load(file_path)\n",
    "vol = data['structure']  # (64,64,64)\n",
    "\n",
    "# 2) 주요 통계 지표 계산\n",
    "mean_val = vol.mean()\n",
    "std_val = vol.std()\n",
    "min_val = vol.min()\n",
    "max_val = vol.max()\n",
    "# 임계치(>0) 기준 실제 VF 계산\n",
    "actual_vf = float((vol > 0).mean())\n",
    "\n",
    "print(f\"Structure mean: {mean_val:.3f}\")\n",
    "print(f\"Structure std:  {std_val:.3f}\")\n",
    "print(f\"Min / Max:      {min_val:.3f} / {max_val:.3f}\")\n",
    "print(f\"Actual VF (vol>0): {actual_vf:.3f}\")\n",
    "\n",
    "# 3) 히스토그램 그리기\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(vol.flatten(), bins=50)\n",
    "plt.title('Distribution of Implicit Field Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) 세 방향 슬라이스 다시 시각화 (컬러맵과 축 제거)\n",
    "slices = {\n",
    "    'X=32': vol[32, :, :],\n",
    "    'Y=32': vol[:, 32, :],\n",
    "    'Z=32': vol[:, :, 32],\n",
    "}\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "for ax, (title, slc) in zip(axes, slices.items()):\n",
    "    im = ax.imshow(slc, interpolation='nearest')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28ddf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 학습된 모델 분석 시작...\n",
      "\n",
      "1️⃣ 모델 로드\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3695256/3728220379.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = th.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UNetModel:\n\tMissing key(s) in state_dict: \"input_blocks.3.0.in_layers.0.weight\", \"input_blocks.3.0.in_layers.0.bias\", \"input_blocks.3.0.in_layers.2.weight\", \"input_blocks.3.0.in_layers.2.bias\", \"input_blocks.3.0.emb_layers.1.weight\", \"input_blocks.3.0.emb_layers.1.bias\", \"input_blocks.3.0.out_layers.0.weight\", \"input_blocks.3.0.out_layers.0.bias\", \"input_blocks.3.0.out_layers.3.weight\", \"input_blocks.3.0.out_layers.3.bias\", \"input_blocks.4.0.op.weight\", \"input_blocks.4.0.op.bias\", \"input_blocks.5.0.skip_connection.weight\", \"input_blocks.5.0.skip_connection.bias\", \"input_blocks.6.0.in_layers.0.weight\", \"input_blocks.6.0.in_layers.0.bias\", \"input_blocks.6.0.in_layers.2.weight\", \"input_blocks.6.0.in_layers.2.bias\", \"input_blocks.6.0.emb_layers.1.weight\", \"input_blocks.6.0.emb_layers.1.bias\", \"input_blocks.6.0.out_layers.0.weight\", \"input_blocks.6.0.out_layers.0.bias\", \"input_blocks.6.0.out_layers.3.weight\", \"input_blocks.6.0.out_layers.3.bias\", \"input_blocks.8.0.op.weight\", \"input_blocks.8.0.op.bias\", \"input_blocks.9.0.in_layers.0.weight\", \"input_blocks.9.0.in_layers.0.bias\", \"input_blocks.9.0.in_layers.2.weight\", \"input_blocks.9.0.in_layers.2.bias\", \"input_blocks.9.0.emb_layers.1.weight\", \"input_blocks.9.0.emb_layers.1.bias\", \"input_blocks.9.0.out_layers.0.weight\", \"input_blocks.9.0.out_layers.0.bias\", \"input_blocks.9.0.out_layers.3.weight\", \"input_blocks.9.0.out_layers.3.bias\", \"input_blocks.9.1.norm.weight\", \"input_blocks.9.1.norm.bias\", \"input_blocks.9.1.qkv.weight\", \"input_blocks.9.1.qkv.bias\", \"input_blocks.9.1.proj_out.weight\", \"input_blocks.9.1.proj_out.bias\", \"input_blocks.12.0.op.weight\", \"input_blocks.12.0.op.bias\", \"input_blocks.13.0.in_layers.0.weight\", \"input_blocks.13.0.in_layers.0.bias\", \"input_blocks.13.0.in_layers.2.weight\", \"input_blocks.13.0.in_layers.2.bias\", \"input_blocks.13.0.emb_layers.1.weight\", \"input_blocks.13.0.emb_layers.1.bias\", \"input_blocks.13.0.out_layers.0.weight\", \"input_blocks.13.0.out_layers.0.bias\", \"input_blocks.13.0.out_layers.3.weight\", \"input_blocks.13.0.out_layers.3.bias\", \"input_blocks.13.0.skip_connection.weight\", \"input_blocks.13.0.skip_connection.bias\", \"input_blocks.13.1.norm.weight\", \"input_blocks.13.1.norm.bias\", \"input_blocks.13.1.qkv.weight\", \"input_blocks.13.1.qkv.bias\", \"input_blocks.13.1.proj_out.weight\", \"input_blocks.13.1.proj_out.bias\", \"input_blocks.14.0.in_layers.0.weight\", \"input_blocks.14.0.in_layers.0.bias\", \"input_blocks.14.0.in_layers.2.weight\", \"input_blocks.14.0.in_layers.2.bias\", \"input_blocks.14.0.emb_layers.1.weight\", \"input_blocks.14.0.emb_layers.1.bias\", \"input_blocks.14.0.out_layers.0.weight\", \"input_blocks.14.0.out_layers.0.bias\", \"input_blocks.14.0.out_layers.3.weight\", \"input_blocks.14.0.out_layers.3.bias\", \"input_blocks.14.1.norm.weight\", \"input_blocks.14.1.norm.bias\", \"input_blocks.14.1.qkv.weight\", \"input_blocks.14.1.qkv.bias\", \"input_blocks.14.1.proj_out.weight\", \"input_blocks.14.1.proj_out.bias\", \"input_blocks.15.0.in_layers.0.weight\", \"input_blocks.15.0.in_layers.0.bias\", \"input_blocks.15.0.in_layers.2.weight\", \"input_blocks.15.0.in_layers.2.bias\", \"input_blocks.15.0.emb_layers.1.weight\", \"input_blocks.15.0.emb_layers.1.bias\", \"input_blocks.15.0.out_layers.0.weight\", \"input_blocks.15.0.out_layers.0.bias\", \"input_blocks.15.0.out_layers.3.weight\", \"input_blocks.15.0.out_layers.3.bias\", \"input_blocks.15.1.norm.weight\", \"input_blocks.15.1.norm.bias\", \"input_blocks.15.1.qkv.weight\", \"input_blocks.15.1.qkv.bias\", \"input_blocks.15.1.proj_out.weight\", \"input_blocks.15.1.proj_out.bias\", \"output_blocks.3.2.conv.weight\", \"output_blocks.3.2.conv.bias\", \"output_blocks.6.1.norm.weight\", \"output_blocks.6.1.norm.bias\", \"output_blocks.6.1.qkv.weight\", \"output_blocks.6.1.qkv.bias\", \"output_blocks.6.1.proj_out.weight\", \"output_blocks.6.1.proj_out.bias\", \"output_blocks.7.1.norm.weight\", \"output_blocks.7.1.norm.bias\", \"output_blocks.7.1.qkv.weight\", \"output_blocks.7.1.qkv.bias\", \"output_blocks.7.1.proj_out.weight\", \"output_blocks.7.1.proj_out.bias\", \"output_blocks.7.2.conv.weight\", \"output_blocks.7.2.conv.bias\", \"output_blocks.11.1.conv.weight\", \"output_blocks.11.1.conv.bias\", \"output_blocks.12.0.in_layers.0.weight\", \"output_blocks.12.0.in_layers.0.bias\", \"output_blocks.12.0.in_layers.2.weight\", \"output_blocks.12.0.in_layers.2.bias\", \"output_blocks.12.0.emb_layers.1.weight\", \"output_blocks.12.0.emb_layers.1.bias\", \"output_blocks.12.0.out_layers.0.weight\", \"output_blocks.12.0.out_layers.0.bias\", \"output_blocks.12.0.out_layers.3.weight\", \"output_blocks.12.0.out_layers.3.bias\", \"output_blocks.12.0.skip_connection.weight\", \"output_blocks.12.0.skip_connection.bias\", \"output_blocks.13.0.in_layers.0.weight\", \"output_blocks.13.0.in_layers.0.bias\", \"output_blocks.13.0.in_layers.2.weight\", \"output_blocks.13.0.in_layers.2.bias\", \"output_blocks.13.0.emb_layers.1.weight\", \"output_blocks.13.0.emb_layers.1.bias\", \"output_blocks.13.0.out_layers.0.weight\", \"output_blocks.13.0.out_layers.0.bias\", \"output_blocks.13.0.out_layers.3.weight\", \"output_blocks.13.0.out_layers.3.bias\", \"output_blocks.13.0.skip_connection.weight\", \"output_blocks.13.0.skip_connection.bias\", \"output_blocks.14.0.in_layers.0.weight\", \"output_blocks.14.0.in_layers.0.bias\", \"output_blocks.14.0.in_layers.2.weight\", \"output_blocks.14.0.in_layers.2.bias\", \"output_blocks.14.0.emb_layers.1.weight\", \"output_blocks.14.0.emb_layers.1.bias\", \"output_blocks.14.0.out_layers.0.weight\", \"output_blocks.14.0.out_layers.0.bias\", \"output_blocks.14.0.out_layers.3.weight\", \"output_blocks.14.0.out_layers.3.bias\", \"output_blocks.14.0.skip_connection.weight\", \"output_blocks.14.0.skip_connection.bias\", \"output_blocks.15.0.in_layers.0.weight\", \"output_blocks.15.0.in_layers.0.bias\", \"output_blocks.15.0.in_layers.2.weight\", \"output_blocks.15.0.in_layers.2.bias\", \"output_blocks.15.0.emb_layers.1.weight\", \"output_blocks.15.0.emb_layers.1.bias\", \"output_blocks.15.0.out_layers.0.weight\", \"output_blocks.15.0.out_layers.0.bias\", \"output_blocks.15.0.out_layers.3.weight\", \"output_blocks.15.0.out_layers.3.bias\", \"output_blocks.15.0.skip_connection.weight\", \"output_blocks.15.0.skip_connection.bias\". \n\tUnexpected key(s) in state_dict: \"input_blocks.3.0.op.weight\", \"input_blocks.3.0.op.bias\", \"input_blocks.4.0.in_layers.0.weight\", \"input_blocks.4.0.in_layers.0.bias\", \"input_blocks.4.0.in_layers.2.weight\", \"input_blocks.4.0.in_layers.2.bias\", \"input_blocks.4.0.emb_layers.1.weight\", \"input_blocks.4.0.emb_layers.1.bias\", \"input_blocks.4.0.out_layers.0.weight\", \"input_blocks.4.0.out_layers.0.bias\", \"input_blocks.4.0.out_layers.3.weight\", \"input_blocks.4.0.out_layers.3.bias\", \"input_blocks.4.0.skip_connection.weight\", \"input_blocks.4.0.skip_connection.bias\", \"input_blocks.6.0.op.weight\", \"input_blocks.6.0.op.bias\", \"input_blocks.7.1.norm.weight\", \"input_blocks.7.1.norm.bias\", \"input_blocks.7.1.qkv.weight\", \"input_blocks.7.1.qkv.bias\", \"input_blocks.7.1.proj_out.weight\", \"input_blocks.7.1.proj_out.bias\", \"input_blocks.8.1.norm.weight\", \"input_blocks.8.1.norm.bias\", \"input_blocks.8.1.qkv.weight\", \"input_blocks.8.1.qkv.bias\", \"input_blocks.8.1.proj_out.weight\", \"input_blocks.8.1.proj_out.bias\", \"input_blocks.8.0.in_layers.0.weight\", \"input_blocks.8.0.in_layers.0.bias\", \"input_blocks.8.0.in_layers.2.weight\", \"input_blocks.8.0.in_layers.2.bias\", \"input_blocks.8.0.emb_layers.1.weight\", \"input_blocks.8.0.emb_layers.1.bias\", \"input_blocks.8.0.out_layers.0.weight\", \"input_blocks.8.0.out_layers.0.bias\", \"input_blocks.8.0.out_layers.3.weight\", \"input_blocks.8.0.out_layers.3.bias\", \"input_blocks.9.0.op.weight\", \"input_blocks.9.0.op.bias\", \"input_blocks.10.0.skip_connection.weight\", \"input_blocks.10.0.skip_connection.bias\", \"output_blocks.2.2.conv.weight\", \"output_blocks.2.2.conv.bias\", \"output_blocks.5.2.conv.weight\", \"output_blocks.5.2.conv.bias\", \"output_blocks.8.1.conv.weight\", \"output_blocks.8.1.conv.bias\". \n\tsize mismatch for time_embed.0.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for time_embed.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for time_embed.2.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for time_embed.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for input_blocks.0.0.weight: copying a param with shape torch.Size([64, 3, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3, 3]).\n\tsize mismatch for input_blocks.0.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for input_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for input_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for input_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for input_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for input_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for input_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.10.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.10.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.10.0.in_layers.2.weight: copying a param with shape torch.Size([256, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.10.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for input_blocks.11.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for middle_block.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.in_layers.2.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for middle_block.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for middle_block.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for middle_block.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for middle_block.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for middle_block.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.in_layers.2.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.2.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for middle_block.2.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for middle_block.2.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.2.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.in_layers.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.0.0.in_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.0.0.in_layers.2.weight: copying a param with shape torch.Size([256, 512, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3, 3]).\n\tsize mismatch for output_blocks.0.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.0.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.0.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.0.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.skip_connection.weight: copying a param with shape torch.Size([256, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1, 1]).\n\tsize mismatch for output_blocks.0.0.skip_connection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.0.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.0.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.0.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([256, 512, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3, 3]).\n\tsize mismatch for output_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.skip_connection.weight: copying a param with shape torch.Size([256, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1, 1]).\n\tsize mismatch for output_blocks.1.0.skip_connection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.1.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.1.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.1.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([256, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3, 3]).\n\tsize mismatch for output_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.skip_connection.weight: copying a param with shape torch.Size([256, 384, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1, 1]).\n\tsize mismatch for output_blocks.2.0.skip_connection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.2.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.2.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.2.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.3.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.3.0.in_layers.2.weight: copying a param with shape torch.Size([128, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 768, 3, 3, 3]).\n\tsize mismatch for output_blocks.3.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.3.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.3.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.3.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.skip_connection.weight: copying a param with shape torch.Size([128, 384, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 768, 1, 1, 1]).\n\tsize mismatch for output_blocks.3.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.1.qkv.weight: copying a param with shape torch.Size([384, 128, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.3.1.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.3.1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.3.1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.4.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.4.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.4.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 768, 3, 3, 3]).\n\tsize mismatch for output_blocks.4.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.4.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.4.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.4.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1, 1]).\n\tsize mismatch for output_blocks.4.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.1.qkv.weight: copying a param with shape torch.Size([384, 128, 1]) from checkpoint, the shape in current model is torch.Size([768, 256, 1]).\n\tsize mismatch for output_blocks.4.1.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.4.1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1]).\n\tsize mismatch for output_blocks.4.1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.5.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.5.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.1.qkv.weight: copying a param with shape torch.Size([384, 128, 1]) from checkpoint, the shape in current model is torch.Size([768, 256, 1]).\n\tsize mismatch for output_blocks.5.1.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.5.1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1]).\n\tsize mismatch for output_blocks.5.1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.6.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.6.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.6.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.6.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.6.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.6.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.6.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.7.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.8.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.8.0.in_layers.2.weight: copying a param with shape torch.Size([128, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.8.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.8.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.8.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.8.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.skip_connection.weight: copying a param with shape torch.Size([128, 192, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.8.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.9.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.9.0.in_layers.2.weight: copying a param with shape torch.Size([64, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.9.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.9.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.9.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.9.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.skip_connection.weight: copying a param with shape torch.Size([64, 192, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.9.0.skip_connection.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.10.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.10.0.in_layers.2.weight: copying a param with shape torch.Size([64, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.10.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.10.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.10.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.10.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.skip_connection.weight: copying a param with shape torch.Size([64, 128, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.10.0.skip_connection.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for output_blocks.11.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for output_blocks.11.0.in_layers.2.weight: copying a param with shape torch.Size([64, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 384, 3, 3, 3]).\n\tsize mismatch for output_blocks.11.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.11.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.11.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.11.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.skip_connection.weight: copying a param with shape torch.Size([64, 128, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1, 1]).\n\tsize mismatch for output_blocks.11.0.skip_connection.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for out.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for out.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for out.2.weight: copying a param with shape torch.Size([6, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 128, 3, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 167\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   3. 정규화 범위가 학습 시와 일치하는가?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[43mtest_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    169\u001b[0m         check_sampling_consistency()\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mtest_trained_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/3d_diff_logdir6/model011000.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 모델 로드 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/topodiff/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNetModel:\n\tMissing key(s) in state_dict: \"input_blocks.3.0.in_layers.0.weight\", \"input_blocks.3.0.in_layers.0.bias\", \"input_blocks.3.0.in_layers.2.weight\", \"input_blocks.3.0.in_layers.2.bias\", \"input_blocks.3.0.emb_layers.1.weight\", \"input_blocks.3.0.emb_layers.1.bias\", \"input_blocks.3.0.out_layers.0.weight\", \"input_blocks.3.0.out_layers.0.bias\", \"input_blocks.3.0.out_layers.3.weight\", \"input_blocks.3.0.out_layers.3.bias\", \"input_blocks.4.0.op.weight\", \"input_blocks.4.0.op.bias\", \"input_blocks.5.0.skip_connection.weight\", \"input_blocks.5.0.skip_connection.bias\", \"input_blocks.6.0.in_layers.0.weight\", \"input_blocks.6.0.in_layers.0.bias\", \"input_blocks.6.0.in_layers.2.weight\", \"input_blocks.6.0.in_layers.2.bias\", \"input_blocks.6.0.emb_layers.1.weight\", \"input_blocks.6.0.emb_layers.1.bias\", \"input_blocks.6.0.out_layers.0.weight\", \"input_blocks.6.0.out_layers.0.bias\", \"input_blocks.6.0.out_layers.3.weight\", \"input_blocks.6.0.out_layers.3.bias\", \"input_blocks.8.0.op.weight\", \"input_blocks.8.0.op.bias\", \"input_blocks.9.0.in_layers.0.weight\", \"input_blocks.9.0.in_layers.0.bias\", \"input_blocks.9.0.in_layers.2.weight\", \"input_blocks.9.0.in_layers.2.bias\", \"input_blocks.9.0.emb_layers.1.weight\", \"input_blocks.9.0.emb_layers.1.bias\", \"input_blocks.9.0.out_layers.0.weight\", \"input_blocks.9.0.out_layers.0.bias\", \"input_blocks.9.0.out_layers.3.weight\", \"input_blocks.9.0.out_layers.3.bias\", \"input_blocks.9.1.norm.weight\", \"input_blocks.9.1.norm.bias\", \"input_blocks.9.1.qkv.weight\", \"input_blocks.9.1.qkv.bias\", \"input_blocks.9.1.proj_out.weight\", \"input_blocks.9.1.proj_out.bias\", \"input_blocks.12.0.op.weight\", \"input_blocks.12.0.op.bias\", \"input_blocks.13.0.in_layers.0.weight\", \"input_blocks.13.0.in_layers.0.bias\", \"input_blocks.13.0.in_layers.2.weight\", \"input_blocks.13.0.in_layers.2.bias\", \"input_blocks.13.0.emb_layers.1.weight\", \"input_blocks.13.0.emb_layers.1.bias\", \"input_blocks.13.0.out_layers.0.weight\", \"input_blocks.13.0.out_layers.0.bias\", \"input_blocks.13.0.out_layers.3.weight\", \"input_blocks.13.0.out_layers.3.bias\", \"input_blocks.13.0.skip_connection.weight\", \"input_blocks.13.0.skip_connection.bias\", \"input_blocks.13.1.norm.weight\", \"input_blocks.13.1.norm.bias\", \"input_blocks.13.1.qkv.weight\", \"input_blocks.13.1.qkv.bias\", \"input_blocks.13.1.proj_out.weight\", \"input_blocks.13.1.proj_out.bias\", \"input_blocks.14.0.in_layers.0.weight\", \"input_blocks.14.0.in_layers.0.bias\", \"input_blocks.14.0.in_layers.2.weight\", \"input_blocks.14.0.in_layers.2.bias\", \"input_blocks.14.0.emb_layers.1.weight\", \"input_blocks.14.0.emb_layers.1.bias\", \"input_blocks.14.0.out_layers.0.weight\", \"input_blocks.14.0.out_layers.0.bias\", \"input_blocks.14.0.out_layers.3.weight\", \"input_blocks.14.0.out_layers.3.bias\", \"input_blocks.14.1.norm.weight\", \"input_blocks.14.1.norm.bias\", \"input_blocks.14.1.qkv.weight\", \"input_blocks.14.1.qkv.bias\", \"input_blocks.14.1.proj_out.weight\", \"input_blocks.14.1.proj_out.bias\", \"input_blocks.15.0.in_layers.0.weight\", \"input_blocks.15.0.in_layers.0.bias\", \"input_blocks.15.0.in_layers.2.weight\", \"input_blocks.15.0.in_layers.2.bias\", \"input_blocks.15.0.emb_layers.1.weight\", \"input_blocks.15.0.emb_layers.1.bias\", \"input_blocks.15.0.out_layers.0.weight\", \"input_blocks.15.0.out_layers.0.bias\", \"input_blocks.15.0.out_layers.3.weight\", \"input_blocks.15.0.out_layers.3.bias\", \"input_blocks.15.1.norm.weight\", \"input_blocks.15.1.norm.bias\", \"input_blocks.15.1.qkv.weight\", \"input_blocks.15.1.qkv.bias\", \"input_blocks.15.1.proj_out.weight\", \"input_blocks.15.1.proj_out.bias\", \"output_blocks.3.2.conv.weight\", \"output_blocks.3.2.conv.bias\", \"output_blocks.6.1.norm.weight\", \"output_blocks.6.1.norm.bias\", \"output_blocks.6.1.qkv.weight\", \"output_blocks.6.1.qkv.bias\", \"output_blocks.6.1.proj_out.weight\", \"output_blocks.6.1.proj_out.bias\", \"output_blocks.7.1.norm.weight\", \"output_blocks.7.1.norm.bias\", \"output_blocks.7.1.qkv.weight\", \"output_blocks.7.1.qkv.bias\", \"output_blocks.7.1.proj_out.weight\", \"output_blocks.7.1.proj_out.bias\", \"output_blocks.7.2.conv.weight\", \"output_blocks.7.2.conv.bias\", \"output_blocks.11.1.conv.weight\", \"output_blocks.11.1.conv.bias\", \"output_blocks.12.0.in_layers.0.weight\", \"output_blocks.12.0.in_layers.0.bias\", \"output_blocks.12.0.in_layers.2.weight\", \"output_blocks.12.0.in_layers.2.bias\", \"output_blocks.12.0.emb_layers.1.weight\", \"output_blocks.12.0.emb_layers.1.bias\", \"output_blocks.12.0.out_layers.0.weight\", \"output_blocks.12.0.out_layers.0.bias\", \"output_blocks.12.0.out_layers.3.weight\", \"output_blocks.12.0.out_layers.3.bias\", \"output_blocks.12.0.skip_connection.weight\", \"output_blocks.12.0.skip_connection.bias\", \"output_blocks.13.0.in_layers.0.weight\", \"output_blocks.13.0.in_layers.0.bias\", \"output_blocks.13.0.in_layers.2.weight\", \"output_blocks.13.0.in_layers.2.bias\", \"output_blocks.13.0.emb_layers.1.weight\", \"output_blocks.13.0.emb_layers.1.bias\", \"output_blocks.13.0.out_layers.0.weight\", \"output_blocks.13.0.out_layers.0.bias\", \"output_blocks.13.0.out_layers.3.weight\", \"output_blocks.13.0.out_layers.3.bias\", \"output_blocks.13.0.skip_connection.weight\", \"output_blocks.13.0.skip_connection.bias\", \"output_blocks.14.0.in_layers.0.weight\", \"output_blocks.14.0.in_layers.0.bias\", \"output_blocks.14.0.in_layers.2.weight\", \"output_blocks.14.0.in_layers.2.bias\", \"output_blocks.14.0.emb_layers.1.weight\", \"output_blocks.14.0.emb_layers.1.bias\", \"output_blocks.14.0.out_layers.0.weight\", \"output_blocks.14.0.out_layers.0.bias\", \"output_blocks.14.0.out_layers.3.weight\", \"output_blocks.14.0.out_layers.3.bias\", \"output_blocks.14.0.skip_connection.weight\", \"output_blocks.14.0.skip_connection.bias\", \"output_blocks.15.0.in_layers.0.weight\", \"output_blocks.15.0.in_layers.0.bias\", \"output_blocks.15.0.in_layers.2.weight\", \"output_blocks.15.0.in_layers.2.bias\", \"output_blocks.15.0.emb_layers.1.weight\", \"output_blocks.15.0.emb_layers.1.bias\", \"output_blocks.15.0.out_layers.0.weight\", \"output_blocks.15.0.out_layers.0.bias\", \"output_blocks.15.0.out_layers.3.weight\", \"output_blocks.15.0.out_layers.3.bias\", \"output_blocks.15.0.skip_connection.weight\", \"output_blocks.15.0.skip_connection.bias\". \n\tUnexpected key(s) in state_dict: \"input_blocks.3.0.op.weight\", \"input_blocks.3.0.op.bias\", \"input_blocks.4.0.in_layers.0.weight\", \"input_blocks.4.0.in_layers.0.bias\", \"input_blocks.4.0.in_layers.2.weight\", \"input_blocks.4.0.in_layers.2.bias\", \"input_blocks.4.0.emb_layers.1.weight\", \"input_blocks.4.0.emb_layers.1.bias\", \"input_blocks.4.0.out_layers.0.weight\", \"input_blocks.4.0.out_layers.0.bias\", \"input_blocks.4.0.out_layers.3.weight\", \"input_blocks.4.0.out_layers.3.bias\", \"input_blocks.4.0.skip_connection.weight\", \"input_blocks.4.0.skip_connection.bias\", \"input_blocks.6.0.op.weight\", \"input_blocks.6.0.op.bias\", \"input_blocks.7.1.norm.weight\", \"input_blocks.7.1.norm.bias\", \"input_blocks.7.1.qkv.weight\", \"input_blocks.7.1.qkv.bias\", \"input_blocks.7.1.proj_out.weight\", \"input_blocks.7.1.proj_out.bias\", \"input_blocks.8.1.norm.weight\", \"input_blocks.8.1.norm.bias\", \"input_blocks.8.1.qkv.weight\", \"input_blocks.8.1.qkv.bias\", \"input_blocks.8.1.proj_out.weight\", \"input_blocks.8.1.proj_out.bias\", \"input_blocks.8.0.in_layers.0.weight\", \"input_blocks.8.0.in_layers.0.bias\", \"input_blocks.8.0.in_layers.2.weight\", \"input_blocks.8.0.in_layers.2.bias\", \"input_blocks.8.0.emb_layers.1.weight\", \"input_blocks.8.0.emb_layers.1.bias\", \"input_blocks.8.0.out_layers.0.weight\", \"input_blocks.8.0.out_layers.0.bias\", \"input_blocks.8.0.out_layers.3.weight\", \"input_blocks.8.0.out_layers.3.bias\", \"input_blocks.9.0.op.weight\", \"input_blocks.9.0.op.bias\", \"input_blocks.10.0.skip_connection.weight\", \"input_blocks.10.0.skip_connection.bias\", \"output_blocks.2.2.conv.weight\", \"output_blocks.2.2.conv.bias\", \"output_blocks.5.2.conv.weight\", \"output_blocks.5.2.conv.bias\", \"output_blocks.8.1.conv.weight\", \"output_blocks.8.1.conv.bias\". \n\tsize mismatch for time_embed.0.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for time_embed.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for time_embed.2.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for time_embed.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for input_blocks.0.0.weight: copying a param with shape torch.Size([64, 3, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3, 3]).\n\tsize mismatch for input_blocks.0.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for input_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for input_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for input_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n\tsize mismatch for input_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for input_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for input_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for input_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for input_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.10.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.10.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for input_blocks.10.0.in_layers.2.weight: copying a param with shape torch.Size([256, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for input_blocks.10.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for input_blocks.11.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for middle_block.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.in_layers.2.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for middle_block.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for middle_block.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for middle_block.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for middle_block.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for middle_block.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.in_layers.2.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.2.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for middle_block.2.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for middle_block.2.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for middle_block.2.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for middle_block.2.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.in_layers.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.0.0.in_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.0.0.in_layers.2.weight: copying a param with shape torch.Size([256, 512, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3, 3]).\n\tsize mismatch for output_blocks.0.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.0.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.0.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.0.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.0.skip_connection.weight: copying a param with shape torch.Size([256, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1, 1]).\n\tsize mismatch for output_blocks.0.0.skip_connection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.0.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.0.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.0.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.0.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([256, 512, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3, 3]).\n\tsize mismatch for output_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.0.skip_connection.weight: copying a param with shape torch.Size([256, 512, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1, 1]).\n\tsize mismatch for output_blocks.1.0.skip_connection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.1.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.1.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.1.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.1.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([256, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3, 3]).\n\tsize mismatch for output_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.0.skip_connection.weight: copying a param with shape torch.Size([256, 384, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1, 1]).\n\tsize mismatch for output_blocks.2.0.skip_connection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.2.1.qkv.weight: copying a param with shape torch.Size([768, 256, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.2.1.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.2.1.proj_out.weight: copying a param with shape torch.Size([256, 256, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.2.1.proj_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.3.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.3.0.in_layers.2.weight: copying a param with shape torch.Size([128, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 768, 3, 3, 3]).\n\tsize mismatch for output_blocks.3.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for output_blocks.3.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_blocks.3.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.3.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.0.skip_connection.weight: copying a param with shape torch.Size([128, 384, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 768, 1, 1, 1]).\n\tsize mismatch for output_blocks.3.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.3.1.qkv.weight: copying a param with shape torch.Size([384, 128, 1]) from checkpoint, the shape in current model is torch.Size([1536, 512, 1]).\n\tsize mismatch for output_blocks.3.1.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for output_blocks.3.1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\n\tsize mismatch for output_blocks.3.1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.4.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.4.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.4.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 768, 3, 3, 3]).\n\tsize mismatch for output_blocks.4.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.4.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.4.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.4.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1, 1]).\n\tsize mismatch for output_blocks.4.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.4.1.qkv.weight: copying a param with shape torch.Size([384, 128, 1]) from checkpoint, the shape in current model is torch.Size([768, 256, 1]).\n\tsize mismatch for output_blocks.4.1.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.4.1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1]).\n\tsize mismatch for output_blocks.4.1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.5.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.5.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.5.1.qkv.weight: copying a param with shape torch.Size([384, 128, 1]) from checkpoint, the shape in current model is torch.Size([768, 256, 1]).\n\tsize mismatch for output_blocks.5.1.qkv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for output_blocks.5.1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1]).\n\tsize mismatch for output_blocks.5.1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.6.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.6.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.6.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.6.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.6.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.6.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.6.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.6.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([128, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.7.0.skip_connection.weight: copying a param with shape torch.Size([128, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.7.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.8.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.8.0.in_layers.2.weight: copying a param with shape torch.Size([128, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.8.0.in_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.emb_layers.1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.8.0.emb_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.8.0.out_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.out_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.out_layers.3.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.8.0.out_layers.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.8.0.skip_connection.weight: copying a param with shape torch.Size([128, 192, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.8.0.skip_connection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.9.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.9.0.in_layers.2.weight: copying a param with shape torch.Size([64, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.9.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.9.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.9.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.9.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.9.0.skip_connection.weight: copying a param with shape torch.Size([64, 192, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.9.0.skip_connection.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.10.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.10.0.in_layers.2.weight: copying a param with shape torch.Size([64, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for output_blocks.10.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.10.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.10.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.10.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.10.0.skip_connection.weight: copying a param with shape torch.Size([64, 128, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1, 1]).\n\tsize mismatch for output_blocks.10.0.skip_connection.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.in_layers.0.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for output_blocks.11.0.in_layers.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for output_blocks.11.0.in_layers.2.weight: copying a param with shape torch.Size([64, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 384, 3, 3, 3]).\n\tsize mismatch for output_blocks.11.0.in_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.emb_layers.1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for output_blocks.11.0.emb_layers.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for output_blocks.11.0.out_layers.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.out_layers.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.out_layers.3.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for output_blocks.11.0.out_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for output_blocks.11.0.skip_connection.weight: copying a param with shape torch.Size([64, 128, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1, 1]).\n\tsize mismatch for output_blocks.11.0.skip_connection.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for out.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for out.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for out.2.weight: copying a param with shape torch.Size([6, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 128, 3, 3, 3])."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "학습된 모델의 정확한 동작 방식 확인\n",
    "프로젝트 루트에서 실행: python model_debug_script.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "# 프로젝트 경로 추가\n",
    "sys.path.insert(0, '/home/yeoneung/Euihyun/3D_TPMS_topoDIff')\n",
    "\n",
    "from topodiff import dist_util\n",
    "from topodiff.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from topodiff.image_datasets_diffusion_model import load_data\n",
    "\n",
    "def test_trained_model():\n",
    "    \"\"\"학습된 모델의 정확한 동작 확인\"\"\"\n",
    "    \n",
    "    print(\"🔍 학습된 모델 분석 시작...\")\n",
    "    \n",
    "    # 1. 모델 로드\n",
    "    print(\"\\n1️⃣ 모델 로드\")\n",
    "    model, diffusion = create_model_and_diffusion(**model_and_diffusion_defaults())\n",
    "    \n",
    "    model_path = \"./checkpoints/3d_diff_logdir6/model011000.pt\"\n",
    "    state_dict = th.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ 모델 로드 완료\")\n",
    "    print(f\"✅ 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 2. 학습 데이터 확인\n",
    "    print(\"\\n2️⃣ 학습 데이터 포맷 확인\")\n",
    "    data_dir = \"/home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\"\n",
    "    \n",
    "    try:\n",
    "        data = load_data(\n",
    "            data_dir=data_dir,\n",
    "            batch_size=1,\n",
    "            image_size=64,\n",
    "            split='train',\n",
    "            num_workers=1\n",
    "        )\n",
    "        \n",
    "        batch, cond_dict = next(data)\n",
    "        print(f\"✅ 학습 데이터 로드 성공\")\n",
    "        print(f\"✅ 배치 형태: {batch.shape}\")\n",
    "        print(f\"✅ 조건 키: {cond_dict.keys()}\")\n",
    "        print(f\"✅ 조건값 - VF: {cond_dict['target_vf'].item():.3f}, YM: {cond_dict['target_ym'].item():.3f}\")\n",
    "        \n",
    "        # 채널별 분석\n",
    "        print(f\"\\n📊 채널별 분석:\")\n",
    "        print(f\"   채널 0 (구조): mean={batch[0,0].mean():.3f}, std={batch[0,0].std():.3f}, range=[{batch[0,0].min():.3f}, {batch[0,0].max():.3f}]\")\n",
    "        print(f\"   채널 1 (VF):   mean={batch[0,1].mean():.3f}, std={batch[0,1].std():.6f}\")\n",
    "        print(f\"   채널 2 (YM):   mean={batch[0,2].mean():.3f}, std={batch[0,2].std():.6f}\")\n",
    "        \n",
    "        # VF, YM이 상수인지 확인\n",
    "        vf_unique = th.unique(batch[0,1])\n",
    "        ym_unique = th.unique(batch[0,2])\n",
    "        print(f\"   VF 고유값 개수: {len(vf_unique)} (상수인지: {len(vf_unique) == 1})\")\n",
    "        print(f\"   YM 고유값 개수: {len(ym_unique)} (상수인지: {len(ym_unique) == 1})\")\n",
    "        \n",
    "        training_sample = batch\n",
    "        training_cond = cond_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터 로드 실패: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 3. 모델 Forward 테스트\n",
    "    print(\"\\n3️⃣ 모델 Forward 테스트\")\n",
    "    \n",
    "    with th.no_grad():\n",
    "        # 학습 데이터로 테스트\n",
    "        output = model(training_sample, th.tensor([500]))\n",
    "        \n",
    "        print(f\"✅ 모델 출력 성공\")\n",
    "        print(f\"✅ 입력 형태: {training_sample.shape}\")\n",
    "        print(f\"✅ 출력 형태: {output.shape}\")\n",
    "        print(f\"✅ 출력 범위: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "        \n",
    "        # learn_sigma 확인\n",
    "        learn_sigma = output.shape[1] == 6\n",
    "        print(f\"✅ learn_sigma: {learn_sigma} ({'6채널' if learn_sigma else '3채널'} 출력)\")\n",
    "        \n",
    "        if learn_sigma:\n",
    "            mean_pred = output[:, :3]\n",
    "            var_pred = output[:, 3:]\n",
    "            print(f\"   평균 예측: {mean_pred.shape}, 범위: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]\")\n",
    "            print(f\"   분산 예측: {var_pred.shape}, 범위: [{var_pred.min():.3f}, {var_pred.max():.3f}]\")\n",
    "        \n",
    "    # 4. 노이즈 추가 테스트 (학습 시와 동일)\n",
    "    print(\"\\n4️⃣ 노이즈 추가 테스트 (q_sample_conditional)\")\n",
    "    \n",
    "    t = th.tensor([500])  # 중간 타임스텝\n",
    "    noise = th.randn_like(training_sample[:, :1])  # 구조 채널만\n",
    "    \n",
    "    # q_sample_conditional 호출\n",
    "    noisy_sample = diffusion.q_sample_conditional(training_sample, t, noise)\n",
    "    \n",
    "    print(f\"✅ 노이즈 추가 완료\")\n",
    "    print(f\"✅ 원본 vs 노이즈: 구조 채널 std {training_sample[0,0].std():.3f} -> {noisy_sample[0,0].std():.3f}\")\n",
    "    print(f\"✅ 조건 채널 유지: VF {noisy_sample[0,1].mean():.3f} (원본: {training_sample[0,1].mean():.3f})\")\n",
    "    print(f\"✅ 조건 채널 유지: YM {noisy_sample[0,2].mean():.3f} (원본: {training_sample[0,2].mean():.3f})\")\n",
    "    \n",
    "    # 5. 샘플링 스텝 시뮬레이션\n",
    "    print(\"\\n5️⃣ 샘플링 스텝 시뮬레이션\")\n",
    "    \n",
    "    # 조건값 설정\n",
    "    target_vf = 0.3\n",
    "    target_ym = 50.0\n",
    "    \n",
    "    # 정규화 (데이터셋에서 가져온 범위 사용)\n",
    "    dataset = load_data(data_dir=data_dir, batch_size=1, image_size=64, split='train', num_workers=1)\n",
    "    sample_batch, _ = next(dataset)\n",
    "    \n",
    "    # VolumeDataset 인스턴스에서 범위 가져오기 (임시 방법)\n",
    "    vf_range = (0.1, 0.9)  # 임시값, 실제로는 dataset에서 가져와야 함\n",
    "    ym_range = (10.0, 300.0)  # 임시값\n",
    "    \n",
    "    vf_norm = (target_vf - vf_range[0]) / (vf_range[1] - vf_range[0])\n",
    "    ym_norm = (target_ym - ym_range[0]) / (ym_range[1] - ym_range[0])\n",
    "    \n",
    "    print(f\"   타겟 조건: VF={target_vf}, YM={target_ym}\")\n",
    "    print(f\"   정규화된 조건: VF={vf_norm:.3f}, YM={ym_norm:.3f}\")\n",
    "    \n",
    "    # 샘플링용 초기 텐서 생성\n",
    "    sample_img = th.zeros(1, 3, 64, 64, 64)\n",
    "    sample_img[:, 0] = th.randn(1, 64, 64, 64)  # 구조는 노이즈\n",
    "    sample_img[:, 1] = vf_norm  # VF 조건\n",
    "    sample_img[:, 2] = ym_norm  # YM 조건\n",
    "    \n",
    "    print(f\"   샘플링 초기 텐서: {sample_img.shape}\")\n",
    "    print(f\"   초기 조건 확인: VF={sample_img[0,1].mean():.3f}, YM={sample_img[0,2].mean():.3f}\")\n",
    "    \n",
    "    # 모델 예측\n",
    "    with th.no_grad():\n",
    "        sample_output = model(sample_img, th.tensor([500]))\n",
    "        print(f\"   샘플링 모델 출력: {sample_output.shape}\")\n",
    "        print(f\"   출력 범위: [{sample_output.min():.3f}, {sample_output.max():.3f}]\")\n",
    "    \n",
    "    # 6. 결론\n",
    "    print(\"\\n📋 분석 결과:\")\n",
    "    print(\"✅ 모델이 정상적으로 로드됨\")\n",
    "    print(\"✅ 학습 데이터 포맷 확인됨 (3채널: 구조+VF+YM)\")\n",
    "    print(\"✅ 조건이 상수맵으로 브로드캐스트됨\")\n",
    "    print(\"✅ 모델이 조건 포함 입력을 받아 처리함\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def check_sampling_consistency():\n",
    "    \"\"\"샘플링 일관성 확인\"\"\"\n",
    "    print(\"\\n🔄 샘플링 일관성 체크\")\n",
    "    \n",
    "    # p_sample_loop_progressive에서 조건 유지가 되는지 확인\n",
    "    # 이 부분은 실제 샘플링 코드 실행 필요\n",
    "    \n",
    "    print(\"⚠️  샘플링 과정에서 확인할 사항:\")\n",
    "    print(\"   1. 각 디노이징 스텝에서 조건 채널이 유지되는가?\")\n",
    "    print(\"   2. 구조 채널만 업데이트되고 조건은 고정되는가?\")\n",
    "    print(\"   3. 정규화 범위가 학습 시와 일치하는가?\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_trained_model()\n",
    "    if success:\n",
    "        check_sampling_consistency()\n",
    "    else:\n",
    "        print(\"❌ 모델 분석 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "meta_path = '/home/yeoneung/Euihyun/3D_TPMS_topoDIff/data/t0.6_dia0.1_pri0.4_neo0.5_meta.npy'\n",
    "\n",
    "# 1) 로드\n",
    "loaded = np.load(meta_path, allow_pickle=True)\n",
    "\n",
    "# 2) 실제 메타 객체 추출\n",
    "if isinstance(loaded, np.lib.npyio.NpzFile):\n",
    "    # 혹시 .npz 형태로 저장됐으면\n",
    "    meta = {k: loaded[k] for k in loaded.files}\n",
    "elif isinstance(loaded, np.ndarray):\n",
    "    # npy → ndarray 반환\n",
    "    if loaded.dtype == object and loaded.shape == ():\n",
    "        # 단일 object array → .item() 시도\n",
    "        try:\n",
    "            meta = loaded.item()\n",
    "        except Exception:\n",
    "            meta = loaded.tolist()\n",
    "    else:\n",
    "        # 일반 배열 → 리스트로 변환\n",
    "        meta = loaded.tolist()\n",
    "else:\n",
    "    # 기타 형태일 땐 그대로 사용\n",
    "    meta = loaded\n",
    "\n",
    "# 3) 출력\n",
    "print(\"== Loaded meta object type:\", type(meta))\n",
    "if isinstance(meta, dict):\n",
    "    for k, v in meta.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "elif isinstance(meta, (list, tuple)):\n",
    "    print(\"List/tuple contents:\")\n",
    "    for i, v in enumerate(meta):\n",
    "        print(f\"  [{i}]: {v}\")\n",
    "else:\n",
    "    print(\"Value:\", meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# (노트북 루트 디렉터리로 가정)\n",
    "export PYTHONPATH=/home/yeoneung/Euihyun/3D_TPMS_topoDIff:$PYTHONPATH\n",
    "export TOPODIFF_LOGDIR=./checkpoints/3d_diff_logdir3\n",
    "\n",
    "torchrun \\\n",
    "  --nnodes=1 \\\n",
    "  --nproc_per_node=4 \\\n",
    "  -m topodiff.scripts.image_train \\\n",
    "  $MODEL_FLAGS \\\n",
    "  $DIFFUSION_FLAGS \\\n",
    "  $TRAIN_FLAGS \\\n",
    "  --data_dir /home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_log_file(log_path):\n",
    "    \"\"\"로그 파일을 파싱해서 메트릭들을 추출\"\"\"\n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # 로그 블록들을 찾기 (| key | value | 형태)\n",
    "    log_blocks = re.findall(r'(?:\\| \\w+.*?\\|.*?\\|\\n)+', content)\n",
    "    \n",
    "    for block in log_blocks:\n",
    "        step_data = {}\n",
    "        lines = block.strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # | key | value | 형태 파싱\n",
    "            match = re.match(r'\\|\\s*(\\w+)\\s*\\|\\s*([0-9\\.e\\-\\+]+)\\s*\\|', line)\n",
    "            if match:\n",
    "                key, value = match.groups()\n",
    "                try:\n",
    "                    step_data[key] = float(value)\n",
    "                except ValueError:\n",
    "                    step_data[key] = value\n",
    "        \n",
    "        # step이 있으면 해당 블록의 데이터 저장\n",
    "        if 'step' in step_data:\n",
    "            for key, value in step_data.items():\n",
    "                metrics[key].append(value)\n",
    "    \n",
    "    return dict(metrics)\n",
    "\n",
    "def plot_training_metrics(log_path):\n",
    "    \"\"\"훈련 메트릭들을 그래프로 시각화\"\"\"\n",
    "    metrics = parse_log_file(log_path)\n",
    "    \n",
    "    if not metrics or 'step' not in metrics:\n",
    "        print(\"❌ 로그 파일에서 step 정보를 찾을 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    steps = metrics['step']\n",
    "    \n",
    "    # 사용 가능한 메트릭들 확인\n",
    "    available_metrics = [k for k in metrics.keys() if k != 'step' and len(metrics[k]) == len(steps)]\n",
    "    \n",
    "    print(f\"📊 발견된 메트릭들: {available_metrics}\")\n",
    "    print(f\"📈 총 {len(steps)} 스텝의 데이터\")\n",
    "    \n",
    "    # 서브플롯 개수 결정\n",
    "    n_metrics = len(available_metrics)\n",
    "    if n_metrics == 0:\n",
    "        print(\"❌ 플롯할 메트릭이 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 그리드 크기 계산\n",
    "    n_cols = min(3, n_metrics)\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*n_rows))\n",
    "    \n",
    "    for i, metric in enumerate(available_metrics, 1):\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        values = metrics[metric]\n",
    "        \n",
    "        plt.plot(steps, values, 'b-', linewidth=2, alpha=0.8)\n",
    "        plt.title(f'{metric.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel(metric)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Y축 스케일 조정\n",
    "        if metric in ['loss', 'vb'] and max(values) > 10:\n",
    "            plt.yscale('log')\n",
    "            plt.ylabel(f'{metric} (log scale)')\n",
    "        \n",
    "        # 통계 정보 표시\n",
    "        mean_val = np.mean(values)\n",
    "        final_val = values[-1] if values else 0\n",
    "        plt.text(0.02, 0.98, f'Mean: {mean_val:.3f}\\nFinal: {final_val:.3f}', \n",
    "                transform=plt.gca().transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 요약 통계\n",
    "    print(\"\\n📋 훈련 요약:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric in available_metrics:\n",
    "        values = metrics[metric]\n",
    "        if values:\n",
    "            initial = values[0]\n",
    "            final = values[-1]\n",
    "            change = ((final - initial) / initial * 100) if initial != 0 else 0\n",
    "            print(f\"{metric:12}: {initial:8.4f} → {final:8.4f} ({change:+6.1f}%)\")\n",
    "\n",
    "def analyze_learning_progress(log_path):\n",
    "    \"\"\"학습 진행 상황 상세 분석\"\"\"\n",
    "    metrics = parse_log_file(log_path)\n",
    "    \n",
    "    if 'loss' not in metrics or 'step' not in metrics:\n",
    "        print(\"❌ loss 또는 step 정보가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    steps = np.array(metrics['step'])\n",
    "    loss = np.array(metrics['loss'])\n",
    "    \n",
    "    # 학습 안정성 분석\n",
    "    if len(loss) > 5:\n",
    "        # 최근 손실의 변화율\n",
    "        recent_steps = min(10, len(loss))\n",
    "        recent_loss = loss[-recent_steps:]\n",
    "        loss_trend = np.polyfit(range(recent_steps), recent_loss, 1)[0]\n",
    "        \n",
    "        print(f\"\\n🔍 학습 상태 분석:\")\n",
    "        print(f\"- 총 스텝: {len(steps)}\")\n",
    "        print(f\"- 초기 손실: {loss[0]:.4f}\")\n",
    "        print(f\"- 현재 손실: {loss[-1]:.4f}\")\n",
    "        print(f\"- 최근 {recent_steps}스텝 트렌드: {'⬇️ 감소' if loss_trend < 0 else '⬆️ 증가'} ({loss_trend:.6f}/step)\")\n",
    "        \n",
    "        # 수렴 여부 판단\n",
    "        if len(loss) > 20:\n",
    "            recent_var = np.var(recent_loss)\n",
    "            if recent_var < 0.1 and abs(loss_trend) < 0.01:\n",
    "                print(\"✅ 모델이 수렴하고 있습니다!\")\n",
    "            elif loss_trend > 0.1:\n",
    "                print(\"⚠️ 손실이 증가하고 있습니다. 학습률을 낮춰보세요.\")\n",
    "            else:\n",
    "                print(\"🔄 모델이 계속 학습 중입니다.\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    log_path = \"/home/yeoneung/Euihyun/3D_TPMS_topoDIff/topodiff/checkpoints/3d_diff_logdir/log.txt\"\n",
    "    \n",
    "    print(\"🚀 훈련 로그 분석 시작...\")\n",
    "    plot_training_metrics(log_path)\n",
    "    analyze_learning_progress(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95983ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def convert_npz_keys_to_arr0(directory):\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith(\".npz\"):\n",
    "            path = os.path.join(directory, fname)\n",
    "            try:\n",
    "                data = np.load(path)\n",
    "                if 'surface_field' in data:\n",
    "                    # 'surface_field' -> 'arr_0'로 다시 저장\n",
    "                    np.savez(path, arr_0=data['surface_field'])\n",
    "                    print(f\"[변환 완료] {fname}\")\n",
    "                elif 'arr_0' in data:\n",
    "                    print(f\"[스킵] 이미 arr_0 있음: {fname}\")\n",
    "                else:\n",
    "                    print(f\"[경고] 유효한 key 없음: {fname}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[에러] {fname}: {e}\")\n",
    "\n",
    "# 사용 예시\n",
    "convert_npz_keys_to_arr0(\"/home/yeoneung/Euihyun/3D_TPMS_topoDIff/data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0f4ff",
   "metadata": {},
   "source": [
    "By the end of the training, you should get in the diff_logdir a series of checkpoints. You can then use the last checkpoint as the difusion model when sampling from TopoDiff (see the notebook **4_TopoDiff_sample**)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
