"""
ë””ë²„ê¹… ë° ë°ì´í„° ë¶„ì„ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— debug_functions.pyë¡œ ì €ì¥
"""

import os
import numpy as np
import torch as th
import random

def analyze_dataset_ranges(data_dir, sample_size=100):
    """
    ğŸ”§ ì‹¤ì œ ë°ì´í„°ì—ì„œ VF/YM ë²”ìœ„ ê³„ì‚°
    """
    print("=== Dataset Range Analysis ===")
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    implicit_files = [f for f in os.listdir(data_dir) if f.endswith('_implicit.npz')]
    meta_files = [f for f in os.listdir(data_dir) if f.endswith('_meta.npy')]
    
    print(f"Found {len(implicit_files)} implicit files")
    print(f"Found {len(meta_files)} meta files")
    
    # ìƒ˜í”Œë§í•˜ì—¬ ë²”ìœ„ ê³„ì‚°
    sample_files = random.sample(meta_files, min(sample_size, len(meta_files)))
    
    vf_values = []
    ym_values = []
    
    for meta_file in sample_files:
        meta_path = os.path.join(data_dir, meta_file)
        try:
            meta = np.load(meta_path, allow_pickle=True)
            
            # ë°°ì—´ í˜•íƒœ ì²˜ë¦¬
            if isinstance(meta, np.ndarray) and meta.shape == (2,):
                vf, ym = meta[0], meta[1]
            elif isinstance(meta, np.ndarray) and meta.dtype == object:
                meta_dict = meta.item()
                vf = meta_dict.get('vf', None)
                ym = meta_dict.get('ym', None)
            else:
                print(f"Unexpected meta format in {meta_file}: {meta}")
                continue
            
            if vf is not None and ym is not None:
                vf_values.append(float(vf))
                ym_values.append(float(ym))
                
        except Exception as e:
            print(f"Error reading {meta_file}: {e}")
            continue
    
    if vf_values and ym_values:
        vf_range = (min(vf_values), max(vf_values))
        ym_range = (min(ym_values), max(ym_values))
        
        print(f"âœ… VF range: {vf_range} (mean: {np.mean(vf_values):.3f})")
        print(f"âœ… YM range: {ym_range} (mean: {np.mean(ym_values):.1f})")
        print(f"âœ… Sample count: {len(vf_values)}")
        
        return vf_range, ym_range
    else:
        print("âŒ No valid VF/YM data found")
        return None, None


def test_model_forward(model_path, device='cpu'):
    """
    ğŸ”§ ëª¨ë¸ forward í…ŒìŠ¤íŠ¸
    """
    print("=== Model Forward Test ===")
    
    try:
        from topodiff.script_util import create_model_and_diffusion, model_and_diffusion_defaults
        
        # ëª¨ë¸ ìƒì„±
        model, diffusion = create_model_and_diffusion(**model_and_diffusion_defaults())
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        state_dict = th.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
        
        print(f"âœ… Model loaded successfully")
        print(f"âœ… Parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = th.randn(1, 3, 64, 64, 64).to(device)
        test_t = th.tensor([500]).to(device)
        
        print(f"âœ… Test input shape: {test_input.shape}")
        
        with th.no_grad():
            output = model(test_input, test_t)
            
        print(f"âœ… Model forward successful")
        print(f"âœ… Output shape: {output.shape}")
        print(f"âœ… Output range: [{output.min():.3f}, {output.max():.3f}]")
        print(f"âœ… Contains NaN: {th.isnan(output).any()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Model test failed: {e}")
        return False


def test_data_loading(data_dir):
    """
    ğŸ”§ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
    """
    print("=== Data Loading Test ===")
    
    try:
        from topodiff.image_datasets_diffusion_model import load_data
        
        data = load_data(
            data_dir=data_dir,
            batch_size=2,
            image_size=64,
            split='train',
            num_workers=1
        )
        
        # ì²« ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
        batch, cond_dict = next(data)
        
        print(f"âœ… Data loading successful")
        print(f"âœ… Batch shape: {batch.shape}")
        print(f"âœ… Condition keys: {cond_dict.keys()}")
        
        if 'target_vf' in cond_dict:
            vf_vals = cond_dict['target_vf']
            print(f"âœ… VF values: {vf_vals}")
        
        if 'target_ym' in cond_dict:
            ym_vals = cond_dict['target_ym']
            print(f"âœ… YM values: {ym_vals}")
        
        # ì±„ë„ë³„ í†µê³„
        print(f"âœ… Channel 0 (struct): range=[{batch[:,0].min():.3f}, {batch[:,0].max():.3f}]")
        print(f"âœ… Channel 1 (vf): mean={batch[:,1].mean():.3f}, std={batch[:,1].std():.6f}")
        print(f"âœ… Channel 2 (ym): mean={batch[:,2].mean():.3f}, std={batch[:,2].std():.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Data loading failed: {e}")
        return False


def create_proper_meta_file(data_dir, output_path):
    """
    ğŸ”§ ì ì ˆí•œ meta.pt íŒŒì¼ ìƒì„±
    """
    print("=== Creating Meta File ===")
    
    vf_range, ym_range = analyze_dataset_ranges(data_dir)
    
    if vf_range and ym_range:
        meta = {
            'vf_range': vf_range,
            'ym_range': ym_range
        }
        
        th.save(meta, output_path)
        print(f"âœ… Meta file saved to: {output_path}")
        print(f"âœ… Content: {meta}")
        return True
    else:
        print("âŒ Failed to create meta file")
        return False


def full_debug_pipeline():
    """
    ğŸ”§ ì „ì²´ ë””ë²„ê¹… íŒŒì´í”„ë¼ì¸
    """
    print("ğŸš€ Starting Full Debug Pipeline")
    
    # ê²½ë¡œ ì„¤ì •
    data_dir = "/home/yeoneung/Euihyun/3D_TPMS_topoDIff/data"
    model_path = "./checkpoints/3d_diff_logdir6/model011000.pt"
    meta_path = "./checkpoints/3d_diff_logdir6/meta.pt"
    
    # 1. ë°ì´í„° ë²”ìœ„ ë¶„ì„
    vf_range, ym_range = analyze_dataset_ranges(data_dir)
    
    # 2. meta.pt ìƒì„±
    if vf_range and ym_range:
        create_proper_meta_file(data_dir, meta_path)
    
    # 3. ëª¨ë¸ í…ŒìŠ¤íŠ¸
    test_model_forward(model_path)
    
    # 4. ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
    test_data_loading(data_dir)
    
    print("ğŸ‰ Debug pipeline completed!")


if __name__ == "__main__":
    full_debug_pipeline()